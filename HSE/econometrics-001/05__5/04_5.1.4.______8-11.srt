1
00:00:13,270 --> 00:00:17,523
Давайте попытаемся ответить
на вопрос: когда в

2
00:00:17,523 --> 00:00:21,333
реальности может возникать
гетероскедастичность?

3
00:00:21,333 --> 00:00:25,566
Я напомню, что мы работаем в парадигме,
где мы предполагаем, что наши регрессоры

4
00:00:25,566 --> 00:00:30,799
являются случайными, и наши наблюдения
являются случайной выборкой из какой-то

5
00:00:30,799 --> 00:00:36,875
большой генеральной совокупности,
то есть отдельно взятые наблюдения x_i,

6
00:00:36,875 --> 00:00:42,888
y_i не зависят от наблюдений x_j,
y_j и имеют одинаковое с ним распределение.

7
00:00:42,888 --> 00:00:48,654
Откуда же может взяться разная условная
дисперсия случайной ошибки ε_i?

8
00:00:48,654 --> 00:00:51,586
Ответ на этот вопрос довольно прост.

9
00:00:51,586 --> 00:00:55,610
Ну давайте, например,
возьмём выборку предприятий.

10
00:00:55,610 --> 00:00:59,289
Скажем, исследователь моделирует
зависимость прибыли от каких-то

11
00:00:59,289 --> 00:01:00,890
характеристик предприятия.

12
00:01:00,890 --> 00:01:06,471
Естественно ожидать, что на крупном
предприятии колебания прибыли от месяца

13
00:01:06,471 --> 00:01:11,150
к месяцу гораздо более значительные,
чем на маленьком предприятии.

14
00:01:11,150 --> 00:01:16,465
На крупном предприятии колебания в
прибыли в сто тысяч рублей в месяц

15
00:01:16,465 --> 00:01:20,402
могут быть совершенным пустяком, в то
время как сумма такая для индивидуального

16
00:01:20,402 --> 00:01:25,581
предпринимателя может быть максимумом,
который он может заработать за месяц.

17
00:01:25,581 --> 00:01:30,325
То есть когда мы в реальности видим,
что к нам в выборку могут попасть объекты

18
00:01:30,325 --> 00:01:34,558
разного размера, — при этом размером может
быть что угодно: на предприятии это может

19
00:01:34,558 --> 00:01:38,650
быть, скажем, численность персонала; если
мы исследуем какую-нибудь зависимость,

20
00:01:38,650 --> 00:01:42,521
скажем, расходов домохозяйств
от каких-то их характеристик,

21
00:01:42,521 --> 00:01:47,292
то размером объекта будет являться
численность домохозяйства; или, скажем,

22
00:01:47,292 --> 00:01:51,350
если мы пытаемся понять,
от чего зависят расходы человека на отдых,

23
00:01:51,350 --> 00:01:56,967
то тут в каком-то смысле размером
может быть доход: чем больше доход,

24
00:01:56,967 --> 00:02:00,706
тем больше будут колебаться
расходы человека на отдых, тот,

25
00:02:00,706 --> 00:02:04,222
у кого нет возможности поехать
и тратить много на отдых,

26
00:02:04,222 --> 00:02:09,062
у него будут небольшие колебания расходов
на отдых, а тот, кто может в один

27
00:02:09,062 --> 00:02:13,996
сезон поехать куда-то очень далеко,
сделать много поездок за короткое время,

28
00:02:13,996 --> 00:02:17,830
— у него колебания расходов,
скорее всего, будут существенно выше.

29
00:02:17,830 --> 00:02:23,770
То есть как только у нас в выборке могут
попасться объекты разного размера,

30
00:02:23,770 --> 00:02:29,020
что бы этот размер ни значил,
скорее всего, есть основания подозревать

31
00:02:29,020 --> 00:02:33,397
наличие условной гетероскедастичности.

32
00:02:33,397 --> 00:02:38,363
Ну а поскольку тот или иной
размер есть и у домохозяйства,

33
00:02:38,363 --> 00:02:43,670
и у предприятия,
и у отдельно взятого индивида,

34
00:02:43,670 --> 00:02:48,184
то поэтому мы получаем,
что гетероскедастичность условная имеет

35
00:02:48,184 --> 00:02:51,360
место практически в
любой случайной выборке.

36
00:02:51,360 --> 00:02:56,510
Поэтому с гетероскедастичностью мы
сталкиваемся практически постоянно.

37
00:02:56,510 --> 00:03:00,877
Давайте посмотрим,
чем нам грозит столь частое явление.

38
00:03:00,877 --> 00:03:05,750
Во-первых, отметим, что мы изучаем
гетероскедастичность отдельно от

39
00:03:05,750 --> 00:03:09,450
других нарушений,
потенциальных нарушений предпосылок.

40
00:03:09,450 --> 00:03:14,370
То есть сейчас мы предполагаем,
что имеет место строгая экзогенность,

41
00:03:14,370 --> 00:03:20,450
то есть математическое ожидание ошибки
ε_i при фиксированных X равно нулю.

42
00:03:20,450 --> 00:03:26,325
Мы также предполагаем,
что у нас математическое ожидание

43
00:03:26,325 --> 00:03:32,468
произведения двух ε_i,
ε_j при фиксированном X также равно нулю,

44
00:03:32,468 --> 00:03:38,350
то есть нет зависимости между ε_i, ε_j,
относящимся к разным наблюдениям.

45
00:03:38,350 --> 00:03:42,353
Мы также предполагаем, что у нас,
естественно, наблюдений больше,

46
00:03:42,353 --> 00:03:46,170
чем параметров в линейной зависимости,
которую мы хотим оценить.

47
00:03:46,170 --> 00:03:50,389
Мы предполагаем,
что у нас матрица регрессоров имеет

48
00:03:50,389 --> 00:03:54,691
полный ранг с вероятностью 1,
то есть, говоря более простым языком,

49
00:03:54,691 --> 00:04:00,217
с вероятностью 1 среди объясняющих
переменных нет линейно зависимых.

50
00:04:00,217 --> 00:04:03,789
Мы по-прежнему предполагаем
случайную выборку,

51
00:04:03,789 --> 00:04:09,740
то есть отдельные наблюдения являются
независимыми и одинаково распределёнными,

52
00:04:09,740 --> 00:04:15,930
то есть никакие другие предпосылки
стандартной модели у нас не нарушены.

53
00:04:15,930 --> 00:04:21,480
И мы по случайности, по незнанию

54
00:04:21,480 --> 00:04:27,567
или намеренно используем старые формулы,
которые, мы доказали,

55
00:04:27,567 --> 00:04:32,830
что являются очень хорошими оценками
в случае условной гомоскедастичности.

56
00:04:32,830 --> 00:04:37,807
То есть мы используем классические
оценки метода наименьших квадратов для

57
00:04:37,807 --> 00:04:42,550
неизвестных коэффициентов β,
и именно: у нас β с крышкой равняется

58
00:04:42,550 --> 00:04:46,490
X'X в минус 1 помножить
на X транспонированное y.

59
00:04:46,490 --> 00:04:52,506
И мы используем стандартную
формулу для оценок дисперсии,

60
00:04:52,506 --> 00:04:57,442
оценок метода наименьших квадратов,
а именно: сумма квадратов остатков,

61
00:04:57,442 --> 00:05:00,920
делённая на (n – k),
помножить на X'X в минус 1.

62
00:05:00,920 --> 00:05:05,389
То есть мы используем стандартные
формулы как для оценок самих

63
00:05:05,389 --> 00:05:07,464
коэффициентов β с крышкой,

64
00:05:07,464 --> 00:05:12,230
также мы используем стандартные формулы
для оценки дисперсии коэффициентов.

65
00:05:12,230 --> 00:05:16,744
Ну в частности, оценка для
отдельно взятой дисперсии одного

66
00:05:16,744 --> 00:05:20,560
коэффициента — это σ² с крышкой,
RSS,

67
00:05:20,560 --> 00:05:25,182
делённое на (n – k),
делённое на RSS_j, то есть RSS,

68
00:05:25,182 --> 00:05:31,740
которое получается в регрессии j-того
объясняющей переменной на остальные.

69
00:05:31,740 --> 00:05:35,039
И вот мы используем стандартные
формулы и посмотрим,

70
00:05:35,039 --> 00:05:37,270
к каким последствиям это приведёт.

71
00:05:37,270 --> 00:05:41,668
Давайте перечислим сначала все те выводы,
которые были у нас,

72
00:05:41,668 --> 00:05:44,769
когда ни одна предпосылка
не была нарушена.

73
00:05:44,769 --> 00:05:49,536
Итак, у нас свойства делились на
свойства для конечных выборок и

74
00:05:49,536 --> 00:05:51,640
асимптотические свойства.

75
00:05:51,640 --> 00:05:54,930
Сначала рассмотрим свойства
для конечных выборок.

76
00:05:54,930 --> 00:05:59,405
Свойства для конечных выборок, в свою
очередь, у нас делились на два класса:

77
00:05:59,405 --> 00:06:03,880
свойства, в которых предполагалась
нормальность ошибок ε_i-тых,

78
00:06:03,880 --> 00:06:09,147
и свойства, в которых нормальность
ошибок не требовалась.

79
00:06:09,147 --> 00:06:14,596
Какие свойства, не требующие
нормальности ошибок ε_i-тых, у нас были?

80
00:06:14,596 --> 00:06:17,500
Ну, во-первых, это линейность по y.

81
00:06:17,500 --> 00:06:19,303
То есть, что она означает?

82
00:06:19,303 --> 00:06:24,038
Она означает, что при увеличении всех y,
всех объясняющих переменных,

83
00:06:24,038 --> 00:06:29,160
в 10 раз, у нас все оценки β с
крышкой увеличивались в 10 раз.

84
00:06:29,160 --> 00:06:31,237
К счастью, это свойство остаётся.

85
00:06:31,237 --> 00:06:36,604
Поскольку используется старая формула для
β с крышкой, поскольку в ней y входит

86
00:06:36,604 --> 00:06:41,430
линейно, то по-прежнему
линейность оценок у нас осталась.

87
00:06:41,430 --> 00:06:46,270
Второе свойство, которое не
требует нормальности ε и выполнено

88
00:06:46,270 --> 00:06:50,857
для конечных выборок,
это условная несмещённость оценок,

89
00:06:50,857 --> 00:06:55,797
или просто несмещённость,
то есть математическое ожидание оценок β с

90
00:06:55,797 --> 00:07:00,251
крышкой при фиксированных регрессорах X,
равнялось раньше β.

91
00:07:00,251 --> 00:07:05,935
Это свойство по-прежнему выполнено,
то есть, в среднем, получаемые нами

92
00:07:05,935 --> 00:07:11,215
оценки могут попасть то слева,
то справа от неизвестного коэффициента,

93
00:07:11,215 --> 00:07:15,065
но в среднем попадают в неизвестный
коэффициент, и это очень приятная новость.

94
00:07:15,065 --> 00:07:19,774
Это говорит о том, что, да, мы иногда,
когда оцениваем неизвестные параметры,

95
00:07:19,774 --> 00:07:24,950
можем иногда завысить истину или занизить,
но, в среднем, мы не ошибаемся.

96
00:07:24,950 --> 00:07:28,460
Следующая новость, к сожалению, плохая.

97
00:07:28,460 --> 00:07:32,918
Мы говорили, что без требования
нормальности ε для конечных

98
00:07:32,918 --> 00:07:37,169
выборок оценки β с крышкой, получаемых
по методу наименьших квадратов при

99
00:07:37,169 --> 00:07:41,660
выполнении всех предпосылок, — они были
эффективны, то есть они были наилучшими.

100
00:07:41,660 --> 00:07:45,844
То есть нельзя было придумать
альтернативную оценку,

101
00:07:45,844 --> 00:07:49,813
— β с крышкой альтернативная,
— которая была бы несмещённой,

102
00:07:49,813 --> 00:07:54,369
линейной и при этом которая была бы лучше,
у которой была бы меньше дисперсия.

103
00:07:54,369 --> 00:07:58,759
На этот раз это свойство не выполнено,
и, соответственно,

104
00:07:58,759 --> 00:08:03,589
получаемые нами оценки,
хотя они несмещённые, они не самые

105
00:08:03,589 --> 00:08:07,520
лучшие: у них не самая маленькая
дисперсия, которая могла бы быть.

