1
00:00:13,250 --> 00:00:19,757
Сейчас мы разберём пример теста Уайта.

2
00:00:19,757 --> 00:00:23,045
Итак, предположим,
исследователь оценил модель.

3
00:00:23,045 --> 00:00:27,456
Он оценивал зависимость спроса на
мороженое, q_i, то есть количество

4
00:00:27,456 --> 00:00:32,012
купленного мороженого в киоске,
от некоторых объясняющих переменных.

5
00:00:32,012 --> 00:00:35,890
Ну, например, это была p_i,
средняя цена мороженого в киоске,

6
00:00:35,890 --> 00:00:40,652
плюс β3 умножить на a_i, на ассортимент, то
есть количество разных видов мороженого,

7
00:00:40,652 --> 00:00:46,270
которые продаются в i-том киоске,
плюс β4 помножить на d_i,

8
00:00:46,270 --> 00:00:51,026
d_i — это расстояние от киоска до ближайшей
остановки общественного транспорта,

9
00:00:51,026 --> 00:00:52,230
плюс случайная ошибка.

10
00:00:52,230 --> 00:00:55,301
И вот исследователь оценил
эту модель и хочет проверить,

11
00:00:55,301 --> 00:00:57,557
а есть ли в модели гетероскедастичность.

12
00:00:57,557 --> 00:01:02,200
То есть исследователь хочет протестировать
H0 о том, что гетероскедастичности на

13
00:01:02,200 --> 00:01:06,538
самом деле нет условной и есть условная
гомоскедастичность, то есть хочет

14
00:01:06,538 --> 00:01:11,320
проверить гипотезу, что условная
дисперсия равна константе σ²,

15
00:01:11,320 --> 00:01:16,853
против альтернативной гипотезы о том,
что условная дисперсия

16
00:01:16,853 --> 00:01:21,590
ε_i при фиксированных X не равна σ²,

17
00:01:21,590 --> 00:01:25,850
то есть зависит от i.

18
00:01:25,850 --> 00:01:28,625
Выбран какой-то уровень значимости.

19
00:01:28,625 --> 00:01:31,160
Давайте выберем стандартные 5 %.

20
00:01:31,160 --> 00:01:35,970
И, соответственно,
исследователь хочет провести тест Уайта.

21
00:01:35,970 --> 00:01:40,620
Для теста Уайта проводится некоторая
вспомогательная регрессия,

22
00:01:40,620 --> 00:01:42,348
помимо основной.

23
00:01:42,348 --> 00:01:47,105
Проводится вспомогательная регрессия,
и известно,

24
00:01:47,105 --> 00:01:50,634
что в этой вспомогательной
регрессии R² во

25
00:01:50,634 --> 00:01:55,685
вспомогательной регрессии
оказался равен 0,15.

26
00:01:55,685 --> 00:02:00,283
И эта зависимость спроса на мороженое от
средней цены мороженого в данном киоске,

27
00:02:00,283 --> 00:02:06,005
количества разных видов мороженого и
расстояния до ближайшей остановки —

28
00:02:06,005 --> 00:02:11,060
эта зависимость оценивалась, эта
регрессия строилась по 200 наблюдениям.

29
00:02:11,060 --> 00:02:14,893
Вот у нас есть все данные,
и мы можем провести тест Уайта.

30
00:02:14,893 --> 00:02:17,375
Ну для начала нам, конечно,
неплохо бы понять,

31
00:02:17,375 --> 00:02:19,148
что это за вспомогательная регрессия.

32
00:02:19,148 --> 00:02:23,855
Понятно, что мы, конечно, даже не будем
её строить руками, а компьютер будет её

33
00:02:23,855 --> 00:02:27,100
делать сам, но тем не менее понимать,
что за вспомогательная регрессии, нужно.

34
00:02:27,100 --> 00:02:28,920
Поэтому давайте поставим
вспомогательный вопрос.

35
00:02:28,920 --> 00:02:34,165
Помимо тестирования собственно гипотезы
H0, давайте зададимся вопросом,

36
00:02:34,165 --> 00:02:41,630
как выглядит вспомогательная регрессия.

37
00:02:41,630 --> 00:02:46,075
Соответственно, во
вспомогательной регрессии

38
00:02:46,075 --> 00:02:48,378
нам нужны остатки от исходной модели.

39
00:02:48,378 --> 00:02:56,460
То есть происходит на самом деле за кадром
шаг 1 — оценивается исходная модель,

40
00:02:56,460 --> 00:03:01,760
и мы получаем из исходной
модели оценки ε_i с крышкой.

41
00:03:01,760 --> 00:03:08,090
И на шаге 2 строится вспомогательная
регрессия, а именно: остатки,

42
00:03:08,090 --> 00:03:12,914
полученные после оценивания исходной
модели, ε_i с крышкой в квадрате,

43
00:03:12,914 --> 00:03:18,070
— мы строим их регрессию на
исходные объясняющие переменные,

44
00:03:18,070 --> 00:03:22,750
то есть α1 + α2p_i +

45
00:03:22,750 --> 00:03:28,580
α3a_i + α4d_i,

46
00:03:28,580 --> 00:03:35,430
это расстояние до ближайшей остановки,
плюс квадраты исходных переменных.

47
00:03:35,430 --> 00:03:40,533
То есть пошли дальше коэффициенты:
α5 * p_i² + α6

48
00:03:40,533 --> 00:03:45,710
* a_i² + α7 * d_i²

49
00:03:45,710 --> 00:03:50,610
плюс — во вспомогательную регрессию, где
мы пытаемся понять, а не зависит ли вдруг

50
00:03:50,610 --> 00:03:56,669
от чего-нибудь разброс ε —
сюда ещё добавляются попарные,

51
00:03:56,669 --> 00:04:00,990
все попарные произведения всех
объясняющих переменных исходной модели,

52
00:04:00,990 --> 00:04:06,362
то есть + α8 * p_i

53
00:04:06,362 --> 00:04:12,630
* a_i + α9

54
00:04:12,630 --> 00:04:18,480
* p_i * d_i + α10

55
00:04:18,480 --> 00:04:23,165
* a_i * d_i + какая-то своя ошибка.

56
00:04:23,165 --> 00:04:27,850
Вот какая страшная модель с большим
количеством коэффициентов оценивается на

57
00:04:27,850 --> 00:04:31,300
втором шаге.

58
00:04:31,300 --> 00:04:35,601
Но, к счастью, компьютеру всё равно что
оценивать, всё оценивается быстро и легко.

59
00:04:35,601 --> 00:04:40,073
И из этой вспомогательной регрессии, — вот
в ней известно, что R² оказался

60
00:04:40,073 --> 00:04:44,317
равен 0,15, — и мы хотим понять,
много по-хорошему это или мало.

61
00:04:44,317 --> 00:04:48,838
Если эта величина большая,
то это говорит о том, что размер остатка,

62
00:04:48,838 --> 00:04:53,077
то есть квадрат ε с крышкой,
зависит от объясняющих

63
00:04:53,077 --> 00:04:57,124
переменных и имеет место
гетероскедастичность.

64
00:04:57,124 --> 00:05:03,474
Мы в лекционной части говорили, что нам
нужен тест Уайта, который устроен просто

65
00:05:03,474 --> 00:05:09,060
по принципу n помножить на R² во
вспомогательной регрессии на втором шаге.

66
00:05:09,060 --> 00:05:14,205
В нашем случае мы получаем:
200 помножить на 0,15.

67
00:05:14,205 --> 00:05:16,858
Получается 30.

68
00:05:16,858 --> 00:05:22,183
И при верной H0, при H0 статистика

69
00:05:22,183 --> 00:05:26,260
Уайта имеет хи-квадрат,

70
00:05:26,260 --> 00:05:32,200
асимптотический хи-квадрат распределение,
с количеством степеней свободы,

71
00:05:32,200 --> 00:05:36,890
равным количеству параметров в этой
вспомогательной регрессии минус 1.

72
00:05:36,890 --> 00:05:39,495
Тут 10 параметров минус 1 — это,
собственно,

73
00:05:39,495 --> 00:05:41,373
сколько здесь осмысленных регрессоров.

74
00:05:41,373 --> 00:05:42,320
Тут 9 регрессоров.

75
00:05:42,320 --> 00:05:46,500
Соответственно, это хи-квадрат
с 9 степенями свободы.

76
00:05:46,500 --> 00:05:51,030
График функции плотности
хи-квадрат с 9 степенями

77
00:05:51,030 --> 00:05:55,125
свободы имеет примерно вот такой вот вид.

78
00:05:55,125 --> 00:05:58,774
И есть некая хи-квадрат критическая.

79
00:05:58,774 --> 00:06:05,580
Хи-квадрат критическую мы можем установить
либо с помощью таблиц, либо с помощью R.

80
00:06:05,580 --> 00:06:08,871
Давайте мы напишем команду,
которая нам нужна.

81
00:06:08,871 --> 00:06:12,164
Нам нужна квантиль
хи-квадрат распределения.

82
00:06:12,164 --> 00:06:15,975
Если мы хотим, чтобы здесь было 5 %,
значит мы хотим,

83
00:06:15,975 --> 00:06:20,620
чтобы слева от хи-квадрат
критического было 95 % площади.

84
00:06:20,620 --> 00:06:25,743
Соответственно, чтобы узнать
хи-квадрат критическое, мы можем дать

85
00:06:25,743 --> 00:06:32,900
следующую команду R: квантиль
хи-квадрат распределения порядка 0,95.

86
00:06:32,900 --> 00:06:38,620
И если выдать эту команду в R,
то получится 16,9 примерно.

87
00:06:38,620 --> 00:06:43,830
Значит, хи-квадрат критическое равно 16,9,

88
00:06:43,830 --> 00:06:49,015
а наблюдаемое значение статистики
Уайта оказалось рано 30.

89
00:06:49,015 --> 00:06:54,200
Соответственно, у нас получается вывод,
что значение статистики Уайта слишком

90
00:06:54,200 --> 00:06:58,897
далеко от математического ожидания
хи-квадрат распределения,

91
00:06:58,897 --> 00:07:02,200
то есть оно вышло за
хи-квадрат критическое.

92
00:07:02,200 --> 00:07:05,040
Это говорит о том,
что R-квадрат слишком большой.

93
00:07:05,040 --> 00:07:09,166
Это говорит о том, что размер остатка,
измеряемый как ε с крышкой в квадрате,

94
00:07:09,166 --> 00:07:11,050
хорошо объясняется регрессорами,

95
00:07:11,050 --> 00:07:15,202
чего в условиях гомоскедастичности
не должно быть.

96
00:07:15,202 --> 00:07:22,241
Значит, мы получаем вывод, что H0 о том,
что у нас имеет место гомоскедастичность,

97
00:07:22,241 --> 00:07:26,639
эта H0 отвергается в пользу
H альтернативное о том,

98
00:07:26,639 --> 00:07:30,870
что имеет место условная
гетероскедастичность.

99
00:07:30,870 --> 00:07:34,443
Соответственно, вот здесь можно уточнить.

100
00:07:34,443 --> 00:07:39,938
Вот здесь у нас H0 не отвергается,
а вот здесь вот

101
00:07:39,938 --> 00:07:45,930
у нас H0 отвергается.

