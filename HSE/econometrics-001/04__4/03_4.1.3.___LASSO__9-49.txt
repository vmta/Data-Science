
﻿1
00:00:13,620 --> 00:00:18,848
Второй способ борьбы с
мультиколлинеарностью
– это включение штрафа в
сумму наименьших квадратов.
Соответственно, мы минимизируем не
просто сумму квадратов остатков, а мы
минимизируем сумму квадратов остатков плюс
штраф за слишком большие коэффициенты.
Мы штрафуем нашу модель за то,
что коэффициенты β с крышкой
оказываются слишком далеко от 0.
Соответственно, наиболее распространенными
являются 3 формы штрафа.
Первая форма штрафа,
где мы добавляем с некоторым весом λ сумму
квадратов оцененных коэффициентов,
этот метод называется ридж-регрессия.
Второй алгоритм называется LASSO,
где мы штрафуем нашу сумму
квадратов остатков на сумму модулей
оцененных коэффициентов,
опять же с весом λ.
И третий компромиссный вариант
– метод эластичной сети,
где мы штрафуем нашу сумму квадратов
остатков и на сумму модулей
оценок коэффициентов,
и на сумму квадратов оценок коэффициентов.
Соответственно, в каждом из
этих случаев оказывается,
что даже при наличии строгой
мультиколлинеарности,
оценка β с крышкой существует
единственная, и, таким образом,
можно решить проблему
мультиколлинеарности.
Давайте разберем простой
пример и в простейшей ситуации
получим β с крышкой – оценку,
оцененную методом ридж-регрессии.
Мы на примере сравним оценки
МНК и ридж-регрессии.
Рассмотрим такой простой
набор данных из 3 наблюдений.
y_i, x_i.
x_i принимает значение 1,
1 и 2, — три наблюдения,
и соответствующие им значения
зависимой переменной – 10, 20 и 30.
И давайте для начала найдем β с крышкой
обычного метода наименьших квадратов и β с
крышкой ридж-регрессии.
Соответственно, выписываем RSS.
RSS = ∑(y_i- y_i с крышкой) в квадрате, и,
конечно, нам нужна какая-то модель,
чтобы мы могли посчитать y с крышкой.
Ну, давайте предположим,
что у нас y_i с крышкой
= β с крышкой * x_i,
простая модель с одним коэффициентом.
Ну, то есть за кадром подразумевается,
что y_i = β * x_i + ε_i.
При таких предположениях
RSS = ∑(y_i- β с крышкой * x_i)².
И, соответственно, если раскрыть скобки,
то получится ∑(y_i²-
2y_i * β с крышкой * x_i
+ β с крышкой² * x_i²) Ну,
и получаем, что это равняется
= ∑y_i²- 2β с крышкой *
∑x_iy_i + β с крышкой
в квадрате * ∑x_i² Соответственно,
метод наименьших квадратов состоит в том,
что минимизируется просто RSS,
Если мы возьмем производную от RSS, ну,
давайте назовем эту функцию
Q(β с крышкой) для краткости,
соответственно, Q' Q' =
-2∑x_iy_i + 2β с крышкой*∑x_i²
Q' = -2∑x_iy_i + 2β с крышкой * ∑x_i².
Ну, и, соответственно, получается,
что β с крышкой = ∑x_iy_i/∑x_i².
В нашем случае, если мы подставим наши x_i,
y_i, то мы получим,
что сумма квадратов отдельных x – это 1 в
квадрате + 1 в квадрате + 2 в квадрате,
а сумма попарных произведений – это 10,
20 и 30 * 2 = 60.
Соответственно, мы получим 90/6 = 15,
это β с крышкой обычного
метода наименьших квадратов.
В чем состоит ридж-регрессия?
В ридж-регрессии мы минимизируем не RSS,
а RSS плюс некий штраф за
размер величины β с крышкой.
Есть некий штрафной коэффициент, штрафной
коэффициент λ, коэффициент штрафа,
и ридж-регрессия минимизирует
RSS RSS + λ*β с крышкой в квадрате.
Поскольку у нас в модели только один β,
то мы складываем только один
квадрат этого коэффициента.
Ну, соответственно,
давайте минимизировать вот эту функцию,
минимизируем, подбирая β с крышкой.
Соответственно, если я
обозначу эту функцию
Q_r Q_r(β
с крышкой),
начинаться она будет точно также,
а еще в конце будет добавлена
λ * β с крышкой в квадрате.
То есть это будет
∑y_i²- 2β с крышкой
* ∑x_iy_i + β с крышкой в квадрате *
∑x_i² + λ * β с крышкой ²,
ну, только здесь добавится довесочек,
этот штраф за размер β с крышкой.
И, соответственно,
производная от RSS плюс штраф
для ридж-регрессии по
началу будет точно такая
же: 2∑x_iy_i + 2β с крышкой * ∑x_i²
а здесь еще добавится + 2β с крышкой*λ Ну,
и,
соответственно, приравняв производную к 0,
если я Q_r' приравняю к 0 и выражу
отсюда β с крышкой, то у меня получится,
что λ прибавляется к ∑x_i².
Вот это, можно привести подобные
слагаемые с β с крышкой,
и получится в результате,
что β с крышкой =
∑x_iy_i/∑x_i² +
λ β с крышкой = ∑x_iy_i/∑x_i² + λ.
И давайте для примера
возьмем какое-нибудь λ.
Ну, если я возьму, например, λ = 240,
то у меня получится 90,
мы уже считали ∑x_iy_i,
90/6 + 240 = 90/246,
то есть гораздо меньше 1.
То есть чем больше λ,
тем меньше будет β с крышкой,
тем β с крышкой будет ближе к 0,
потому что λ стоит в знаменателе.
И мы видим, что в такой простой
модели в некоторых ситуациях оценки
ридж-регресии могут не существовать.
Ну, например, если бы все x_i = 0,
то тогда в знаменателе в
формуле для обычного МНК стоял бы 0,
а в β с крышкой ридж-регрессии
в знаменателе прибавляется λ,
и если λ – это положительный коэффициент,
то даже если все x = 0,
даже если МНК оценка не существует, β с
крышкой ридж-регрессия будет существовать.
И чем больше этот штрафной коэффициент λ,
тем меньше будет β с крышкой.
Мы рассмотрели введение штрафа
в функцию МНК как один из
способов борьбы с мультиколлинеарностью.
При этом надо осознавать,
что за этот способ приходится платить
при использовании ридж-регрессии,
LASSO-регрессии или
метода эластичной сети.
Мы уже не можем строить доверительные
интервалы или проверять гипотезы, но,
тем не менее, мы можем получить
осмысленные оценки коэффициентов,
которые были похожи на то,
что нас интересует.
А теперь мы перейдем к следующему
большому сюжету сегодняшней лекции
– это метод главных компонент.
Он может использоваться как сам по себе,
так и как одно из средств
борьбы с мультиколлинеарностью.

