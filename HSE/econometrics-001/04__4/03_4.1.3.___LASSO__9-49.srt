1
00:00:13,620 --> 00:00:18,848
Второй способ борьбы с
мультиколлинеарностью

2
00:00:18,848 --> 00:00:23,763
– это включение штрафа в
сумму наименьших квадратов.

3
00:00:23,763 --> 00:00:30,290
Соответственно, мы минимизируем не
просто сумму квадратов остатков, а мы

4
00:00:30,290 --> 00:00:35,190
минимизируем сумму квадратов остатков плюс
штраф за слишком большие коэффициенты.

5
00:00:35,190 --> 00:00:37,248
Мы штрафуем нашу модель за то,

6
00:00:37,248 --> 00:00:41,530
что коэффициенты β с крышкой
оказываются слишком далеко от 0.

7
00:00:41,530 --> 00:00:46,320
Соответственно, наиболее распространенными
являются 3 формы штрафа.

8
00:00:46,320 --> 00:00:51,472
Первая форма штрафа,
где мы добавляем с некоторым весом λ сумму

9
00:00:51,472 --> 00:00:56,480
квадратов оцененных коэффициентов,
этот метод называется ридж-регрессия.

10
00:00:56,480 --> 00:00:59,418
Второй алгоритм называется LASSO,

11
00:00:59,418 --> 00:01:04,842
где мы штрафуем нашу сумму
квадратов остатков на сумму модулей

12
00:01:04,842 --> 00:01:09,668
оцененных коэффициентов,
опять же с весом λ.

13
00:01:09,668 --> 00:01:14,150
И третий компромиссный вариант
– метод эластичной сети,

14
00:01:14,150 --> 00:01:20,695
где мы штрафуем нашу сумму квадратов
остатков и на сумму модулей

15
00:01:20,695 --> 00:01:25,710
оценок коэффициентов,
и на сумму квадратов оценок коэффициентов.

16
00:01:25,710 --> 00:01:30,156
Соответственно, в каждом из
этих случаев оказывается,

17
00:01:30,156 --> 00:01:34,261
что даже при наличии строгой
мультиколлинеарности,

18
00:01:34,261 --> 00:01:39,407
оценка β с крышкой существует
единственная, и, таким образом,

19
00:01:39,407 --> 00:01:43,530
можно решить проблему
мультиколлинеарности.

20
00:01:43,530 --> 00:01:49,098
Давайте разберем простой
пример и в простейшей ситуации

21
00:01:49,098 --> 00:01:54,111
получим β с крышкой – оценку,
оцененную методом ридж-регрессии.

22
00:01:54,111 --> 00:01:57,905
Мы на примере сравним оценки
МНК и ридж-регрессии.

23
00:01:57,905 --> 00:02:02,280
Рассмотрим такой простой
набор данных из 3 наблюдений.

24
00:02:02,280 --> 00:02:07,300
y_i, x_i.

25
00:02:07,300 --> 00:02:13,163
x_i принимает значение 1,
1 и 2, — три наблюдения,

26
00:02:13,163 --> 00:02:18,467
и соответствующие им значения
зависимой переменной – 10, 20 и 30.

27
00:02:18,467 --> 00:02:22,881
И давайте для начала найдем β с крышкой

28
00:02:22,881 --> 00:02:27,780
обычного метода наименьших квадратов и β с

29
00:02:27,780 --> 00:02:32,580
крышкой ридж-регрессии.

30
00:02:32,580 --> 00:02:37,917
Соответственно, выписываем RSS.

31
00:02:37,917 --> 00:02:42,491
RSS = ∑(y_i- y_i с крышкой) в квадрате, и,

32
00:02:42,491 --> 00:02:47,949
конечно, нам нужна какая-то модель,

33
00:02:47,949 --> 00:02:51,843
чтобы мы могли посчитать y с крышкой.

34
00:02:51,843 --> 00:02:58,340
Ну, давайте предположим,
что у нас y_i с крышкой

35
00:02:58,340 --> 00:03:03,940
= β с крышкой * x_i,
простая модель с одним коэффициентом.

36
00:03:03,940 --> 00:03:10,510
Ну, то есть за кадром подразумевается,
что y_i = β * x_i + ε_i.

37
00:03:10,510 --> 00:03:16,080
При таких предположениях

38
00:03:16,080 --> 00:03:21,650
RSS = ∑(y_i- β с крышкой * x_i)².

39
00:03:21,650 --> 00:03:25,980
И, соответственно, если раскрыть скобки,

40
00:03:25,980 --> 00:03:30,760
то получится ∑(y_i²-

41
00:03:30,760 --> 00:03:35,770
2y_i * β с крышкой * x_i

42
00:03:35,770 --> 00:03:40,700
+ β с крышкой² * x_i²) Ну,

43
00:03:40,700 --> 00:03:45,770
и получаем, что это равняется

44
00:03:45,770 --> 00:03:50,337
= ∑y_i²- 2β с крышкой *

45
00:03:50,337 --> 00:03:55,990
∑x_iy_i + β с крышкой

46
00:03:55,990 --> 00:04:01,045
в квадрате * ∑x_i² Соответственно,

47
00:04:01,045 --> 00:04:07,780
метод наименьших квадратов состоит в том,
что минимизируется просто RSS,

48
00:04:07,780 --> 00:04:14,030
Если мы возьмем производную от RSS, ну,

49
00:04:14,030 --> 00:04:19,630
давайте назовем эту функцию
Q(β с крышкой) для краткости,

50
00:04:19,630 --> 00:04:25,847
соответственно, Q' Q' =

51
00:04:25,847 --> 00:04:31,690
-2∑x_iy_i + 2β с крышкой*∑x_i²

52
00:04:31,690 --> 00:04:36,360
Q' = -2∑x_iy_i + 2β с крышкой * ∑x_i².

53
00:04:36,360 --> 00:04:40,579
Ну, и, соответственно, получается,

54
00:04:40,579 --> 00:04:47,670
что β с крышкой = ∑x_iy_i/∑x_i².

55
00:04:47,670 --> 00:04:54,196
В нашем случае, если мы подставим наши x_i,
y_i, то мы получим,

56
00:04:54,196 --> 00:05:01,070
что сумма квадратов отдельных x – это 1 в
квадрате + 1 в квадрате + 2 в квадрате,

57
00:05:01,070 --> 00:05:05,736
а сумма попарных произведений – это 10,

58
00:05:05,736 --> 00:05:09,046
20 и 30 * 2 = 60.

59
00:05:09,046 --> 00:05:17,449
Соответственно, мы получим 90/6 = 15,

60
00:05:17,449 --> 00:05:21,990
это β с крышкой обычного
метода наименьших квадратов.

61
00:05:21,990 --> 00:05:25,848
В чем состоит ридж-регрессия?

62
00:05:25,848 --> 00:05:30,876
В ридж-регрессии мы минимизируем не RSS,

63
00:05:30,876 --> 00:05:35,795
а RSS плюс некий штраф за
размер величины β с крышкой.

64
00:05:35,795 --> 00:05:43,669
Есть некий штрафной коэффициент, штрафной
коэффициент λ, коэффициент штрафа,

65
00:05:43,669 --> 00:05:50,146
и ридж-регрессия минимизирует
RSS RSS + λ*β с крышкой в квадрате.

66
00:05:50,146 --> 00:05:53,500
Поскольку у нас в модели только один β,

67
00:05:53,500 --> 00:05:58,267
то мы складываем только один
квадрат этого коэффициента.

68
00:05:58,267 --> 00:06:04,360
Ну, соответственно,
давайте минимизировать вот эту функцию,

69
00:06:04,360 --> 00:06:10,315
минимизируем, подбирая β с крышкой.

70
00:06:10,315 --> 00:06:14,640
Соответственно, если я
обозначу эту функцию

71
00:06:14,640 --> 00:06:19,828
Q_r Q_r(β

72
00:06:19,828 --> 00:06:24,020
с крышкой),
начинаться она будет точно также,

73
00:06:24,020 --> 00:06:28,413
а еще в конце будет добавлена
λ * β с крышкой в квадрате.

74
00:06:28,413 --> 00:06:34,323
То есть это будет
∑y_i²- 2β с крышкой

75
00:06:34,323 --> 00:06:41,441
* ∑x_iy_i + β с крышкой в квадрате *
∑x_i² + λ * β с крышкой ²,

76
00:06:41,441 --> 00:06:46,211
ну, только здесь добавится довесочек,
этот штраф за размер β с крышкой.

77
00:06:46,211 --> 00:06:51,784
И, соответственно,
производная от RSS плюс штраф

78
00:06:51,784 --> 00:06:57,043
для ридж-регрессии по
началу будет точно такая

79
00:06:57,043 --> 00:07:02,810
же: 2∑x_iy_i + 2β с крышкой * ∑x_i²

80
00:07:02,810 --> 00:07:07,768
а здесь еще добавится + 2β с крышкой*λ Ну,
и,

81
00:07:07,768 --> 00:07:12,930
соответственно, приравняв производную к 0,

82
00:07:12,930 --> 00:07:18,307
если я Q_r' приравняю к 0 и выражу

83
00:07:18,307 --> 00:07:23,400
отсюда β с крышкой, то у меня получится,

84
00:07:23,400 --> 00:07:27,197
что λ прибавляется к ∑x_i².

85
00:07:27,197 --> 00:07:32,290
Вот это, можно привести подобные
слагаемые с β с крышкой,

86
00:07:32,290 --> 00:07:36,734
и получится в результате,
что β с крышкой =

87
00:07:36,734 --> 00:07:41,616
∑x_iy_i/∑x_i² +

88
00:07:41,616 --> 00:07:46,340
λ β с крышкой = ∑x_iy_i/∑x_i² + λ.

89
00:07:46,340 --> 00:07:49,566
И давайте для примера
возьмем какое-нибудь λ.

90
00:07:49,566 --> 00:07:54,045
Ну, если я возьму, например, λ = 240,

91
00:07:54,045 --> 00:08:01,271
то у меня получится 90,
мы уже считали ∑x_iy_i,

92
00:08:01,271 --> 00:08:07,520
90/6 + 240 = 90/246,

93
00:08:07,520 --> 00:08:13,205
то есть гораздо меньше 1.

94
00:08:13,205 --> 00:08:17,985
То есть чем больше λ,
тем меньше будет β с крышкой,

95
00:08:17,985 --> 00:08:23,608
тем β с крышкой будет ближе к 0,
потому что λ стоит в знаменателе.

96
00:08:23,608 --> 00:08:28,590
И мы видим, что в такой простой
модели в некоторых ситуациях оценки

97
00:08:28,590 --> 00:08:31,695
ридж-регресии могут не существовать.

98
00:08:31,695 --> 00:08:36,665
Ну, например, если бы все x_i = 0,
то тогда в знаменателе в

99
00:08:36,665 --> 00:08:41,090
формуле для обычного МНК стоял бы 0,

100
00:08:41,090 --> 00:08:48,730
а в β с крышкой ридж-регрессии
в знаменателе прибавляется λ,

101
00:08:48,730 --> 00:08:53,790
и если λ – это положительный коэффициент,
то даже если все x = 0,

102
00:08:53,790 --> 00:08:58,830
даже если МНК оценка не существует, β с
крышкой ридж-регрессия будет существовать.

103
00:08:58,830 --> 00:09:03,800
И чем больше этот штрафной коэффициент λ,
тем меньше будет β с крышкой.

104
00:09:03,800 --> 00:09:08,380
Мы рассмотрели введение штрафа
в функцию МНК как один из

105
00:09:08,380 --> 00:09:11,267
способов борьбы с мультиколлинеарностью.

106
00:09:11,267 --> 00:09:12,720
При этом надо осознавать,

107
00:09:12,720 --> 00:09:16,835
что за этот способ приходится платить
при использовании ридж-регрессии,

108
00:09:16,835 --> 00:09:19,626
LASSO-регрессии или
метода эластичной сети.

109
00:09:19,626 --> 00:09:24,132
Мы уже не можем строить доверительные
интервалы или проверять гипотезы, но,

110
00:09:24,132 --> 00:09:27,274
тем не менее, мы можем получить
осмысленные оценки коэффициентов,

111
00:09:27,274 --> 00:09:29,470
которые были похожи на то,
что нас интересует.

112
00:09:29,470 --> 00:09:33,955
А теперь мы перейдем к следующему
большому сюжету сегодняшней лекции

113
00:09:33,955 --> 00:09:36,090
– это метод главных компонент.

114
00:09:36,090 --> 00:09:39,351
Он может использоваться как сам по себе,

115
00:09:39,351 --> 00:09:44,020
так и как одно из средств
борьбы с мультиколлинеарностью.

