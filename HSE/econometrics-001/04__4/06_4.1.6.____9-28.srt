1
00:00:08,230 --> 00:00:12,229
В общем

2
00:00:12,229 --> 00:00:17,564
случае мы получаем,

3
00:00:17,564 --> 00:00:22,571
что у нас k главных компонент, то есть
главных компонент ровно столько же,

4
00:00:22,571 --> 00:00:24,690
сколько исходных объясняющих переменных.

5
00:00:24,690 --> 00:00:29,958
И каждая главная компонента выражена как
линейная комбинация исходных регрессоров.

6
00:00:29,958 --> 00:00:33,985
Первая главная компонента,
у нее свои веса перед каждым регрессором.

7
00:00:33,985 --> 00:00:37,090
Вторая главная компонента,
у нее свои веса и так далее.

8
00:00:37,090 --> 00:00:41,780
При этом главные компоненты обладают
свойством, что выборочные корреляции между

9
00:00:41,780 --> 00:00:46,155
двумя разными новыми переменными,
главными компонентами, равна нулю.

10
00:00:46,155 --> 00:00:50,675
И второе очень важное красивое свойство,
что суммарная дисперсия,

11
00:00:50,675 --> 00:00:54,775
суммарный разброс всех
исходных регрессоров

12
00:00:54,775 --> 00:00:59,642
равен суммарному разбросу
всех главных компонент.

13
00:00:59,642 --> 00:01:02,066
Это очень важное свойство.

14
00:01:02,066 --> 00:01:07,631
Поскольку главные компоненты подбираются
так, чтобы дисперсия выборочная

15
00:01:07,631 --> 00:01:12,982
каждой главной компоненты на каждом шаге
была максимально возможной при условии,

16
00:01:12,982 --> 00:01:18,062
что они независимы друг от друга,
то поэтому получается, зачастую,

17
00:01:18,062 --> 00:01:22,440
что первая главная компонента,
ее выборочная дисперсия,

18
00:01:22,440 --> 00:01:27,155
вбирает в себя существенную
часть суммарного разброса,

19
00:01:27,155 --> 00:01:31,870
суммарной дисперсии всех исходных
переменных, всех исходных иксов.

20
00:01:31,870 --> 00:01:37,964
То есть оказывается, что можно
заменить все k исходных переменных.

21
00:01:37,964 --> 00:01:43,113
Ну, если повезет, то на одну или на две,
ну, возможно, на большее число, но тем не

22
00:01:43,113 --> 00:01:48,730
менее гораздо меньше, чем изначально было
новых искусственных главных компонент.

23
00:01:48,730 --> 00:01:54,253
То есть меньшее количество переменных
несет в себе почти всю информацию,

24
00:01:54,253 --> 00:01:57,538
которая содержится в
исходном наборе данных.

25
00:01:57,538 --> 00:02:02,989
То есть, скажем, первые две главных
компоненты, первая главная компонента

26
00:02:02,989 --> 00:02:08,341
№ 1 и № 2 могут вбирать в себя
90 % общей дисперсии 20-ти

27
00:02:08,341 --> 00:02:13,458
или даже большего количества
исходных объясняющих переменных.

28
00:02:13,458 --> 00:02:19,026
Отметим, что знание линейной
алгебры в данном случае позволяет

29
00:02:19,026 --> 00:02:23,809
легко вывести общие формулы
для каждой главной компоненты.

30
00:02:23,809 --> 00:02:28,295
Оказывается, что квадрат длины
главной компоненты — это

31
00:02:28,295 --> 00:02:33,510
соответствующее собственное
число матрицы X'Х,

32
00:02:33,510 --> 00:02:38,378
а веса с которыми исходные объясняющие
переменные входят в главную

33
00:02:38,378 --> 00:02:43,238
компоненту — это есть
собственный вектор матрицы X'X.

34
00:02:43,238 --> 00:02:46,790
Что же дают нам главные компоненты?

35
00:02:46,790 --> 00:02:51,233
Самое главное,
что позволяют сделать главные компоненты,

36
00:02:51,233 --> 00:02:54,660
это — визуализировать
сложный набор данных.

37
00:02:54,660 --> 00:02:59,130
Если у вас есть тысяча переменных,
и вы не знаете, как,

38
00:02:59,130 --> 00:03:01,750
какие графики можно построить.

39
00:03:01,750 --> 00:03:06,270
Если у вас тысяча переменных, то хотя бы
даже попарных графиков будет очень много,

40
00:03:06,270 --> 00:03:08,812
то можно поступить следующим образом.

41
00:03:08,812 --> 00:03:13,030
Выбрать несколько первых
главных компонент, скажем,

42
00:03:13,030 --> 00:03:18,950
две или три из тысячи, и построить
графики ваших наблюдений в осях.

43
00:03:18,950 --> 00:03:23,680
По одной оси отложить первую главную
компоненту, по другой — вторую.

44
00:03:23,680 --> 00:03:28,030
И, возможно, вы уже увидите что-то
интересное в вашем наборе данных.

45
00:03:28,030 --> 00:03:33,544
Вторая задача, неразрывно
связанная с первой, это — то, что

46
00:03:33,544 --> 00:03:38,630
метод главных компонент позволяет оставить
наиболее информативные переменные.

47
00:03:38,630 --> 00:03:43,722
Если у вас есть несколько переменных,
одна из которых практически не меняется,

48
00:03:43,722 --> 00:03:48,231
а вторая и третья меняются существенно,
то метод главных компонент позволит

49
00:03:48,231 --> 00:03:52,720
выбросить переменную, которая практически
не меняется и практически не несет в себе

50
00:03:52,720 --> 00:03:56,992
полезной информации, и оставить
только информативные переменные,

51
00:03:56,992 --> 00:04:00,730
которые сильнее всего
дифференцируют ваши наблюдения.

52
00:04:00,730 --> 00:04:06,670
В-третьих, метод главных компонент
позволяет увидеть особенные наблюдения.

53
00:04:06,670 --> 00:04:09,874
Если мы увидим, что у какого-то наблюдения

54
00:04:09,874 --> 00:04:14,765
главная компонента принимает очень
непохожее на другие наблюдения значение,

55
00:04:14,765 --> 00:04:18,510
это будет говорить о том, что
наблюдение сильно отличается от других.

56
00:04:18,510 --> 00:04:19,242
И конечно,

57
00:04:19,242 --> 00:04:24,160
метод главных компонент позволяет
перейти к некоррелированным переменным.

58
00:04:24,160 --> 00:04:28,450
А некоррелированные переменные означают,
в частности, автоматически,

59
00:04:28,450 --> 00:04:31,690
отсутствие какой-бы то ни
было мультиколлинеарности.

60
00:04:31,690 --> 00:04:35,781
На практике при использовании
метода главных компонент,

61
00:04:35,781 --> 00:04:39,060
можно столкнуться с
некоторыми трудностями.

62
00:04:39,060 --> 00:04:42,811
Первая трудность — это
принципиально разные переменные,

63
00:04:42,811 --> 00:04:45,980
которые измеряются в
разных единицах измерения.

64
00:04:45,980 --> 00:04:51,110
Ну скажем, одна переменная — это валютный
курс, а вторая — это объем торгов.

65
00:04:51,110 --> 00:04:57,243
В этой ситуации, поскольку они
измеряются в разных единицах измерения,

66
00:04:57,243 --> 00:05:03,780
то разброс одной переменной просто не
сопоставим с разбросом другой переменной.

67
00:05:03,780 --> 00:05:08,650
И вторая потенциальная трудность
— это бездумное применение

68
00:05:08,650 --> 00:05:13,666
метода главных компонент при отборе
переменных для построения регрессии.

69
00:05:13,666 --> 00:05:18,538
Из-за того, что метод главных компонент
выбирает самую изменчивую переменную

70
00:05:18,538 --> 00:05:22,825
или несколько самых изменчивых,
он не дает никакой гарантии,

71
00:05:22,825 --> 00:05:29,660
что будет выбрана переменная сильнее
всего связанная с объясняемой переменной.

72
00:05:29,660 --> 00:05:33,607
И тут надо быть осторожным,
надо понимать, что, действительно,

73
00:05:33,607 --> 00:05:36,444
то что меняется сильнее всего в данных,
это то,

74
00:05:36,444 --> 00:05:39,760
что сильнее всего объясняет
зависимую переменную.

75
00:05:39,760 --> 00:05:44,543
Соответственно, проблема разных единиц
измерения у переменных решается

76
00:05:44,543 --> 00:05:45,670
довольно легко.

77
00:05:45,670 --> 00:05:50,438
Хотя мы понимаем, что простое
применение метода главных компонент в

78
00:05:50,438 --> 00:05:54,053
лоб приведет к тому, что будет
выбрана самая изменчивая переменная,

79
00:05:54,053 --> 00:05:55,710
не факт что сама информативная.

80
00:05:55,710 --> 00:05:58,531
Возможно, наоборот, самая шумная.

81
00:05:58,531 --> 00:06:02,115
Однако, легко скорректировать
переменные так,

82
00:06:02,115 --> 00:06:06,150
чтобы метод главных компонент
был по-прежнему применим.

83
00:06:06,150 --> 00:06:10,004
А именно можно нормировать
каждую переменную,

84
00:06:10,004 --> 00:06:15,235
то есть из каждой рассматриваемой
переменной вычесть ее среднее

85
00:06:15,235 --> 00:06:21,520
арифмитическое и поделить на
стандартную ошибку данной переменной.

86
00:06:21,520 --> 00:06:26,475
Соответственно, при такой нормировке у
нас окажется, что выборочная дисперсия

87
00:06:26,475 --> 00:06:31,430
всех новых переменных одинакова,
и к новым нормированным переменным

88
00:06:31,430 --> 00:06:36,690
уже можно спокойно применять
метод главных компонент.

89
00:06:36,690 --> 00:06:39,701
И вторая трудность,

90
00:06:39,701 --> 00:06:45,441
это — вопрос применения метода
главных компонент в сочетании

91
00:06:45,441 --> 00:06:49,750
с последующей регрессией зависимой
переменной на главные компоненты.

92
00:06:49,750 --> 00:06:52,368
Обычно процедура устроена
следующим образом.

93
00:06:52,368 --> 00:06:55,690
Из большого количество сильно
коррелированных переменных с

94
00:06:55,690 --> 00:07:00,310
помощью метода главных компонент отбирают
несколько главных компонент, скажем, две.

95
00:07:00,310 --> 00:07:04,430
И дальше строят регрессии
не на исходные переменные,

96
00:07:04,430 --> 00:07:08,160
а на главные компоненты.

97
00:07:08,160 --> 00:07:12,975
Соответственно, тут возникают
некоторые проблемы с интерпретацией,

98
00:07:12,975 --> 00:07:20,025
потому что главные компоненты,
они не несут прямого смысла.

99
00:07:20,025 --> 00:07:24,291
Можно попытаться выглядеть смысл в главных
компонентах, но тем не менее четкого,

100
00:07:24,291 --> 00:07:29,124
ясного однозначного смысла каждая
главная компонента не несет.

101
00:07:29,124 --> 00:07:34,822
И, опять же, надо быть очень
уверенным в том, что изменчивость

102
00:07:34,822 --> 00:07:40,510
переменных объясняющих связана с
изменчивостью объясняемой переменной.

103
00:07:40,510 --> 00:07:47,540
Итак, в этой лекции мы рассмотрели метод
главных компонент и мультиколлинеарность.

104
00:07:47,540 --> 00:07:50,808
Отметим, что метод главных компонент,
прежде всего,

105
00:07:50,808 --> 00:07:55,723
полезен сам по себе, он позволяет снизить
размерность имеющегося набора данных,

106
00:07:55,723 --> 00:08:00,622
и от большого числа переменных,
которые трудно интерпретировать, перейти к

107
00:08:00,622 --> 00:08:06,605
меньшему числу переменных, которые,
если повезет, легко интерпретировать.

108
00:08:06,605 --> 00:08:10,720
Ну и, в частности, он может использоваться
при борьбе с мультиколлинеарностью.

109
00:08:10,720 --> 00:08:14,161
Еще раз подведем итог — что
такое мультиколлинеарность.

110
00:08:14,161 --> 00:08:19,352
Мультиколлинеарность — это зависимость,
идеальная или не совсем идеальная,

111
00:08:19,352 --> 00:08:24,313
между объясняющими переменными,
которые приводят прежде всего к тому,

112
00:08:24,313 --> 00:08:28,040
что у нас очень высокие
стандартные ошибки коэффициентов.

113
00:08:28,040 --> 00:08:30,994
Стандартные ошибки
высокие приводят к тому,

114
00:08:30,994 --> 00:08:35,545
что у нас широкие доверительные интервалы
для коэффициентов, и, соответственно,

115
00:08:35,545 --> 00:08:40,369
мы не можем отвергнуть гипотезу о том, что
коэффициент равен нулю, то есть не можем

116
00:08:40,369 --> 00:08:46,700
понять — есть ли зависимость на самом деле
от объясняющей переменной данной или нет.

117
00:08:46,700 --> 00:08:53,276
И, соответственно, в качестве способов
борьбы с мультиколлинеарностью,

118
00:08:53,276 --> 00:08:59,440
мы предложили либо в жертву
принести несмещенность оценок

119
00:08:59,440 --> 00:09:04,400
и отказаться от включения нескольких
переменных в рассматриваемую модель,

120
00:09:04,400 --> 00:09:07,999
либо использовать метод
наименьших квадратов со штрафом,

121
00:09:07,999 --> 00:09:11,935
в частности алгоритм ридж-
регрессии или алгоритм LASSO.

122
00:09:11,935 --> 00:09:15,865
В следующих лекциях мы поговорим
о более серьезных проблемах,

123
00:09:15,865 --> 00:09:18,920
о более серьезных нарушениях
предпосылки теоремы Гаусса-Маркова.

124
00:09:18,920 --> 00:09:22,990
А именно о гетероскедастичности
и об автокорреляции.

