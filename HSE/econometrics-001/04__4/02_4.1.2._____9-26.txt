
﻿1
00:00:13,250 --> 00:00:18,545
Что же можно сделать с
мультиколлинеарностью?
Во-первых, нужно осознавать, что, может
быть, эта мультиколлинеарность не так уж и
страшна, потому что теорема Гаусса-Маркова
по-прежнему в силе, соответственно,
оценки, которые мы получаем,
— они несмещённые,
и они обладают наименьшей дисперсией
среди несмещённых оценок.
И поэтому если вас интересуют прогнозы и
вам неважна интерпретация коэффициентов,
то на мультиколлинеарность можно
вообще не обращать внимания.
Более того, если вот вам нужно оценить
именно коэффициенты при именно ваших
переменных, то, к сожалению, ничего
лучше в условиях мультиколлинеарности
придумать нельзя, потому что полученные
оценки являются несмещёнными,
с наименьшей дисперсией
среди несмещённых оценок.
Однако возможно вы не совсем уверены
в форме спецификации модели,
вы можете не быть уверены в том,
что ваш показатель именно зависит и от
валютного курса на момент начала торгов,
и от валютного курса на момент окончания
торгов, вы можете просто предполагать,
что он зависит от какого-то валютного
курса, но вы не знаете, от какого.
Тогда вы можете пожертвовать
одной из переменных,
которые участвуют в линейном соотношении,
и выкинуть её.
И есть, конечно, ещё идеальный вариант,
который, как правило, недоступен,
поэтому это вариант-мечта — заполучить
побольше наблюдений для оценивания модели.
Это тоже метод борьбы с
мультиколлинеарностью.
Итак, первый метод — это
ничего не делать и получить
и так несмещённые оценки с
наименьшей возможной дисперсией.
Второй метод — это пожертвовать
несмещённостью оценок с целью уменьшить их
дисперсию.
И третий.
третья возможность — это
получить побольше наблюдений.
Как можно пожертвовать несмещённостью
оценок и снизить дисперсию?
Есть два основных пути.
Первый путь, как мы уже сказали,
— это выкинуть несколько переменных,
избавившись от нестрогого
линейного соотношения.
И второй путь — это ввести штраф
в метод наименьших квадратов,
то есть рассмотреть метод
наименьших квадратов со штрафом.
Давайте рассмотрим простое упражнение,
где мы, зная коэффициенты R-квадрат
во всех регрессиях всех объясняющих
переменных на остальные, попробуем понять,
имеет ли место у нас мультиколлинеарность,
и между какими переменными есть линейная
взаимосвязь, какие переменные имеет смысл,
возможно, выкинуть из рассмотрения.
Давайте на примере посмотрим, как
коэффициенты вздутия дисперсии позволяют
определить наличие мультиколлинеарности
в модели линейной регрессии и выяснить,
между какими переменными есть зависимость.
Давайте рассмотрим следующий пример.
Исследователь оценил модель зависимости
y от нескольких объясняющих переменных,
y_i = β₁ + β₂ *
x_i + β₃ *
z_i + β_4 * w_i
+ случайная составляющая ε_i.
И между регрессорами,
между объясняющими переменными x, z и w,
потенциально ожидается
мультиколлинеарность.
И известно,
что при построении регрессора x на
остальные z и w R-квадрат второй во
вспомогательной регрессии равен 0,5.
То есть это имеется в виду R-квадрат в
регрессии x на остальные регрессоры,
то есть в регрессии (x_i = γ₁ + γ₂ *
zi + γ₃ * w_i + какая-то
своя случайная ошибка.
R-квадрат во вспомогательной
регрессии z на остальные
регрессоры равен 0,95.
То есть имеется ввиду,
что это R-квадрат в регрессии z_i
= γ₁ + γ₂ * x_i + γ₃ * w_i + u_i.
Здесь я сознательно,
чтобы не вводить новые буквы,
пишу одинаковые гаммы, хотя,
конечно, результаты оценивания
регрессии x на остальные объясняющие
переменные и регрессии z на остальные
объясняющие переменные, конечно же,
отличаются, но поскольку мне они неважны,
то поэтому я для экономии букв буду
использовать одинаковые буквы,
хотя, конечно,
эта γ₁ и эта γ₁ — это разные константы.
И при регрессии третьего регрессора
R-квадрат четыре оказался равен 0,98.
То есть это регрессия объясняющей
переменной w_i на остальные
объясняющие переменные, w_i = γ₁ + γ₂ * x_i
+ γ₃ * z_i + u_i.
Соответственно, мне нужно выяснить,
есть ли мультиколлинеарность
в этой модели,
и между какими переменными
есть линейная связь.
Для ответа на эти вопросы мы воспользуемся
коэффициентом вздутия дисперсии.
Я напомню, что коэффициент вздутия
дисперсии, он считается отдельно
для каждого регрессора,
для каждой объясняющей переменной.
Соответственно, коэффициент вздутия
дисперсии считается как единичка делить на
один минус R² в соответствующей
вспомогательной регрессии.
Мы легко их рассчитываем.
Коэффициент вздутия дисперсии для
объясняющей переменной x равен 1/ (1-
0,5) = 2.
Коэффициент вздутия дисперсии
для переменной z равен
1/ (1- 0,95), = 20.
И коэффициент вздутия
дисперсии для переменной
w равен 1/ (1- 0,98),
= 50.
Ещё раз хочу подчеркнуть,
что строгой границы для
показателя вздутия дисперсии нет,
но, считается, что где-то значения
больше десяти уже являются
мягким нестрогим признаком
наличия мультиколлинеарности.
Соответственно, мы видим в данном случае,
что переменная x
довольно плохо объясняется остальными
переменными, то есть переменная x
— её можно считать линейно
независимой от остальных переменных.
А вот переменные z и w
достаточно сильно зависят от
других регрессоров, то есть у них
высокий R², соответственно,
высокий коэффициент вздутия дисперсии,
существенно больше 10-ти.
Соответственно, в данном случае мы видим,
что в модели имеет место
мультиколлинеарность.
Эта мультиколлинеарность вызвана тем,
что z хорошо объясняется другими
регрессорами и w хорошо
объясняется другими регрессорами.
Ну и давайте попробуем ответить теперь
на второй вопрос: какие же переменные,
какие регрессоры зависимы между собой?
Ну если их, условно говоря,
нарисовать вот так вот точечками:
вот это x, это z, это w.
Вот мы видим,
что x плохо зависит от z и w.
Плохо в том смысле, что зависимость
далека от линейной: R² маленький,
коэффициент вздутия дисперсии маленький.
То есть x линейно не объясняется с z и w.
Однако z линейно объясняется x и w.
Но z не может объясняться линейно x,
поэтому получается,
что связь линейная есть между z и w.
Мы так можем интерпретировать то,
что коэффициент вздутия дисперсии
при z большой, коэффициент
вздутия дисперсии при w большой,
однако x от них, от z и w, не зависит.
Соответственно, по имеющимся
вспомогательным регрессиям мы можем
сделать вывод, что да,
мультиколлинеарность, скорее всего,
имеет место,
а связаны между собой переменные z и w.

