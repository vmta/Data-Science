
﻿1
00:00:13,330 --> 00:00:19,668
Аналогично можно разобрать
случай ридж-регрессии.
Мы не будем на нем
подробно останавливаться,
поскольку единственное отличие с
точки зрения языка программирования R
состоит в том, что мы используем α = 0.
То есть если я хочу оценить
ридж-регрессию с теми же lambdas,
то я тут должен написать модель
ридж-регрессии и здесь указать α = 0.
В остальном, оценивание и графики можно
построить совершенно аналогичные.
Возникает естественный вопрос.Ну,
а как же выбрать этот самый
штрафной коэффициент lambda?
Первое желание.
А давайте выберем штрафной
коэффициент lambda так, чтобы
величина суммарная RSS плюс штраф была
поменьше, к сожалению, не срабатывает.
Потому что в этом случае надо взять lambda
= 0 и мы таким образом получим случай
обычной регрессии.
Здесь используется так называемый
метод кросс-валидации.
Я не буду на нем подробно останавливаться.
Кратко лишь скажу, что строятся,
наши данные разбиваются на
десять случайных групп.
Соответственно, по девяти группам мы
оцениваем модель и предсказываем,
находим сумму квадратов ошибок для
десятой группы и соответственно,
посчитав десять вариантов суммы квадратов
ошибок, мы выбираем то lambda, при
котором сумма квадратов ошибок каждый раз
выкидывая одну группу, будет наименьшей.
Соответственно, покажем как реализовать
этот метод на примере регрессии lasso
Это один из способов
выбрать оптимальное lambda.
Соответственно, делаем кросс-валидацию
для алгоритма LASSO, значит,
у нас регрессоры будут в x0,
зависимая переменная в y и α,
alpha = 1.
Соответственно, компьютер подобрал
оптимальное lambda, можно посмотреть.
Например, давайте построим
график для нашей ситуации.
Соответственно, что сделал компьютер?
Компьютер перебрал разные
варианты lambdas и обнаружил,
при каком lambda сумма квадратов остатков,
посчитанная путем кросс-валидации
будет наименьшей.
И вторая оценка это там,
где резко увеличив величину штрафа,
мы не сильно проиграем в
сумме квадратов ошибок.
Соответственно есть две ошибки,
две идеи оценивать lambda с крышкой.
Одна – по минимуму суммы квадратов ошибок.
Другая – с некоторой подстраховкой,
которая предпочитает модель с
коэффициентами более близкими к нулю.
Ну, соответственно,
можно посмотреть на оба варианта.
Соответственно, можно вытащить из нашей,
алгоритма можно вытащить lambda
минимизирующая сумма квадратов ошибок.
То есть вот это один из способов
выбрать оптимальное lambda и второй
способ выбрать оптимальное lambda,
он выбирает большее lambda и
при нем коэффициенты соответственно
ближе к нулю оказываются
и можно выбрать для каждого из lambdas,
можно посмотреть, чему равны коэффициенты.
Соответственно, в первом случае можно
посмотреть коэффициенты из модели,
a s указывается, как lambda.1se, например.
Соответственно, это коэффициенты
в модели для случая
lambda равны 7,48.
В этом видеофрагменте мы показали,
как находить оценки ридж и лассо-регрессии
и как с помощью метода кросс-валидации,
не вдаваясь в его детали, получать
некоторое оптимальное lambda и оценки,
которые соответствуют этому
самому оптимальному lambda.

