
﻿1
00:00:13,210 --> 00:00:18,935
Мы проиллюстрировали метод наименьших
квадратов для множественной регрессии,
когда у нас много регрессоров.
И попутно мы обнаружили следующие факты,
что если в модель включён свободный член,
то есть y_i равняется β₁ плюс объясняющие
регрессоры, то есть существует β₁,
то, соответственно, среди столбцов матрицы
X большое будет слева столбик из единичек,
и в этом случае наши
оценки метода наименьших
квадратов будут обладать
следующими свойствами,
а именно: сумма ошибок прогнозов,
сумма ε_i с крышкой будет равняться нулю,
сумма y будет равняться сумме
прогнозных y, или, другими словами,
среднее значение прогноза будет равняться
среднему значению наблюдаемой переменной,
а также будет присутствовать
разложение TSS = RSS + ESS,
на рисунке мы увидели это
как теорему Пифагора.
Наличие последнего разложения общей
суммы квадратов на две составляющие
RRS плюс ESS позволяет придумать
простой показатель качества модели,
а именно R-квадрат, который умными словами
называется коэффициент детерминации.
Мы знаем, что чем прогнозы точнее
похожи на настоящие Y,
тем меньше будут ошибки прогнозов
ε с крышкой и тем меньше будет сумма
квадратов ошибок прогнозов RSS.
Соответственно, отношение ESS к TSS,
или R-квадрат,
будет примерно равно единичке, если RSS,
сумма квадратов ошибок, будет у нуля.
Соответственно, мы получили коэффициент
R-квадрат, коэффициент детерминации,
который всегда лежит от 0 до 1,
и значения, близкие к 0,
означают, что сумма квадратов ошибок,
сумма квадратов остатков очень большая,
а значения R-квадрат,
близкие к 1, говорят о том,
что сумма квадратов остатков,
сумма квадратов ошибок прогноза маленькая.
Или, другими словами,
R-квадрат — это есть доля объяснённого
разброса y к общему разбросу,
общей сумме квадратов.
Также этот коэффициент R-квадрат
можно интерпретировать по-другому,
не только как объяснённую долю разброса y,
но и как квадрат выборочной корреляции,
а именно: R-квадрат — это есть квадрат
выборочного коэффициента корреляции,
который равен дроби: в числителе сумма y_i
минус y среднее, помножить на y_i с крышкой
минус y среднее, и в знаменателе корни
из TSS помножить на корень из ESS.
Оказывается, доказательство
этого факта очень простое.
Дело в том, что выборочный коэффициент
корреляции — это ни что иное,
как косинус угла между некоторыми двумя
векторами, что мы сейчас и увидим.
А вот теперь как раз нам
потребуется четвёртый факт,
что ESS к TSS — это
косинус квадрат угла φ.
Как мы выяснили, показатель детерминации,
коэффициент — это есть
отношение ESS к TSS.
То есть он несёт тот смысл,
что это есть доля объяснённого
разброса переменной y в
общем разбросе переменной y.
Соответственно, из предыдущих формул
следует, что этот показатель лежит от 0 до
1, и R-квадрат — это
косинус квадрат угла φ.
Но, оказывается,
для косинуса у нас есть и другая формула.
Из-за того, что скалярное произведение
любых двух векторов A и B
— это есть длина вектора
A помножить на длину вектора
B на косинус угла между ними,
то из этого факта следует
выражение для косинуса,
а именно: косинус между
любыми двумя векторами — это
есть скалярное произведение
этих двух векторов делить
на длину одного вектора
на длину другого вектора.
Соответственно, что в нашем случае
будет являться R-квадратом,
или косинусом квадрата между какими
двумя векторами является R-квадрат?
Косинусом какого угла он является?
Угол φ — это угол между AB и BC.
Соответственно, у нас вектор AB с
точностью до направления можно задать
как y минус y
среднее помножить на вектор из единичек.
Соответственно...
Это на самом деле, конечно, BA.
А вектор,
вектор BC можно задать как вектор
y с крышкой минус y среднее
помножить на вектор из единичек.
Это вектор BC.
Соответственно, нам нужен косинус
угла между этими векторами.
Соответственно, что мы получим,
когда посчитаем длину первого
вектора и длину второго вектора?
У нас получается,
что скалярное произведение
BC c BA —
это есть скалярное произведение вектора
y минус y средняя помножить
на вектор из единичек,
y с крышкой минус y средняя
помножить на вектор из единичек.
Соответственно, это есть сумма.
Поскольку с другой стороны скалярное
произведение можно определить как,
скалярное произведение a на b можно
определить как сумму a_i на b_i,
то нам просто нужно попарно
перемножить компоненты этих векторов.
Соответственно, у нас получится: первая
компонента этого вектора — это y_1 минус y79
00:06:43,502 --> 00:06:48,930
средняя, первая компонента второго вектора
— это y с крышкой первая минус y средняя.
Соответственно, я беру
y_i минус y средняя и
перемножаю на y_i с
крышкой минус y средняя.
Это я нашёл числитель.
Соответственно, как выглядит знаменатель?
В знаменателе у нас
находится длина вектора BC,
это корень квадратный из суммы
квадратов его компонент,
то есть это корень квадратный из суммы,
BC это y с крышкой,
соответственно: y_i с крышкой
минус y средняя в квадрате.
Ну, а соответственно,
длина вектора BA — это
есть корень из суммы
квадратов его компонент,
то есть корень из суммы y_i
минус y средняя в квадрате.
Соответственно, мы получили ещё одну
формулу для коэффициента R-квадрат,
а именно: R-квадрат
равняется — в числителе сумма y_i минус
y средняя помножить на y_i
с крышкой минус y средняя,
деленное на корень квадратный
из сумма y_i минус y средняя в
квадрате на сумма y_i с крышкой
минус y средняя в квадрате.
А, с другой стороны,
что это такое по сути?
Здесь можно узнать формулу выборочной
корреляции между двумя векторами,
а именно: это выборочная корреляция,
обозначу её sCorr (sample
correlation — выборочная
корреляция), между
векторами y и y с крышкой.
И всё, конечно, нужно взять в квадрат,
поскольку я считал косинус угла,
а R-квадрат — это косинус квадрат угла.
Таким образом,
мы получили две интерпретации для
показателя в качестве регрессии.
С одной стороны, R-квадрат — это
доля объяснённой дисперсии y, доля
объяснённого разброса y: это объяснённый
разброс y делить на общий разброс y.
А с другой стороны,
R-квадрат — это выборочна корреляция между
прогнозами и настоящим y,
взятое в квадрат.
Чем R-квадрат выше,
тем больше y с крышкой похож на y,
чем чем R-квадрат выше,
тем выше доля объяснённой дисперсии.

