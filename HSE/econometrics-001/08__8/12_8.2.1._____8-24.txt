Запускаем RStudio,
открываем заранее
заготовленный маленький файл.
Open file, и выбираем тут lab_08_before.R.
Соответственно, здесь у нас
сразу сделана загрузка пакетов.
Загружаем пакеты.
И начнём мы с тех рядов,
которые мы сами искусственно создадим,
потому что на искусственно созданных рядах
мы знаем, как устроена истина, и это нам…
знание истины, облегчает нам понимание
графиков соответствующего временного ряда,
автокорреляционной функции и
частной автокорреляционной функции.
Для начала сгенерим простой ряд с помощью
команды arima.sim, то есть это симуляция,
искусственно создаём процесс arima.
Возьмём, скажем, 100 наблюдений и
возьмём AR(1)-процесс с параметром,
скажем, равным 0.7, ar=0.7.
Соответственно, мы сгенерировали наш ряд.
Теперь можно построить его график,
plot(y).
Вот так вот выглядит типичный график
для AR(1)-процесса с параметром 0.7.
Можно посмотреть, как выглядит
его автокорреляционная функция,
Acf(y).
И можно посмотреть, как выглядит его
частная автокорреляционная функция
partial autocorrelation function, Pacf(y).
Ну для удобства мы все три графика
выведем на один экран и посмотрим.
Есть специальная функция tsdisplay,
которая выводит
сразу три этих популярных графика
для одномерного временного ряда.
Мы сделаем увеличение картинки,
нажмем Zoom,
немножко тут потянем и получим
такой замечательный график.
Итак, это AR(1) процесс: yt = 0,7yt ‒
1 + εt, Соответственно, что мы видим?
Если посмотреть на сам график, то видно,
что, в принципе, текущее значение y,
— вот если я возьму какое-то значение, —
оно довольно сильно похоже на предыдущее.
Ну то есть если y был низким,
то и следующий y тоже, скорее всего,
будет низким.
А если вот y был высоким, то и следующий
y, скорее всего, будет высоким.
Ну бывают, конечно, довольно резкие
падения, можно найти на графике: вот был
где-то средненький, около единички,
а тут вот резко упал.
Но, тем не менее, около высоких значений
следуют высокие, и это естественное
свойство AR процесса, потому что yt
по-хорошему равен 0,7 помножить на y
предыдущее плюс случайная составляющая,
что мы видим, соответственно, на графике
обычной автокорреляционной функции и
частной автокорреляционной функции?
На этом графике мы видим корреляцию
между yt и y несколько шагов назад.
То есть если я знаю… если вот предыдущий
y выше среднего уровня на единичку,
то следующий y почти на 0,8 будет выше,
чем свой средний уровень.
А если у меня есть информация о том,
что — раз, два, три, — четыре шага назад,
y четыре шага назад,
четыре периода по времени назад был выше
своего среднего уровня на единичку,
то это приведёт к тому,
что сегодняшний y в среднем будет где-то
на 0,4 выше, чем средний уровень.
То есть мы видим,
что влияние прошлых y ослабляется,
вот если мерить совокупный эффект.
Что мы видим на графике частной
автокорреляционной функции?
На графике частной
автокорреляционной функции мы видим,
что частная автокорреляция
φ1 отлична от нуля,
а остальные частные
автокорреляции ну близки к нулю.
Поскольку мы имеем дело не с настоящими
автокорреляционными функциями,
не с теоретическими, а с выборочной,
то вот здесь R автоматом провёл
границы доверительного интервала,
— соответственно, частная автокорреляция,
начиная со второй, третьей,
четвёртой, пятой и так далее, — гипотеза
о том, что они равны нулю не отвергается.
То есть у нас по-хорошему только одна
частная автокорреляция не равна нулю.
И что мы видим?
Как мы проинтерпретируем этот эффект?
Первая автокорреляция,
она совпадает с первой частной, да, вот
эти два столбика первые одинаковой высоты,
а вторая частная
автокорреляция уже равна нулю.
Что это означает?
Это означает,
что если зафиксировать вчерашний y,
и я знаю что-то про позавчерашний, что
позавчерашний был выше своего среднего,
то это не несёт никакой
информации о сегодняшнем y.
Соответственно, давайте теперь посмотрим,
как выглядят аналогичные
графики для MA-процесса,
для процесса скользящего среднего.
Собственно, замена у нас тут будут одна.
Скажем, мы возьмём MA-процесс
коэффициентом минус 0.8.
Сгенерировали y,
и сразу построим один график,
состоящий из трёх составляющих.
Вот перед нами уже типичный MA-процесс.
Здесь 100 наблюдений за
процессом скользящего среднего.
Здесь ситуация ровно противоположная.
Я вижу, что обычная автокорреляция,
вчерашний y несёт в себе информацию
о сегодняшнем, но позавчерашний y
уже никак не связан с сегодняшним y.
С частной автокорреляцией немножко
по-другому: если я зафиксирую вчерашний y,
то позавчерашний начнёт быть
связанным с сегодняшним.
И, соответственно,
также построим некий
более сложный процесс,
а именно: построим симуляции
для процесса ARMA (1, 1).
Откопируем, соответственно,
эти две строчки и укажем,
что по AR части коэффициент, скажем,
равен 0.5.
И посмотрим на наш ряд.
Ну здесь поведение уже
не такое однозначное.
Видно, что первая автокорреляция и частная
автокорреляция выходят за доверительный
интервал, а дальше все
остальные близки к нулю.
Тут, к сожалению или к счастью,
ARMA-процессы могут демонстрировать
совершенно разное поведение,
но общая суть сохраняется.
Давайте я попробую для
примера взять ARMA-процесс
с одинаковыми знаками: -0.8, -0.5.
И для сравнения посмотрим,
как он выглядит.
Здесь уже совершенно другая структура
частной автокорреляции и автокорреляции,
но, тем не менее, общее свойство
стационарных процессов мы здесь видим: что
и обычная автокорреляция
быстро сходит к нулю,
— то есть вот первое значение отлично
от нуля, слишком сильно отрицательное,
второе слишком сильно положительное,
третье ещё не похоже на ноль,
а дальше уже все похожи на ноль,
— и частная автокорреляция: вот первые
несколько не похожи на ноль,
а дальше все похожи на ноль.
То есть вот она типичная структура
частной автокорреляционной функции и
автокорреляционной функции
для ARMA-процесса: она
быстро должна сходиться к нулю.
И сейчас мы для сравнения посмотрим,
как выглядят аналогичные графики
для нестационарных процессов.

