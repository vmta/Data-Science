Модель авторегрессии
и скользящего среднего обобщает собой,
соответственно,
модель авторегрессии и модель
процесса скользящего среднего.
То есть это — стационарный процесс вида
yt = константа + b1yt − 1 + b2y2 − 2 +...
+, p прошлых значений игрека используется,
+ εt + a1εt − 1 + a2 на εt − 2 ,...,
всего q предыдущих значений ε.
И мы предполагаем,
что сумма p + q минимально возможна,
то есть эта запись самая короткая.
Процессы авторегрессии
скользящего среднего,
сокращенно обозначается: ARMA (p, q).
Первое, параметр p — это количество
лагов по AR-части, по y,
и MA — это количество лагов по...
q — это количество лагов по
части скользящего среднего.
Что означает фраза: сумма
p + q минимальна возможна?
Оказывается, один и тот же процесс я
могу записать несколькими уравнениями.
Ну, например, если yt — это просто
белый шум, то есть yt = εt,
то я возьму из сегодняшнего y вычту
вчерашний, и у меня получится,
что yt − yt −1 = εt − εt − 1.
И, соответственно, получается,
что у меня две формы записи по
сути одного и того же процесса.
И, естественно,
мы выбираем самую кратчайшую,
в самой кратчайшей форме записи yt = εt.
Это процесс ARMA (0, 0),
у него нет предыдущих
значений y уравнений и нет
предыдущих значений ε уравнений.
Процессы ARMA — это в каком-то смысле все,
что нам нужно знать про
стационарные процессы.
Есть теорема, которая говорит о том,
что любой стационарный процесс можно
представить в виде AR-бесконечность.
То есть AR-процесс с бесконечным
количеством предыдущих лагов.
А ARMA- процесс, можно выбрать
количество лагов p,q достаточно большим
и выбрать коэффициенты перед игреками
прошлыми и перед эпсилонами прошлыми так,
чтоб приблизить с высокой степенью
точности любой AR (∞) процесс.
Соответственно, получается на практике,
что ARMA (p, q) процессов достаточна,
чтобы моделировать любой
стационарный процесс.
Итого, про ARMA-процессы мы получаем
следующие выводы: что коэффициенты
у них не интерпретируются, интерпретация
коэффициентов отдельных невозможна,
однако они достаточно хорошо прогнозируют,
и именно для этого мы их
и будем использовать.
Чтобы прогнозировать,
надо сначала оценить модель.
Соответственно, у нас на входе
будет T наблюдений: y1, y2,..., yT.
И для оценки неизвестных коэффициентов
мы будем использовать метод
максимального правдоподобия.
В методе максимального
правдоподобия необходимы более
конкретные предпосылки на закон
распределения случайной величины ε.
Стандартно предполагается,
что ε независимы и имеют
нормальное распределение с нулевым
математическим ожиданием и
постоянной дисперсией ς-квадрат.
Также мы предполагаем стационарность
процесса yt и выполнение
уравнения ARMA-процесса на yt.
В результате применения метода
максимального правдоподобия,
мы получаем вектор оценок
неизвестных коэффициентов.
Неизвестными у нас являются показателями:
а1, а2, а3,..., аq, соответственно,
мы получаем а1 с крышечкой,
а2 с крышечкой,..., аq с крышечкой.
И также неизвестными показателями
являются коэффициенты b1, b2,...,
bp, мы получаем их оценки,
то есть bi с крышечками,
и также неизвестными являются
показатели дисперсии белого шума ε,
ς-квадрат, мы получаем его оценку.
Помимо оценок сами коэффициентов,
метод максимального правдоподобия дает
нам также оценку ковариационной матрицы.
VAR с крышкой, θ с скрышкой.
И, соответственно, имея оценки
коэффициентов и оценку их ковариационной
матрицы, мы можем строить доверительные
интервалы и проверять гипотезы.
К сожалению, только асимптотически,
то есть только при больших N.
Здесь результатов для малых выборок
нет даже в предположении нормальности
отдельных εt, но тем не менее при
больших выборках мы можем утверждать,
что оценка коэффициента aj с
крышкой минус настоящая aj,
делить на стандартную ошибку aj с крышкой,
стремится к нормальному стандартному
распределению с математическим
ожиданием 0 и дисперсией, равной 1.
Это нам позволяет проверять гипотезы
и строить доверительные интервалы.

