Из геометрической
интерпретации можно запомнить,
собственно, две важных составляющих,
а именно то,
что мы интерпретируем дисперсию как
квадрат длины случайной величины, и мы
интерпретируем корреляцию между случайными
величинами как косинус угла между ними.
Возникает такой естественный вопрос:
а зачем нам вообще случайные величины
интерпретировать геометрически?
Ответ такой, что на самом деле при
такой интерпретации начинает работать
вся школьная геометрия: оказывается, верна
теорема Пифагора для случайных величин или
теорема Фалеса,
или теорема о трёх перпендикулярах,
— все их можно выписать для случайных
величин, и они будут верными, и,
фактически, появляется бесплатное
доказательство каких-то сложных фактов.
С помощью условного математического
ожидания мы сформулируем стандартные
предпосылки на случайную составляющую ε,
а именно: мы будем
предполагать три предпосылки.
Математическое ожидание от каждой
случайной составляющей при известных
иксах, то есть при известном каждом
регресссоре для каждого наблюдения,
я пишу коротко матрицу X.
Значит, условное математическое
ожидание от ε_i при всех известных
регрессорах равна 0.
Условное математическое
ожидание от ε_i^2 при
всех условных регрессорах равна σ².
Или можно точно так же сказать,
что условная дисперсия ε_i при
известной матрице X равна σ².
И ковариация между ε_i и ε_j при
фиксированной матрице X равна нулю.
Эти три предпосылки, точнее,
две — про дисперсию и ковариацию —
можно коротко записать с помощью такого
понятия, как ковариационная матрица.
Когда мы говорим «ковариационная
матрица некоего случайного вектора»,
мы имеем в виду здоровую табличку чисел.
Первое число в первой
строке — это дисперсия ε_1.
Второе число в первой строке, то есть
первая строчка, второй столбец, 
(1,2) координаты, — это ковариация ε_1 и ε_2.
Соответственно, в ковариационной матрице,
скажем, в третьей строчке,
втором столбце находится ковариация ε_3 и
ε_2, это третья строчка, второй столбец.
Соответственно, в этой матрице
находятся и все дисперсии каждого ε,
и все попарные ковариации: ε_1 с ε_3,
ε_2 с ε_7... все-все-все ковариации
и дисперсии находятся в одной матрице,
в одной табличке чисел.
Соответственно, первые наши две
предпосылки на ε можно как сформулировать?
Что ковариации равны нулю,
а на диагонали дисперсии равны σ².
То есть матрица принимает
такой простой вид.
В этой табличке чисел на диагонали
стоят σ², σ²... σ².
А вне диагонали стоят одни нули.
В линейной алгебре есть такое понятие
«единичная матрица», это матрица,
у которой по диагонали стоят единички,
а вне диагонали — нули.
Соответственно, в нашем случае наши
предпосылки можно записать как
ковариационная матрица вектора ε
при фиксированных регрессорах X
равна σ² умножить на
эту самую единичную матрицу,
которая обозначается буковкой I,
сокращение от английского «identity».
Подведём итог.
Какие у нас есть предпосылки?
У нас есть предпосылки, что дисперсия ε
при фиксированных регрессорах X равна
σ² умножить на единичную матрицу,
что на самом деле просто означает,
что дисперсия ε_i при
фиксированном X равна σ²,
а ковариация разных ε при
фиксированном X равна нулю.
Математическое ожидание от ε_i
при фиксированных X равно нулю,
но нам эта предпосылка
даже не потребуется.
И у нас есть линейная модель,
то есть y_i = β₁ + β₂ x_i + ε_i
(случайная составляющая).
Это для примера двух
объясняющих переменных.
И, соответственно, эти предпосылки
позволяют посчитать дисперсию
любой оценки МНК β_j с крышкой и любую
ковариацию β_j с крышкой и β_l с крышкой.
Мы посчитаем для начала дисперсию,
условную, оценки метода наименьших
квадратов и ковариацию оценки метода
наименьших квадратов для случая
парной регрессии.

