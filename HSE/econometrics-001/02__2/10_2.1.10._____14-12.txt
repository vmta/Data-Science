Оценки коэффициентов,
которые мы получаем методом
наименьших квадратов, обладают рядом
замечательных статистических свойств.
Эти свойства, их очень много, и поэтому,
чтобы как-то их было проще все осознать,
мы их поделим на три части: это
базовые свойства – свойства,
которые верны без предположения о
нормальном распределении ошибок ε_i и
которые верны даже для малых или больших
выборок, неважен размер выборки.
Вторая группа свойств – это
асимптотические свойства,
то есть свойства хорошие, которые
верны при больших n, то есть при n,
формально стремящихся к бесконечности,
с бытовой точки зрения,
на больших выборках в десятки
или в сотни тысяч наблюдений.
И третья группа свойств, которые верны на
малых выборках, даже там на 20, или 30,
или 40 наблюдениях, но которые
требуют выполнения предпосылки о том,
что ошибки ε_i имеют
нормальное распределение.
Вот три группы свойств.
Базовые, казалось бы,
самые интересные, но, к сожалению,
их меньше всего,
и с помощью базовых свойств,
не предположив нормальность распределения
или большой размер выборки, к сожалению,
нельзя проверять гипотезы и
строить доверительные интервалы.
Теперь перейдем ко всем предпосылкам,
рассмотрим каждую предпосылку одну за
одной и посмотрим,
какие три группы свойств из них следуют.
Итак, предпосылки,
необходимые нам для применения
метода наименьших квадратов и для
получения хороших свойств коэффициентов.
Итак, во-первых, мы предполагаем,
что истинная зависимость имеет
вид y_i = β₁ + β₂ х_i + β_3 z_i и так далее.
Я буду рассказывать на примере двух
объясняющих переменных, и, естественно,
большее количество объясняющих
переменных мы просто делаем по аналогии.
Соответственно, первая
предпосылка говорит о том,
что зависимость игрека от
объясняющих переменных линейная.
Линейность у нас по
регрессорам по отдельным,
линейность у нас по коэффициентам β есть.
В матричном виде эту предпосылку
можно записать, что y = Xβ + ε.
Вторая предпосылка состоит в том,
что как устроена реальность,
такая модель и оценивается,
то есть не только истина так устроена,
но мы с помощью метода наименьших
квадратов строим регрессию объясняемой
переменной именно на те регрессоры,
от которых y зависит,
то есть строим регрессию
на константу, х_i и z_i.
В матричном виде мы знаем, что в таком
случае можно выписать формулу для оценок
МНК, и она будет равна (X'X)^{-1} X'y.
Третья предпосылка простая, то,
что у нас наблюдений все-таки больше,
чем оцениваемых коэффициентов β с крышкой.
Дальше следует группа предпосылок о том,
как распределены ошибки ε_i.
Во-первых, мы предполагаем предпосылку,
которая называется предпосылкой
о строгой экзогенности.
Чуть позже мы поговорим,
почему она так называется и так далее,
а сейчас мы просто предположим, что
условное математическое ожидание ошибки
ε_i при фиксированных
регрессорах X равняется нулю.
В матричном виде, соответственно,
E от ε_i при фиксированной
матрице X равняется нулю.
Вторая предпосылка, которая называется
условная гомоскедастичность, то есть
условная дисперсия ε_i при фиксированных
регрессорах постоянна и равна σ².
И третья предпосылка –
отсутствие условной корреляции,
то есть ковариация между ε_i и ε_j,
между разными ε, то есть i не равно j.
Эта ковариация при
фиксированном X равна нулю.
Это были предпосылки на ε.
Кроме того,
у нас есть предпосылки на регрессоры.
Они, конечно, стахостические, случайные,
но все-таки не совсем произвольные.
Во-первых, мы предполагаем,
что вероятность получения строгой линейной
зависимости между регрессорами равна нулю.
То есть например, эта предпосылка
исключает возможность того, что мы среди
объясняющих переменных включили рост в
сантиметрах и рост в метрах одновременно,
либо то, либо другое.
Эти переменные, очевидно, зависимые,
одна в сто раз больше, чем другая.
Это наличие линейной зависимости, оно по
предпосылкам запрещено среди регрессоров.
В матричном виде предпосылку о том, что
среди регрессоров нет линейно зависимых,
можно сформулировать по-разному: можно
сказать, что rank матрицы X равен k,
или можно сказать, что матрица X,
траспонированное X обратима.
И еще одна предпосылка,
здесь для разных ситуаций
используется разная предпосылка,
мы рассмотрим случай,
когда у нас имеет место случайная выборка,
то есть мы предполагаем, что наши объекты,
по которым мы пытаемся оценить
зависимость, – это случайная выборка.
А именно, допустим, у нас было
много-много-много индивидов, все индивиды
страны, мы пытались понять, как зарплата
зависит от уровня образования, и для этого
мы случайным образом из всех-всех-всех
индивидов страны отобрали 1 000 человек
для нашего исследования, или это может
быть случайная выборка предприятий.
Мы пытались понять, от каких факторов,
скажем, зависит, будет
ли предприятие банкротом или нет, и для
этого мы сделали случайную выборку всех
предприятий из всего множества, мы взяли
1 000 предприятий или 2 000 предприятий,
и оценивали модель по
этой случайной выборке.
В частности, это означает, что регрессоры,
относящиеся к различным наблюдениям, 
скажем, уровень образования i-человека
и уровень образования j-человека,
они независимы.
И поскольку имеет место случайная выборка,
то мы предполагаем,
что закон распределения регрессоров
для каждого наблюдения одинаковый.
Когда все эти предпосылки, которые мы
сформулировали, предпосылки о зависимости
y от x, предпосылки о распределении ε,
предпосылки о регрессорах,
когда все они выполнены, мы получаем,
что верны три группы свойств.
Во-первых, базовые свойства.
Первое базовое свойство говорит,
что β с крышкой j-ое,
получаемое по методу наименьших квадратов,
эти оценки линейны по y,
то есть любая оценка коэффициентов,
будь то оценка коэффициента β₁ с крышкой,
или β₂ с крышкой, или β_3 с крышкой,
любая оценка коэффициентов
представляется в виде некоторой функции:
c_1 y_1 + c_2 y_2 и так далее,
где c_1 и c_2 – это величины,
которые зависят только от
объясняющих переменных, но не от y.
Из этого, в частности, следуют кое-какие
свойства приятные о β с крышкой.
Это говорит о том,
что если мы, например,
исследовали зависимость зарплаты в рублях,
то если мы будем измерять зависимую
переменную в других единицах измерения,
скажем, от рублей перейдем к
тысячам рублей, то, соответственно,
все y_i упадут в 1 000 раз,
и линейность по y означает,
что и все оценки коэффициентов β с
крышкой тоже упадут в 1 000 раз.
Если мы регрессоры трогать не будем,
то есть у нас константы c_1, c_2,
c_3 не поменяются,
а зависимую переменную мы будем мерить в
других единицах измерения, соответственно,
и коэффициенты, определяющие зависимость,
точно также упадут в 1 000 раз.
Второе хорошее базовое свойство – то,
что оценки коэффициентов β_j с крышкой
условно не смещены и не смещены,
то есть математическое ожидание
от β с крышкой при фиксированных
регрессорах равно соответствующему
истинному коэффициенту β.
Это очень хорошее свойство.
Это говорит о том, что наш метод, конечно,
может ошибаться, и оценка β с крышкой,
которую мы получаем, может не совпадать
с настоящим β, но β с крышкой иногда
будет больше настоящего β,
иногда будет меньше настоящего β,
но в среднем, β с крышкой
попадает то влево, то вправо,
но в среднем попадает в
неизвестный коэффициент β.
Следующая группа базовых свойств – это то,
что оценки β с крышкой 
являются эффективными среди
линейных и несмещенных оценок.
Что это означает, эффективность
среди линейных и несмещенных оценок?
Это означает, что если мы рассмотрим
любую другую линейную несмещенную оценку,
то есть оценку, которая линейно
выражается через y и которая не смещена,
то есть какая-то β с
крышкой альтернативная,
получаемая не методом наименьших
квадратов, а каким-то другим методом,
то эта оценка будет обладать,
альтернативная оценка,
будет обладать большим разбросом, большей
дисперсией, большей неопределенностью,
меньшей точностью,
чем оценка метода наименьших квадратов.
Это очень хорошее свойство.
Оно говорит о том, что если вы хотите
простую оценку, то есть линейную,
хотите оценку несмещенную, которая в
среднем бы попадала в неизвестную истину,
то ничего лучше оценок метода наименьших
квадратов у нас не получится.
То есть математически это означает,
что условная дисперсия β
с крышкой при фиксированных регрессорах
альтернативного больше либо равна,
чем условная дисперсия β с крышкой,
полученная по методу наименьших
квадратов опять же при
фиксированных регрессорах.
Следующая группа базовых свойств
— это свойства о том, чему
конкретно равны дисперсии, ковариации,
математические ожидания получаемых оценок.
А именно: мы доказывали, что
ковариационная матрица вектора β с крышкой
при фиксированных иксах — это σ²,
помноженная на (X'X) в минус первой.
Соответственно, дисперсия отдельно
взятого коэффициента β_j с крышкой
при фиксированных иксах — это σ²,
делённая на RSS_j,
где RSS_j — это RSS, получаемая в регрессии
j-ой объясняющей переменные на остальные.
Ковариация β_j с крышкой с
любым остатком ε_i с крышкой при
фиксированных иксах равна нулю.
И математическое ожидание σ²
с крышкой при фиксированных иксах
равно σ².
То есть это свойство говорит,
что оценка σ² с крышкой,
предложенная нами, — RSS, делённая
на (n – k), — она тоже несмещённая,
и она несмещённым образом
оценивает σ².
На этом базовые свойства оканчиваются.
Оставшиеся свойства требуют либо
предположения о том, что n,
количество наблюдений,
очень-очень большое,
либо предположение о том дополнительное,
что ε_i имеют нормальное распределение.
Сначала рассмотрим свойства,
которые требуют предположения о том,
что количество наблюдений велико.
Итак, с ростом n, по мере того,
как n стремится к бесконечности,
оценки β_j с крышкой,
— это случайные величины,
— они будут стремится по вероятности
к неизвестным константам β_j.
Второе свойство,
которое нам позволяет проверять гипотезы,
и это очень здорово, что β_j с крышкой,
оценка коэффициента,
минус истинный коэффициент
β_j делить на стандартную
ошибку β_j с крышкой — эта величина,
случайная величина,
и с ростом количества наблюдений
распределение этой случайной величины
становится все более и более похожим на
стандартное нормальное распределение.
Ещё одно свойство, что оценка σ²
с крышкой — тоже замечательное: с
ростом количества наблюдений
случайная величина σ²
с крышкой по вероятности
стремится к константе σ².
На всякий случай я напомню, что σ²
с крышкой — это RSS, делённая на (n – k).
Следующая группа свойств — это свойства,
требующие предположения о нормальности ε_i,
а именно: теперь дополнительно известно,
что при фиксированных иксах ε_i нормальное,
с математическим ожиданием
ноль и дисперсией σ².
При таком дополнительном
предположении оказывается,
что оценки β_j с крышкой эффективные,
то есть являются самыми лучшими,
с наименьшей дисперсией не только
среди линейных по игрекам,
но и среди любых несмещённых оценок.
Это говорит о том,
что если мы хотим несмещённую оценку,
которая в среднем попадает в
неизвестный коэффициент β,
то ничего лучше метода наименьших
квадратов, — даже архисложного,
нелинейного по игрекам, нелинейного
по иксам, — ничего придумать нельзя,
то есть наши оценки β с крышкой
— это несмещённая мечта.
И ещё два свойства,
которые позволяют проверять гипотезы.
Если мы хотим проверить
гипотезы о неизвестном β_j,
то мы можем воспользоваться тем фактом,
что оценка β_j с крышкой минус β_j
делить на стандартную ошибку в малой
выборке при фиксированных иксах имеет
t-распределение с (n – k) степенями свободы,
где n — это количество наблюдений,
k — это количество коэффициентов.
И аналогично можно проверять гипотезы
и строить доверительные интервалы для
σ², используя тот факт, что RSS,
делённая на σ²
при фиксированных иксах,
имеет хи-квадрат распределение
с (n – k) степенями свободы.
На этом весь список свойств
пока заканчивается.

