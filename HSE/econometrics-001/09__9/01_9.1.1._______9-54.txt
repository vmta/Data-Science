Любой школьник знает,
что разложение в сумму
совершенно неоднозначно.
Можно сказать, что 4 — это 2 + 2,
а можно сказать,
что 4 — это 3 + 1, и оба этих
утверждения будут одинаково верными,
ни одно из них не является
более правильным, чем другое.
То же самое явление с неоднозначностью
разложения в сумму возникает
и в линейных моделях.
Например, я могу сказать,
что модель A верна и модель
A будет иметь вид yi = 2 * xi + εi,
а могу сказать,
что верна модель Б,
где модель Б — это yi = 3 * xi + ui.
Ну естественно чтобы обе
модели были верными,
случайные составляющие εi-тое и ui-тое
должны быть связаны простым соотношением:
εi-тое должно конечно равняться 1
помножить на xi-тое плюс ui-тое.
Но тем не менее, обе формы записи
модели абсолютно правильны.
И возникает такой естественный вопрос:
ну хорошо, если обе формы записи модели
правильные, то что же произойдет,
если я буду оценивать модель с
помощью метода наименьших квадратов
и построю регрессию y на x.
Все-таки, какой мне коэффициент выдаст
алгоритм метода наименьших квадратов: 2,
примерно, или примерно 3?
И на этот вопрос на самом
деле мы уже отвечали,
когда изучали свойства оценок МНК
— метода наименьших квадратов.
Мы сказали, что если модель
представлена в форме линейной
yi = β1 + β2 * xi + β3 * zi + εi,
где у нас помимо большого
количества предпосылок
среди этого большого списка фигурировала
предпосылка, что условное математическое
ожидание εi-того при известных
регрессорах x должна равняться нулю.
И вот если все предпосылки из
большого списка выполнены,
но сейчас я сакцентирую сознательно
внимание на предпосылке о том,
что условное математическое
ожидание εi-того равно нулю.
Если эта предпосылка выполнена,
то у нас было такое свойство метода
наименьших квадратов как состоятельность,
что означает что с ростом количества
наблюдений оценка β с крышкой
становится похожей на настоящий
неизвестный параметр β,
и оценки метода наименьших
квадратов также были несмещенными.
То есть в среднем β с крышкой
равнялись неизвестному β.
В чем состоит смысл предпосылки то что
условное математическое ожидание ε
при известных регрессорах равно нулю?
Это означает, что среднее значение
случайной составляющей не только
не зависит от объясняющих переменных,
но так же еще и равно нулю.
В частности, из предпосылки о том,
что условное математическое ожидание
ε при фиксированных x равно нулю,
можно в качестве следствия получить два
факта: в среднем ошибка равна нулю,
математическое ожидание от ε равно
нулю и второй факт — то что ковариация
или сила линейной связи между
ε и x также равна нулю.
Соответственно, эта предпосылка
может быть нарушена.
Например, если ковариация между xi-тым
и εi-тым не равна нулю, то указанная
предпосылка нарушена и оценки метода
наименьших квадратов будут несостоятельны,
то есть с ростом количества наблюдений
β с крышкой не будут стремиться к
β в уравнении модели, и оценки метода
наименьших квадратов будут смещены,
то есть в среднем оценка не будет
попадать в настоящий коэффициент β.
Давайте на примере
посмотрим как будут вести
себя оценки метода наименьших квадратов в
случае корреляции между εi-тым и xi-тым.
Для этого предположим,
что мы знаем как y зависит от x.
Пусть y = 2 + 3 * xi + εi.
И как мы и обещали, предположим,
что ковариация между
xi-тое и εi-тое не равна нулю и
пусть для конкретики равна –2.
Пусть также дисперсия
регрессора xi равна 4,
и дисперсия случайной
составляющей εi равна 3.
И наша задача посмотреть что
произойдет в идеальной ситуации,
когда мы будем применять метод
наименьших квадратов и у нас будет
много-много-много-много наблюдений.
То есть мы посчитаем предел по
вероятности, поэтому он пишется
plim (probability limit), предел по
вероятности при количестве наблюдений,
стремящихся к бесконечности от β2 с
крышкой, то есть коэффициент при x.
Мы знаем,
что в этой задаче настоящее β2 = 3, в
этой форме записи β2 = 3 и нас интересует
предел по вероятности β2 с крышкой.
Я напомню,
что мы выводили формулы для оценки метода
наименьших квадратов β2
с крышкой = ∑(xi – x
среднее) * (yi – y среднее)
делить на ∑(xi – x среднее)².
Такую формулу предлагает
метод наименьших квадратов.
Мы поделим числитель и знаменатель на
n – 1, чтобы увидеть, что в числителе
и знаменателе на самом деле находятся
некие выборочные характеристики.
А именно, если поделить в
числителе и в знаменателе на
n – 1, естественно
на всю дробь это никак не повлияет,
поэтому так можно сделать.
Оказывается, что в числителе у нас
находится выборочная ковариация,
а в знаменателе находится
выборочная дисперсия, вот эта.
Это выборочная ковариация
между x и y,
а вот это — это выборочная
дисперсия набора x1, ..., xn.
Выборочная ковариация и
выборочная дисперсия хороши тем,
что они обладают теми же свойствами,
что и настоящая ковариация и дисперсия,
поэтому мы для удобства будем
использовать обозначение sCov, ну,
означает, sample —
выборочная по-английски.
sample covariance между x и y
делить на samle variance между...
samle variance x, выборочная дисперсия x.
Хороши тем, что свойства у
выборочной дисперсии такие же,
как у настоящей дисперсии.
Например, выборочная дисперсия удвоенного
x — это четыре выборочных дисперсии x,
выборочная дисперсия вектора (x
+ 7) — это выборочная дисперсия
вектора x и так далее.
Все свойства полностью аналогичны
свойствам истинной ковариации и истинной
дисперсии.
Ну, соответственно...
И еще на нашей стороне
закон больших чисел.
Закон больших чисел говорит, что если
у меня случайные величины независимы
и одинаково распределены, если я возьму
их среднее арифметическое, то при большом
количестве наблюдений, — а нас интересует
именно большое количество наблюдений,
— среднее арифметическое будет
совпадать с математическим ожиданием.
Применительно к данной задаче
закон больших чисел говорит,
что выборочная ковариация между x
и y с ростом количества наблюдений
будет стремиться к ковариации
между xi-тым и yi-тым,
а выборочная дисперсия выборочная
дисперсия x будет стремиться к
настоящей дисперсией xi-того.
Соответственно, вот эта форма позволяет
легко получить предел по вероятности,
а именно — при большом
количестве наблюдений
β2 с крышкой окажется равным ковариации
между xi-тым и yi-тым деленная
на дисперсию xi-того.
А здесь мы уже все знаем.
yi-тое — это 2 + 3xi
+ εi делить на дисперсию xi-того.
И мы получаем ковариацию xi-того с
3xi-тым — это 3 на дисперсию xi-того.
Ковариация xi-тое с εi-тое
по условию это – 2 делить
на дисперсию xi-того и мы
получим 3 –
2 делить на дисперсию xi-того,
она равна 4.
И получаем здесь 2,5.
И таким образом мы видим, что даже при
очень большом количестве наблюдений,
при количестве наблюдений, стремящемся
к бесконечности, β2 с крышкой, оценка,
которую дает метод наименьших квадратов,
она становится все более и более на 2,5,
а не настоящее β, 2, которое в нашем
примере, в нашей форме записи равно 3.

