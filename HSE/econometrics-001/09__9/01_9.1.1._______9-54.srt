1
00:00:13,250 --> 00:00:18,165
Любой школьник знает,

2
00:00:18,165 --> 00:00:21,056
что разложение в сумму
совершенно неоднозначно.

3
00:00:21,056 --> 00:00:23,579
Можно сказать, что 4 — это 2 + 2,
а можно сказать,

4
00:00:23,579 --> 00:00:27,017
что 4 — это 3 + 1, и оба этих
утверждения будут одинаково верными,

5
00:00:27,017 --> 00:00:29,890
ни одно из них не является
более правильным, чем другое.

6
00:00:29,890 --> 00:00:32,908
То же самое явление с неоднозначностью

7
00:00:32,908 --> 00:00:36,940
разложения в сумму возникает
и в линейных моделях.

8
00:00:36,940 --> 00:00:42,277
Например, я могу сказать,
что модель A верна и модель

9
00:00:42,277 --> 00:00:47,826
A будет иметь вид yi = 2 * xi + εi,
а могу сказать,

10
00:00:47,826 --> 00:00:53,780
что верна модель Б,
где модель Б — это yi = 3 * xi + ui.

11
00:00:53,780 --> 00:00:56,474
Ну естественно чтобы обе
модели были верными,

12
00:00:56,474 --> 00:01:01,470
случайные составляющие εi-тое и ui-тое
должны быть связаны простым соотношением:

13
00:01:01,470 --> 00:01:06,751
εi-тое должно конечно равняться 1
помножить на xi-тое плюс ui-тое.

14
00:01:06,751 --> 00:01:11,000
Но тем не менее, обе формы записи
модели абсолютно правильны.

15
00:01:11,000 --> 00:01:15,541
И возникает такой естественный вопрос:
ну хорошо, если обе формы записи модели

16
00:01:15,541 --> 00:01:20,043
правильные, то что же произойдет,
если я буду оценивать модель с

17
00:01:20,043 --> 00:01:24,627
помощью метода наименьших квадратов
и построю регрессию y на x.

18
00:01:24,627 --> 00:01:30,142
Все-таки, какой мне коэффициент выдаст
алгоритм метода наименьших квадратов: 2,

19
00:01:30,142 --> 00:01:31,990
примерно, или примерно 3?

20
00:01:31,990 --> 00:01:35,662
И на этот вопрос на самом
деле мы уже отвечали,

21
00:01:35,662 --> 00:01:40,738
когда изучали свойства оценок МНК
— метода наименьших квадратов.

22
00:01:40,738 --> 00:01:45,212
Мы сказали, что если модель
представлена в форме линейной

23
00:01:45,212 --> 00:01:49,580
yi = β1 + β2 * xi + β3 * zi + εi,

24
00:01:49,580 --> 00:01:55,705
где у нас помимо большого
количества предпосылок

25
00:01:55,705 --> 00:02:00,737
среди этого большого списка фигурировала
предпосылка, что условное математическое

26
00:02:00,737 --> 00:02:05,100
ожидание εi-того при известных
регрессорах x должна равняться нулю.

27
00:02:05,100 --> 00:02:08,670
И вот если все предпосылки из
большого списка выполнены,

28
00:02:08,670 --> 00:02:12,274
но сейчас я сакцентирую сознательно
внимание на предпосылке о том,

29
00:02:12,274 --> 00:02:15,510
что условное математическое
ожидание εi-того равно нулю.

30
00:02:15,510 --> 00:02:19,506
Если эта предпосылка выполнена,

31
00:02:19,506 --> 00:02:24,060
то у нас было такое свойство метода
наименьших квадратов как состоятельность,

32
00:02:24,060 --> 00:02:27,940
что означает что с ростом количества
наблюдений оценка β с крышкой

33
00:02:27,940 --> 00:02:31,410
становится похожей на настоящий
неизвестный параметр β,

34
00:02:31,410 --> 00:02:35,130
и оценки метода наименьших
квадратов также были несмещенными.

35
00:02:35,130 --> 00:02:40,100
То есть в среднем β с крышкой
равнялись неизвестному β.

36
00:02:40,100 --> 00:02:44,895
В чем состоит смысл предпосылки то что

37
00:02:44,895 --> 00:02:49,690
условное математическое ожидание ε
при известных регрессорах равно нулю?

38
00:02:49,690 --> 00:02:54,698
Это означает, что среднее значение
случайной составляющей не только

39
00:02:54,698 --> 00:02:59,871
не зависит от объясняющих переменных,
но так же еще и равно нулю.

40
00:02:59,871 --> 00:03:02,272
В частности, из предпосылки о том,

41
00:03:02,272 --> 00:03:06,400
что условное математическое ожидание
ε при фиксированных x равно нулю,

42
00:03:06,400 --> 00:03:11,830
можно в качестве следствия получить два
факта: в среднем ошибка равна нулю,

43
00:03:11,830 --> 00:03:17,260
математическое ожидание от ε равно
нулю и второй факт — то что ковариация

44
00:03:17,260 --> 00:03:23,564
или сила линейной связи между
ε и x также равна нулю.

45
00:03:23,564 --> 00:03:27,195
Соответственно, эта предпосылка
может быть нарушена.

46
00:03:27,195 --> 00:03:32,380
Например, если ковариация между xi-тым
и εi-тым не равна нулю, то указанная

47
00:03:32,380 --> 00:03:37,229
предпосылка нарушена и оценки метода
наименьших квадратов будут несостоятельны,

48
00:03:37,229 --> 00:03:42,658
то есть с ростом количества наблюдений
β с крышкой не будут стремиться к

49
00:03:42,658 --> 00:03:48,315
β в уравнении модели, и оценки метода
наименьших квадратов будут смещены,

50
00:03:48,315 --> 00:03:53,850
то есть в среднем оценка не будет
попадать в настоящий коэффициент β.

51
00:03:53,850 --> 00:03:57,121
Давайте на примере
посмотрим как будут вести

52
00:03:57,121 --> 00:04:02,120
себя оценки метода наименьших квадратов в
случае корреляции между εi-тым и xi-тым.

53
00:04:02,120 --> 00:04:07,977
Для этого предположим,
что мы знаем как y зависит от x.

54
00:04:07,977 --> 00:04:12,588
Пусть y = 2 + 3 * xi + εi.

55
00:04:12,588 --> 00:04:17,950
И как мы и обещали, предположим,
что ковариация между

56
00:04:17,950 --> 00:04:23,336
xi-тое и εi-тое не равна нулю и
пусть для конкретики равна –2.

57
00:04:23,336 --> 00:04:28,370
Пусть также дисперсия
регрессора xi равна 4,

58
00:04:28,370 --> 00:04:32,673
и дисперсия случайной
составляющей εi равна 3.

59
00:04:32,673 --> 00:04:36,519
И наша задача посмотреть что
произойдет в идеальной ситуации,

60
00:04:36,519 --> 00:04:39,334
когда мы будем применять метод
наименьших квадратов и у нас будет

61
00:04:39,334 --> 00:04:41,290
много-много-много-много наблюдений.

62
00:04:41,290 --> 00:04:44,933
То есть мы посчитаем предел по
вероятности, поэтому он пишется

63
00:04:44,933 --> 00:04:49,692
plim (probability limit), предел по
вероятности при количестве наблюдений,

64
00:04:49,692 --> 00:04:54,352
стремящихся к бесконечности от β2 с
крышкой, то есть коэффициент при x.

65
00:04:54,352 --> 00:04:59,475
Мы знаем,
что в этой задаче настоящее β2 = 3, в

66
00:04:59,475 --> 00:05:05,950
этой форме записи β2 = 3 и нас интересует
предел по вероятности β2 с крышкой.

67
00:05:05,950 --> 00:05:11,930
Я напомню,
что мы выводили формулы для оценки метода

68
00:05:11,930 --> 00:05:17,024
наименьших квадратов β2
с крышкой = ∑(xi – x

69
00:05:17,024 --> 00:05:22,050
среднее) * (yi – y среднее)

70
00:05:22,050 --> 00:05:27,860
делить на ∑(xi – x среднее)².

71
00:05:27,860 --> 00:05:31,556
Такую формулу предлагает
метод наименьших квадратов.

72
00:05:31,556 --> 00:05:36,176
Мы поделим числитель и знаменатель на
n – 1, чтобы увидеть, что в числителе

73
00:05:36,176 --> 00:05:40,180
и знаменателе на самом деле находятся
некие выборочные характеристики.

74
00:05:40,180 --> 00:05:45,520
А именно, если поделить в
числителе и в знаменателе на

75
00:05:45,520 --> 00:05:50,570
n – 1, естественно

76
00:05:50,570 --> 00:05:56,940
на всю дробь это никак не повлияет,
поэтому так можно сделать.

77
00:05:56,940 --> 00:06:02,517
Оказывается, что в числителе у нас
находится выборочная ковариация,

78
00:06:02,517 --> 00:06:07,100
а в знаменателе находится
выборочная дисперсия, вот эта.

79
00:06:07,100 --> 00:06:11,720
Это выборочная ковариация

80
00:06:11,720 --> 00:06:16,520
между x и y,

81
00:06:16,520 --> 00:06:20,966
а вот это — это выборочная

82
00:06:20,966 --> 00:06:26,260
дисперсия набора x1, ..., xn.

83
00:06:26,260 --> 00:06:30,395
Выборочная ковариация и
выборочная дисперсия хороши тем,

84
00:06:30,395 --> 00:06:35,833
что они обладают теми же свойствами,
что и настоящая ковариация и дисперсия,

85
00:06:35,833 --> 00:06:40,686
поэтому мы для удобства будем
использовать обозначение sCov, ну,

86
00:06:40,686 --> 00:06:44,803
означает, sample —
выборочная по-английски.

87
00:06:44,803 --> 00:06:50,554
sample covariance между x и y
делить на samle variance между...

88
00:06:50,554 --> 00:06:54,650
samle variance x, выборочная дисперсия x.

89
00:06:54,650 --> 00:06:57,836
Хороши тем, что свойства у
выборочной дисперсии такие же,

90
00:06:57,836 --> 00:06:59,336
как у настоящей дисперсии.

91
00:06:59,336 --> 00:07:04,967
Например, выборочная дисперсия удвоенного
x — это четыре выборочных дисперсии x,

92
00:07:04,967 --> 00:07:09,692
выборочная дисперсия вектора (x
+ 7) — это выборочная дисперсия

93
00:07:09,692 --> 00:07:11,089
вектора x и так далее.

94
00:07:11,089 --> 00:07:15,643
Все свойства полностью аналогичны
свойствам истинной ковариации и истинной

95
00:07:15,643 --> 00:07:16,410
дисперсии.

96
00:07:16,410 --> 00:07:18,859
Ну, соответственно...

97
00:07:18,859 --> 00:07:22,640
И еще на нашей стороне
закон больших чисел.

98
00:07:22,640 --> 00:07:27,594
Закон больших чисел говорит, что если
у меня случайные величины независимы

99
00:07:27,594 --> 00:07:32,515
и одинаково распределены, если я возьму
их среднее арифметическое, то при большом

100
00:07:32,515 --> 00:07:37,207
количестве наблюдений, — а нас интересует
именно большое количество наблюдений,

101
00:07:37,207 --> 00:07:42,430
— среднее арифметическое будет
совпадать с математическим ожиданием.

102
00:07:42,430 --> 00:07:47,048
Применительно к данной задаче
закон больших чисел говорит,

103
00:07:47,048 --> 00:07:52,524
что выборочная ковариация между x
и y с ростом количества наблюдений

104
00:07:52,524 --> 00:07:57,070
будет стремиться к ковариации
между xi-тым и yi-тым,

105
00:07:57,070 --> 00:08:03,088
а выборочная дисперсия выборочная

106
00:08:03,088 --> 00:08:09,020
дисперсия x будет стремиться к
настоящей дисперсией xi-того.

107
00:08:09,020 --> 00:08:16,620
Соответственно, вот эта форма позволяет
легко получить предел по вероятности,

108
00:08:16,620 --> 00:08:22,080
а именно — при большом
количестве наблюдений

109
00:08:22,080 --> 00:08:28,706
β2 с крышкой окажется равным ковариации

110
00:08:28,706 --> 00:08:35,860
между xi-тым и yi-тым деленная
на дисперсию xi-того.

111
00:08:35,860 --> 00:08:39,144
А здесь мы уже все знаем.

112
00:08:39,144 --> 00:08:46,480
yi-тое — это 2 + 3xi

113
00:08:46,480 --> 00:08:53,490
+ εi делить на дисперсию xi-того.

114
00:08:53,490 --> 00:08:57,734
И мы получаем ковариацию xi-того с

115
00:08:57,734 --> 00:09:02,481
3xi-тым — это 3 на дисперсию xi-того.

116
00:09:02,481 --> 00:09:08,510
Ковариация xi-тое с εi-тое
по условию это – 2 делить

117
00:09:08,510 --> 00:09:13,421
на дисперсию xi-того и мы

118
00:09:13,421 --> 00:09:19,115
получим 3 –

119
00:09:19,115 --> 00:09:25,640
2 делить на дисперсию xi-того,
она равна 4.

120
00:09:25,640 --> 00:09:28,171
И получаем здесь 2,5.

121
00:09:28,171 --> 00:09:34,505
И таким образом мы видим, что даже при
очень большом количестве наблюдений,

122
00:09:34,505 --> 00:09:38,702
при количестве наблюдений, стремящемся
к бесконечности, β2 с крышкой, оценка,

123
00:09:38,702 --> 00:09:43,939
которую дает метод наименьших квадратов,
она становится все более и более на 2,5,

124
00:09:43,939 --> 00:09:50,240
а не настоящее β, 2, которое в нашем
примере, в нашей форме записи равно 3.

