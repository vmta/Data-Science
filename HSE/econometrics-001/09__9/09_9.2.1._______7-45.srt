1
00:00:12,495 --> 00:00:19,740
Перейдем к компьютерной
части 9-той недели.

2
00:00:19,740 --> 00:00:25,537
Запуская R-studio,
открываем файл с заготовленной загрузкой

3
00:00:25,537 --> 00:00:31,540
пакетов, lab_09_before_R,

4
00:00:31,540 --> 00:00:36,590
выделяем все и запускам, то есть мы
активируем необходимые нам пакеты.

5
00:00:36,590 --> 00:00:42,085
Первое, с чего я хочу начать
— это еще раз подчеркнуть

6
00:00:42,085 --> 00:00:46,980
что эндогенность не имеет никакого
отношения к прогнозированию.

7
00:00:46,980 --> 00:00:51,351
Если мы хотим прогнозировать,
если наша задача прогнозировать,

8
00:00:51,351 --> 00:00:55,370
то нам абсолютно не важна форма
записи моделей с эндогенностью,

9
00:00:55,370 --> 00:00:59,861
и поэтому мы всегда можем предполагать
при прогнозировании что наши

10
00:00:59,861 --> 00:01:03,700
регрессоры не коррелированы
со случайными ошибками.

11
00:01:03,700 --> 00:01:06,949
Ну и давайте покажем, что нам может
быть важно при прогнозировании.

12
00:01:06,949 --> 00:01:10,430
На самом деле при прогнозировании
важно только одно — хорошие прогнозы.

13
00:01:10,430 --> 00:01:16,529
Ни значимость, ни вопросы
эндогенности мы никогда не касаемся.

14
00:01:16,529 --> 00:01:21,914
Давайте для примера загрузим в набор
данных h, да, убедимся, что мы находимся

15
00:01:21,914 --> 00:01:27,293
в нужной папке, установим Set Working
Directory To Source File Location.

16
00:01:27,293 --> 00:01:30,787
Соответственно, загрузим в наши,

17
00:01:30,787 --> 00:01:36,323
в набор данных h данные по
стоимости квартир в Москве,

18
00:01:36,323 --> 00:01:39,380
укажем, что в этих данных есть заголовок,

19
00:01:39,380 --> 00:01:44,980
разделитель между наблюдениями,
табуляция, десятичный разделитель,

20
00:01:44,980 --> 00:01:50,715
точка.

21
00:01:50,715 --> 00:01:54,532
Загрузили данные,
бросили взгляд на данные, чтобы убедиться,

22
00:01:54,532 --> 00:01:56,450
что они корректно загрузились.

23
00:01:56,450 --> 00:01:59,538
И теперь при прогнозировании
надо говорить всегда,

24
00:01:59,538 --> 00:02:04,539
что когда мы оцениваем качество прогнозов,
нам важно честное качество прогнозов,

25
00:02:04,539 --> 00:02:09,384
то есть я должен оценить модель по одной
части выборки, спрогнозировать другую

26
00:02:09,384 --> 00:02:13,920
часть выборки и посмотреть качество
прогнозов на другой части выборки.

27
00:02:13,920 --> 00:02:17,009
Потому что если я оцениваю качество
прогнозов по той части выборки,

28
00:02:17,009 --> 00:02:20,337
по какой я оцениваю модель,
то я могу добиться идеального попадания,

29
00:02:20,337 --> 00:02:22,670
включив достаточное
количество регрессоров.

30
00:02:22,670 --> 00:02:26,199
Поэтому первый шаг при честном
прогнозировании, при оценке честной

31
00:02:26,199 --> 00:02:31,016
качества прогнозов — это разделить выборку
на две части: обучающую и тестовую.

32
00:02:31,016 --> 00:02:35,626
Ну, соответственно,
давайте мы попросим компьютер случайно

33
00:02:35,626 --> 00:02:39,570
поделить наши наблюдения на две
части: обучающую и тестовую.

34
00:02:39,570 --> 00:02:43,650
И, соответственно, номера наблюдений,
давайте их назовем

35
00:02:43,650 --> 00:02:49,313
in_train — это будут номера наблюдений,

36
00:02:49,313 --> 00:02:54,941
которые относятся к обучающей
части выборки, соответственно,

37
00:02:54,941 --> 00:03:00,049
я возьму команду из пакета
carrot createDataPartition,

38
00:03:00,049 --> 00:03:04,436
она длинная, но я нажал Tab,
посмотрел как она называется.

39
00:03:04,436 --> 00:03:07,280
И здесь надо указать зависимую переменную,

40
00:03:07,280 --> 00:03:12,760
зависимая переменная пусть у нас будет из
набора данных мы возьмем цену квартиры.

41
00:03:12,760 --> 00:03:17,799
Надо указать сколько наблюдений
мы относим к обучающей выборке,

42
00:03:17,799 --> 00:03:23,310
то есть по которой мы будем оценивать
модель, ну давайте 75 %, 0.75

43
00:03:23,310 --> 00:03:29,040
отнесем к обучающей части выборки,
а по 25 % выборки будем тестировать.

44
00:03:29,040 --> 00:03:33,200
И тут надо еще указать такую
опцию для удобства list=FALSE.

45
00:03:33,200 --> 00:03:35,666
Соответственно, что такое in_train?

46
00:03:35,666 --> 00:03:40,661
Ну можно посмотреть на этот in_train,
на самом деле это просто номера случайно

47
00:03:40,661 --> 00:03:46,180
отобранных наблюдений,
по которым мы будем оценивать регрессию.

48
00:03:46,180 --> 00:03:49,624
Соответственно, h_train
— то есть это та часть,

49
00:03:49,624 --> 00:03:52,976
по которой мы оценивам модель методом МНК.

50
00:03:52,976 --> 00:03:58,029
Это мы из набора данных возьмем номера,
которые входят в вектор in_train,

51
00:03:58,029 --> 00:04:03,228
ну и, соответственно, все столбики,
а h_test — это будет вторая

52
00:04:03,228 --> 00:04:07,708
часть выборки, по ней мы будем
оценивать качество прогнозов.

53
00:04:07,708 --> 00:04:12,738
Соответственно, те наблюдения, которые
попали в обучающую часть выборки их не

54
00:04:12,738 --> 00:04:18,523
надо брать, остальные надо взять, поэтому
здесь перед индексом стоит значок минус.

55
00:04:18,523 --> 00:04:23,733
Ну, соответственно, если посмотреть
вот количество строк в наборе данных

56
00:04:23,733 --> 00:04:28,569
h — это 2040, количество строк
в наборе данных h_train — это

57
00:04:28,569 --> 00:04:34,030
1533 и остальные отнесены к тестовой части

58
00:04:34,030 --> 00:04:39,170
nrow(h_test) 507,
если сложить, получится 2040.

59
00:04:39,170 --> 00:04:42,627
Соответственно, я могу оценить две модели.

60
00:04:42,627 --> 00:04:47,486
Давайте оценим model_1 —
это линейная модель по

61
00:04:47,486 --> 00:04:52,192
данным из набора данных
h_train вот это важно,

62
00:04:52,192 --> 00:04:58,340
не исходный набор данных, а кусочек его,
и любая формула, например,

63
00:04:58,340 --> 00:05:03,890
логарифм цены как он зависит от логарифма

64
00:05:03,890 --> 00:05:09,360
общей площади, логарифма площади кухни,

65
00:05:09,360 --> 00:05:15,230
логарифма жилой площади.

66
00:05:15,230 --> 00:05:22,140
Оценил одну модель,
пропустил буковку e — livespend,

67
00:05:22,140 --> 00:05:26,635
чуть-чуть подвинем.

68
00:05:26,635 --> 00:05:31,130
Оцениваем model_2,
ну давайте совершенно другую модель-2,

69
00:05:31,130 --> 00:05:37,330
данные мы берем из обучающей
выборки и логарифм цены,

70
00:05:37,330 --> 00:05:42,980
строим регрессию на логарифм общей
площади плюс кирпичность дома.

71
00:05:42,980 --> 00:05:49,569
И теперь я спрогнозирую

72
00:05:49,569 --> 00:05:54,527
на тестовую часть выборки и посмотрю
как эти две модели, то есть я даже не

73
00:05:54,527 --> 00:05:58,130
гляжу на значимость коэффициентов,
если моя задача прогнозировать.

74
00:05:58,130 --> 00:06:02,903
Во-первых, y — это будет

75
00:06:02,903 --> 00:06:07,527
из тестовой части выборки
надо взять price и

76
00:06:07,527 --> 00:06:13,865
взять логарифм,

77
00:06:13,865 --> 00:06:19,600
соответственно, вот можем посмотреть
на наш y какие значения он принимает.

78
00:06:19,600 --> 00:06:23,920
Можем спрогнозировать по первой модели

79
00:06:23,920 --> 00:06:28,823
predict модель

80
00:06:28,823 --> 00:06:33,710
мы используем model_1,
а данные мы используем уже h_test.

81
00:06:33,710 --> 00:06:38,940
Ну давайте посмотрим на

82
00:06:38,940 --> 00:06:43,680
прогнозы по модели-1.

83
00:06:43,680 --> 00:06:45,794
Вот прогнозы по модели-1.

84
00:06:45,794 --> 00:06:52,244
И точно так же можно
спрогнозировать по модели-2,

85
00:06:52,244 --> 00:06:55,250
model_2.

86
00:06:55,250 --> 00:07:00,248
И ну простейший критерий качества —
сумма квадратов ошибок прогнозов,

87
00:07:00,248 --> 00:07:04,540
я возьму сумму квадратов
и здесь внутри надо

88
00:07:04,540 --> 00:07:09,491
посчитать ошибки прогнозов (y – y_hat_1)

89
00:07:09,491 --> 00:07:14,040
— это сумма квадратов ошибок
прогнозов по первой модели,

90
00:07:14,040 --> 00:07:18,952
а сумма квадратов ошибок
прогнозов по второй модели

91
00:07:18,952 --> 00:07:23,355
— она оказывается чуть-чуть меньше.

92
00:07:23,355 --> 00:07:27,530
Соответственно, вот несмотря на то,
что во второй модели меньше переменных,

93
00:07:27,530 --> 00:07:34,994
она вне обучающей выборки
оказывается прогнозирует лучше.

94
00:07:34,994 --> 00:07:40,840
И сейчас мы перейдем к теме интерпретации,
то есть к эндогенности.

