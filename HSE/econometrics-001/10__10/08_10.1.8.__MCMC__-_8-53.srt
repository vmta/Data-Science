1
00:00:13,270 --> 00:00:21,383
Зачастую апостериорная функция плотности
оказывается довольно сложного вида.

2
00:00:21,383 --> 00:00:24,106
И тогда ее надо как-то
описать на компьютере.

3
00:00:24,106 --> 00:00:29,293
К сожалению, компьютер плохо работает с
формулами в явном виде, и поэтому для

4
00:00:29,293 --> 00:00:34,609
описания апостериорной функции
используют следующий подход — а давайте

5
00:00:34,609 --> 00:00:39,472
вместо сложной функции плотности возьмем
большую выборку из данного распределения.

6
00:00:39,472 --> 00:00:43,290
То есть, вместо того чтобы описывать
конкретную функциональную зависимость,

7
00:00:43,290 --> 00:00:47,948
мы возьмем много-много-много-много
чисел: r_1, r_2, ...,

8
00:00:47,948 --> 00:00:52,849
r10000, с помощью которых можно примерно
восстановить закон распределения.

9
00:00:52,849 --> 00:00:56,921
Ну, скажем, если меня интересует
математическое ожидание апостериорного

10
00:00:56,921 --> 00:01:02,045
распределения, я просто возьму среднее
арифметическое моих r_1, ..., r_10000.

11
00:01:02,045 --> 00:01:09,980
Если меня интересует вероятность того,
что апостериорное распределение больше 0,

12
00:01:09,980 --> 00:01:15,100
то я просто возьму количество наблюдений,
которые оказались больше 0.

13
00:01:15,100 --> 00:01:18,472
То есть я заменяю функцию
плотности на большую выборку из

14
00:01:18,472 --> 00:01:21,075
соответствующего закона распределения.

15
00:01:21,075 --> 00:01:23,784
Так информацию удобней
хранить в компьютере,

16
00:01:23,784 --> 00:01:27,490
просто огромный массив чисел
описывает нам функцию плотности.

17
00:01:27,490 --> 00:01:30,784
И, соответственно, нужен некий алгоритм,

18
00:01:30,784 --> 00:01:36,795
который позволяет по исходным
наблюдениям соответственно модели и

19
00:01:36,795 --> 00:01:41,280
априорному распределению получить большую
выборку из апостериорного распределения.

20
00:01:41,280 --> 00:01:45,954
Этот алгоритм называется алгоритмом
Монте-Карло по схеме Марковской цепи.

21
00:01:45,954 --> 00:01:48,820
Для нас он окажется пока
что «черным ящиком»,

22
00:01:48,820 --> 00:01:51,573
потому что он сам по себе
достаточно сложно устроен,

23
00:01:51,573 --> 00:01:54,937
но тем не менее надо себе представлять,
что это такой алгоритм,

24
00:01:54,937 --> 00:01:59,929
на входе которого мы подаем модель,
которая описывает наши данные,

25
00:01:59,929 --> 00:02:04,200
наше априорное мнение о
неизвестных параметрах.

26
00:02:04,200 --> 00:02:07,545
А на выходе из этого алгоритма
мы получаем большую выборку

27
00:02:07,545 --> 00:02:09,670
из апостериорного распределения.

28
00:02:09,670 --> 00:02:13,820
Надо отметить еще раз,
что этот алгоритм случайный, и,

29
00:02:13,820 --> 00:02:17,991
соответственно, если применить этот
алгоритм еще раз к той же самой модели,

30
00:02:17,991 --> 00:02:21,380
к тому же самому априорному распределению
и тому же самому набору данных,

31
00:02:21,380 --> 00:02:23,628
то получатся слегка другие оценки.

32
00:02:23,628 --> 00:02:28,807
И, соответственно, в качестве плюсов
этого байесовского подхода можно отметить

33
00:02:28,807 --> 00:02:34,220
следующее: во-первых, можно задавать
вопросы напрямую о неизвестных параметрах.

34
00:02:34,220 --> 00:02:38,381
То есть в классическом подходе
вопрос — какова вероятность того,

35
00:02:38,381 --> 00:02:40,696
что β3 > 0, он был бессмысленный.

36
00:02:40,696 --> 00:02:45,936
Мы этого в классическом подходе не знаем,
β_3 — это неизвестная константа, она либо

37
00:02:45,936 --> 00:02:50,461
> 0, либо < 0, но говорить о вероятности
того, что β_3 > 0, бессмысленно.

38
00:02:50,461 --> 00:02:54,950
Она либо 1, либо 0, и мы этого не знаем и
никогда не узнаем в классическом подходе.

39
00:02:54,950 --> 00:02:58,165
В байесовском подходе вопрос:
какова вероятность того,

40
00:02:58,165 --> 00:03:03,830
что третий неизвестный параметр β_3 > 0, он
осмысленный, на него можно получить ответ.

41
00:03:03,830 --> 00:03:09,522
Можно задать вопрос в байесовском подходе
— какова вероятность того, что β_3 = 0.

42
00:03:09,522 --> 00:03:14,857
Можно в байесовском подходе ответить
на вопрос — чему равно математическое

43
00:03:14,857 --> 00:03:19,350
ожидание от β_3 с учетом
имеющейся информации о выборке.

44
00:03:19,350 --> 00:03:20,711
Это первый плюс.

45
00:03:20,711 --> 00:03:26,079
В байесовском подходе естественным
образом задаются вопросы, и все

46
00:03:26,079 --> 00:03:32,270
выводы байесовского подхода формулируются
именно о неизвестных параметрах β.

47
00:03:32,270 --> 00:03:38,295
Нет никакого уровня доверия, нету такого
понятия, как доверительные интервалы.

48
00:03:38,295 --> 00:03:42,744
Мы не говорим, мы просто говорим,
задаем конкретный вопрос про β

49
00:03:42,744 --> 00:03:46,735
и на него получаем ответ,
именно в вероятностях рассуждаем.

50
00:03:46,735 --> 00:03:49,327
И второй плюс байесовского подхода — то,

51
00:03:49,327 --> 00:03:52,689
что апостериорное распределение
всегда существует.

52
00:03:52,689 --> 00:03:57,540
Не важно, какие у нас есть проблемы в
данных — жесткая мультиколлинеарность,

53
00:03:57,540 --> 00:04:01,731
малое количество наблюдений,
оценивается количество параметров больше,

54
00:04:01,731 --> 00:04:06,121
чем наблюдений — для байесовского
подхода все это, при праве реализации,

55
00:04:06,121 --> 00:04:07,840
естественно, не проблема.

56
00:04:07,840 --> 00:04:12,225
Потому что, даже если себе представить
ситуацию полного отсутствия наблюдений,

57
00:04:12,225 --> 00:04:15,260
даже в этой ситуации
апостериорное распределение есть.

58
00:04:15,260 --> 00:04:19,056
Оно, конечно, совпадет с априорным,
если вы во что-то верили и у вас нет

59
00:04:19,056 --> 00:04:23,110
наблюдений, но в это же вы и будете
верить с учетом отсутствующих наблюдений.

60
00:04:23,110 --> 00:04:28,305
Но в большинстве ситуаций любое,
даже небольшое количество наблюдений

61
00:04:28,305 --> 00:04:33,500
изменит как-то ваше мнение
о неизвестных параметрах.

62
00:04:33,500 --> 00:04:38,123
Но среди минусов байесовского подхода,
самый главный, на мой взгляд, это то,

63
00:04:38,123 --> 00:04:39,453
что его не все знают.

64
00:04:39,453 --> 00:04:44,115
Просто классический подход в силу
исторических причин, сейчас доминирует

65
00:04:44,115 --> 00:04:48,580
байесовский, хотя байесовский гораздо
более естественный в смысле интерпретации.

66
00:04:48,580 --> 00:04:51,984
И второй минус байесовского
подхода — это то,

67
00:04:51,984 --> 00:04:56,174
что зачастую алгоритмы требуют
большого объема вычислений,

68
00:04:56,174 --> 00:04:59,703
и какая-нибудь модель,
которая оценивается байесовским подходом,

69
00:04:59,703 --> 00:05:05,627
вполне себе может на современной технике
оцениваться день, два или недельку.

70
00:05:05,627 --> 00:05:12,435
И, соответственно, мы применим
байесовский подход к двум моделям.

71
00:05:12,435 --> 00:05:14,490
Первая модель — логит.

72
00:05:14,490 --> 00:05:17,921
Когда мы изучали логит-модель, мы
коснулись одной из проблем логит-модели,

73
00:05:17,921 --> 00:05:19,670
а именно — идеальное прогнозирование.

74
00:05:19,670 --> 00:05:23,771
Иногда в логит-модели
складывается ситуация,

75
00:05:23,771 --> 00:05:29,150
когда оценки β с крышкой методом
максимального правдоподобия не существует.

76
00:05:29,150 --> 00:05:32,813
Это связано с тем, что оптимально
взять коэффициент β с крышкой,

77
00:05:32,813 --> 00:05:34,590
стремящийся к бесконечности.

78
00:05:34,590 --> 00:05:37,670
Я вкратце напомню, что такое логит-модель.

79
00:05:37,670 --> 00:05:40,981
У нас зависимая переменная
принимает значение 1 или 0,

80
00:05:40,981 --> 00:05:44,300
у нас был пример — выжил ли пассажир
на «Титанике» или не выжил.

81
00:05:44,300 --> 00:05:48,519
И при этом мы предполагаем, что есть
некая скрытая переменная y со звездочкой,

82
00:05:48,519 --> 00:05:54,585
отвечающая за склонность выжить, и y_i = 1,
если склонность y_i со звездочкой > 0,

83
00:05:54,585 --> 00:05:59,280
и y_i = 0, если скрытая
переменная yi со звездочкой < 0.

84
00:05:59,280 --> 00:06:03,368
Ну и соответственно, уже скрытая
переменная yi со звездочкой линейно

85
00:06:03,368 --> 00:06:07,640
зависит от объясняющих переменных,
yi со звездочкой = β1

86
00:06:07,640 --> 00:06:12,560
+ β_2x_i + случайная составляющая, имеющая,
скажем, логистическое распределение.

87
00:06:12,560 --> 00:06:17,424
Соответственно, эта модель в байесовском
подходе дополняется априорным

88
00:06:17,424 --> 00:06:20,511
распределением о неизвестных параметрах β.

89
00:06:20,511 --> 00:06:24,187
Соответственно, один из
вариантов дополнения,

90
00:06:24,187 --> 00:06:29,131
вполне распространенный, состоит в том,
чтобы сказать, что априорно мы считает,

91
00:06:29,131 --> 00:06:34,083
что β — неизвестные параметры имеют
многомерное нормальное распределение с

92
00:06:34,083 --> 00:06:42,647
вектором средним b0 и ковариационной
матрицей В0 в (−1) степени.

93
00:06:42,647 --> 00:06:46,521
Ну тут вопрос возникает: хорошо,
а как это вот уточнить?

94
00:06:46,521 --> 00:06:51,190
Ну на практике берут совсем конкретную
спецификацию, например, мы считаем,

95
00:06:51,190 --> 00:06:54,492
как правило, чаще всего мы ничего
не знаем о неизвестных параметрах,

96
00:06:54,492 --> 00:07:00,038
поэтому разумно считать, что они лежат в
очень широком диапазоне от (− ∞) до (+∞),

97
00:07:00,038 --> 00:07:03,483
ну или там от (− 100) до (+ 100).

98
00:07:03,483 --> 00:07:08,992
Соответственно, можно сказать,
что математическое ожидание вектора,

99
00:07:08,992 --> 00:07:12,820
параметров, вектора β — это b0,
это — вектор из нулей.

100
00:07:12,820 --> 00:07:17,385
А, соответственно, матрица точности,
обратная матрице, ковариационной матрице,

101
00:07:17,385 --> 00:07:22,850
диагональная матрица, где на диагонали
стоят d, d, d, d, d, вне диагонали — нули.

102
00:07:22,850 --> 00:07:26,480
И вот это d — мало, соответственно,

103
00:07:26,480 --> 00:07:31,091
у нас число d, имеет смысл,

104
00:07:31,091 --> 00:07:36,860
что это 1, деленное на дисперсию
соответствующего коэффициента.

105
00:07:36,860 --> 00:07:42,693
И, соответственно, если d мало,
то 1 на d, дисперсия достаточно высока,

106
00:07:42,693 --> 00:07:47,070
и это отражает мою степень
неуверенности в параметре β.

107
00:07:47,070 --> 00:07:51,140
И, соответственно, можно привести
пример проблемной ситуации: если

108
00:07:51,140 --> 00:07:55,614
у меня объясняющая переменная x
принимает значения: 1, 2 и 3,

109
00:07:55,614 --> 00:07:58,462
а y принимает значения: 0, 0 и 1,

110
00:07:58,462 --> 00:08:04,198
то в этой ситуации оценки метода
максимального правдоподобия не существует.

111
00:08:04,198 --> 00:08:09,105
Однако, если я априорно скажу, скажем,
что β_1 имеет нормальное распределение с

112
00:08:09,105 --> 00:08:13,465
математическим ожиданием 0 и дисперсией
100, ну то есть это означает,

113
00:08:13,465 --> 00:08:17,060
что 95%, я считаю,
что β1 лежит от (− 20) до 20.

114
00:08:17,060 --> 00:08:22,130
А β_2 независимо от β_1 априорно
нормально распределено

115
00:08:22,130 --> 00:08:26,869
с матожиданием 0 и дисперсией 100,
то тогда, оказывается,

116
00:08:26,869 --> 00:08:32,549
с помощью алгоритма Монте-Карло по
схеме Марковской цепи можно установить,

117
00:08:32,549 --> 00:08:37,120
что уравнение для скрытой переменной
оцененной имеет вид — y_i с

118
00:08:37,120 --> 00:08:42,140
крышкой = −10,8 + 4,5 умножить на x_i.

119
00:08:42,140 --> 00:08:49,150
И никаких проблем с несуществованием
оценок у нас в этом случае не оказывается.

