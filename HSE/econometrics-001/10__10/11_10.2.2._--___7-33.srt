1
00:00:12,630 --> 00:00:19,990
Перейдем к иллюстрациям
байесовского подхода.

2
00:00:19,990 --> 00:00:25,030
Bayesian apрroach.

3
00:00:25,030 --> 00:00:31,650
Сначала проиллюстрируем метод Монте-Карло
по схеме марковской цепи в ситуации,

4
00:00:31,650 --> 00:00:37,177
где обычная логит-модель приводит
к отсутствию оценок β (с крышкой).

5
00:00:37,177 --> 00:00:40,320
Давайте создадим
искусственный набор данных.

6
00:00:40,320 --> 00:00:44,450
Назовем его bad,
bad — это будет плохой набор данных.

7
00:00:44,450 --> 00:00:49,494
data.frame Y будет равняться (0,0,1),

8
00:00:49,494 --> 00:00:53,695
а X будет равняться (1,2,3).

9
00:00:53,695 --> 00:00:58,390
То есть bad — это плохой набор данных.

10
00:00:58,390 --> 00:01:02,173
Здесь действительно видно,
что возможно, в кавычках,

11
00:01:02,173 --> 00:01:05,314
так называемое совершенное
прогнозирование,

12
00:01:05,314 --> 00:01:11,016
а именно — легко понять в нашем наборе
данных, что если X > 2,5, то Y = 1.

13
00:01:11,016 --> 00:01:14,590
Очень все просто и понятно.

14
00:01:14,590 --> 00:01:19,675
Это будет означать, что оценки метода
максимального правдоподобия не существует.

15
00:01:19,675 --> 00:01:24,293
Ну, то есть если я попробую
оценить model_logit,

16
00:01:24,293 --> 00:01:29,040
то есть glm по набору данных bad,

17
00:01:29,040 --> 00:01:34,810
попробую оценить модель
зависимости Y от X и укажу,

18
00:01:34,810 --> 00:01:39,710
что речь идет о логит-модели,

19
00:01:39,710 --> 00:01:44,679
link="logit", то, соответственно,

20
00:01:44,679 --> 00:01:50,164
я получу вот такое вот предупреждение
от R, что возникли подогнанные

21
00:01:50,164 --> 00:01:55,528
вероятности 0 или 1, по английски —
fitted probabilities 0 or 1 occured.

22
00:01:55,528 --> 00:01:59,183
И если я посмотрю на описание модели,

23
00:01:59,183 --> 00:02:03,305
summary (model _logit),

24
00:02:03,305 --> 00:02:07,880
то, соответственно,

25
00:02:07,880 --> 00:02:13,220
я получу здесь вероятности,

26
00:02:13,220 --> 00:02:18,560
близкие к одному, хотя на самом деле
коэффициенты, в общем-то, значимы.

27
00:02:18,560 --> 00:02:21,074
Да?
Проблема связана с тем,

28
00:02:21,074 --> 00:02:24,400
что оценки — очень большие по модулю,

29
00:02:24,400 --> 00:02:29,575
стандартные ошибки у них еще больше,
и получается, что p value близко к 1,

30
00:02:29,575 --> 00:02:34,511
хотя на самом деле коэффициенты там
довольно значимые, зависимость есть.

31
00:02:34,511 --> 00:02:38,198
И, соответственно,
что предлагает байесовский подход?

32
00:02:38,198 --> 00:02:43,580
Байесовский подход предлагает сделать
какое-то априорное предположение о том,

33
00:02:43,580 --> 00:02:46,890
какие значения принимают
коэффициенты β_1 и β_2.

34
00:02:46,890 --> 00:02:49,175
Ну, даже если вы не знаете,
какие они могут быть,

35
00:02:49,175 --> 00:02:52,739
у вас все равно могут быть какие-то
соображения здравого смысла, ну,

36
00:02:52,739 --> 00:02:56,377
что при моих единицах измерения
β вряд ли будет больше 100.

37
00:02:56,377 --> 00:02:58,255
Это означает, что вы считаете,

38
00:02:58,255 --> 00:03:02,430
что с вероятностью 95 % β лежит
в диапазоне от (- 100) до 100.

39
00:03:02,430 --> 00:03:05,080
Это означает, что вы предполагаете,

40
00:03:05,080 --> 00:03:08,449
что стандартное отклонение
β априорное равно 50.

41
00:03:08,449 --> 00:03:12,985
И вы можете моделировать β нормальным
распределением с матожиданием 0 и

42
00:03:12,985 --> 00:03:14,720
дисперсией 50^2.

43
00:03:14,720 --> 00:03:17,427
Ну, например, мы так и поступим.

44
00:03:17,427 --> 00:03:22,427
То есть если я предполагаю
априорное распределение,

45
00:03:22,427 --> 00:03:27,090
что β у меня нормально с
математическим ожиданием 0 и

46
00:03:27,090 --> 00:03:31,250
дисперсией 50^2,
то как мне это реализовать в R?

47
00:03:31,250 --> 00:03:35,842
Соответственно, я воспользуюсь уже

48
00:03:35,842 --> 00:03:40,564
написанной функцией для реализации модели

49
00:03:40,564 --> 00:03:45,841
логит в байесовском подходе,
функция mcmclogit.

50
00:03:45,841 --> 00:03:52,974
И здесь точно так же указывается набор
данных, точно так же указывается формула.

51
00:03:52,974 --> 00:03:56,865
Ну, дальше мне надо указать то,
во что я верю априорно.

52
00:03:56,865 --> 00:04:00,621
Если я считаю, что у меня β

53
00:04:00,621 --> 00:04:07,172
неизвестная от (- 100) до 100,
то это означает,

54
00:04:07,172 --> 00:04:12,370
что стандартное отклонение равно
50 и дисперсия равна 50^2.

55
00:04:12,370 --> 00:04:17,749
И я указываю, b0,
вектор средних – это 0 и B0 – это 1 делить

56
00:04:17,749 --> 00:04:22,790
на дисперсию, это, соответственно, 1/50^2.

57
00:04:22,790 --> 00:04:27,539
Соответственно, сделав
такие предположения,

58
00:04:27,539 --> 00:04:30,305
я могу оценить логит-модель.

59
00:04:30,305 --> 00:04:34,708
Здесь уже не надо пугаться
этих fitted probabilities,

60
00:04:34,708 --> 00:04:39,984
потому что в алгоритме mcmc только
на первом шаге используется

61
00:04:39,984 --> 00:04:44,074
стартовая точка,
для алгоритма mcmc используется старая,

62
00:04:44,074 --> 00:04:47,508
но тем не менее сейчас это
предупреждение стало неопасным,

63
00:04:47,508 --> 00:04:53,455
и если мы посмотрим на
отчет по данной модели,

64
00:04:53,455 --> 00:04:58,906
то мы получим уже вполне

65
00:04:58,906 --> 00:05:06,490
себе замечательные коэффициенты с средним,

66
00:05:06,490 --> 00:05:10,411
стандартной ошибкой.

67
00:05:10,411 --> 00:05:14,282
И можно строить доверительные интервалы,

68
00:05:14,282 --> 00:05:19,100
можно проверять гипотезы,
но уже немножко по-другому: то есть можно,

69
00:05:19,100 --> 00:05:23,742
например, посчитать вероятность того,
что коэффициент больше 0.

70
00:05:23,742 --> 00:05:27,900
Вот я вижу,
что если строить байесовский интервал,

71
00:05:27,900 --> 00:05:32,125
то 90-процентный интервал, то есть...

72
00:05:32,125 --> 00:05:36,224
Не, давайте лучше возьмем
95-процентный интервал,

73
00:05:36,224 --> 00:05:41,088
который можно по умолчанию строить, не
вызывая дополнительных подсчетов, от 4,23

74
00:05:41,088 --> 00:05:46,920
до 47 — это у нас получится 95-процентный
интервал для коэффициента при X.

75
00:05:46,920 --> 00:05:51,204
И таким образом можно получить
некий байесовский аналог того,

76
00:05:51,204 --> 00:05:53,830
что коэффициент на самом деле значим.

77
00:05:53,830 --> 00:05:58,250
0,95 %-ным интервалом не пересекается.

78
00:05:58,250 --> 00:06:02,154
И точно так же, конечно, надо подчеркнуть,

79
00:06:02,154 --> 00:06:08,177
что в байесовском подходе выводы зависят
от того, во что вы верили изначально.

80
00:06:08,177 --> 00:06:11,253
Некоторые считают это недостатком
байесовского подхода,

81
00:06:11,253 --> 00:06:13,361
но на самом деле это просто такая...

82
00:06:13,361 --> 00:06:16,170
факт математической
честности с самим собой.

83
00:06:16,170 --> 00:06:19,220
Результаты зависят от
изначальных предположений.

84
00:06:19,220 --> 00:06:22,925
Соответственно, если я предположу,
скажем, изначально, что я считал,

85
00:06:22,925 --> 00:06:27,687
что у меня коэффициенты от там
(-20) до 20, то есть можно считать,

86
00:06:27,687 --> 00:06:33,324
что стандартная ошибка 10, то,
соответственно, дисперсия 10^2,

87
00:06:33,324 --> 00:06:37,883
коэффициент точности B(большое)0 =1/10^2.

88
00:06:37,883 --> 00:06:43,555
И при переоценке той же самой модели
я уже получу другие коэффициенты.

89
00:06:43,555 --> 00:06:48,240
Вот коэффициент, давайте сравним,
вот здесь был 4,53,

90
00:06:48,240 --> 00:06:53,990
а раньше был коэффициент 21,
апостериорное среднее было.

91
00:06:53,990 --> 00:06:57,516
То есть если я изначально верил,
что коэффициент ближе к нулю,

92
00:06:57,516 --> 00:06:59,550
я получил соответствующие выводы.

93
00:06:59,550 --> 00:07:07,026
И здесь у меня по-прежнему 95-процентный
байесовский интервал от 0,04 до 10,28.

94
00:07:07,026 --> 00:07:11,688
Тоже он 0 не содержит,
поэтому это является аналогом классической

95
00:07:11,688 --> 00:07:15,432
проверки гипотез, и мы делаем вывод о том,
что коэффициент значим.

96
00:07:15,432 --> 00:07:19,054
Ну и, действительно,
здесь все-таки ясно видно,

97
00:07:19,054 --> 00:07:24,552
что с ростом X меняется, в такой простой
выборке меняется вероятность того,

98
00:07:24,552 --> 00:07:28,740
что Y = 0, а в классическом подходе
оценки просто не существовали.

