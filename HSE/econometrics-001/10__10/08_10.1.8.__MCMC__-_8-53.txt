
﻿1
00:00:13,270 --> 00:00:21,383
Зачастую апостериорная функция плотности
оказывается довольно сложного вида.
И тогда ее надо как-то
описать на компьютере.
К сожалению, компьютер плохо работает с
формулами в явном виде, и поэтому для
описания апостериорной функции
используют следующий подход — а давайте
вместо сложной функции плотности возьмем
большую выборку из данного распределения.
То есть, вместо того чтобы описывать
конкретную функциональную зависимость,
мы возьмем много-много-много-много
чисел: r_1, r_2, ...,
r10000, с помощью которых можно примерно
восстановить закон распределения.
Ну, скажем, если меня интересует
математическое ожидание апостериорного
распределения, я просто возьму среднее
арифметическое моих r_1, ..., r_10000.
Если меня интересует вероятность того,
что апостериорное распределение больше 0,
то я просто возьму количество наблюдений,
которые оказались больше 0.
То есть я заменяю функцию
плотности на большую выборку из
соответствующего закона распределения.
Так информацию удобней
хранить в компьютере,
просто огромный массив чисел
описывает нам функцию плотности.
И, соответственно, нужен некий алгоритм,
который позволяет по исходным
наблюдениям соответственно модели и
априорному распределению получить большую
выборку из апостериорного распределения.
Этот алгоритм называется алгоритмом
Монте-Карло по схеме Марковской цепи.
Для нас он окажется пока
что «черным ящиком»,
потому что он сам по себе
достаточно сложно устроен,
но тем не менее надо себе представлять,
что это такой алгоритм,
на входе которого мы подаем модель,
которая описывает наши данные,
наше априорное мнение о
неизвестных параметрах.
А на выходе из этого алгоритма
мы получаем большую выборку
из апостериорного распределения.
Надо отметить еще раз,
что этот алгоритм случайный, и,
соответственно, если применить этот
алгоритм еще раз к той же самой модели,
к тому же самому априорному распределению
и тому же самому набору данных,
то получатся слегка другие оценки.
И, соответственно, в качестве плюсов
этого байесовского подхода можно отметить
следующее: во-первых, можно задавать
вопросы напрямую о неизвестных параметрах.
То есть в классическом подходе
вопрос — какова вероятность того,
что β3 > 0, он был бессмысленный.
Мы этого в классическом подходе не знаем,
β_3 — это неизвестная константа, она либо
> 0, либо < 0, но говорить о вероятности
того, что β_3 > 0, бессмысленно.
Она либо 1, либо 0, и мы этого не знаем и
никогда не узнаем в классическом подходе.
В байесовском подходе вопрос:
какова вероятность того,
что третий неизвестный параметр β_3 > 0, он
осмысленный, на него можно получить ответ.
Можно задать вопрос в байесовском подходе
— какова вероятность того, что β_3 = 0.
Можно в байесовском подходе ответить
на вопрос — чему равно математическое
ожидание от β_3 с учетом
имеющейся информации о выборке.
Это первый плюс.
В байесовском подходе естественным
образом задаются вопросы, и все
выводы байесовского подхода формулируются
именно о неизвестных параметрах β.
Нет никакого уровня доверия, нету такого
понятия, как доверительные интервалы.
Мы не говорим, мы просто говорим,
задаем конкретный вопрос про β
и на него получаем ответ,
именно в вероятностях рассуждаем.
И второй плюс байесовского подхода — то,
что апостериорное распределение
всегда существует.
Не важно, какие у нас есть проблемы в
данных — жесткая мультиколлинеарность,
малое количество наблюдений,
оценивается количество параметров больше,
чем наблюдений — для байесовского
подхода все это, при праве реализации,
естественно, не проблема.
Потому что, даже если себе представить
ситуацию полного отсутствия наблюдений,
даже в этой ситуации
апостериорное распределение есть.
Оно, конечно, совпадет с априорным,
если вы во что-то верили и у вас нет
наблюдений, но в это же вы и будете
верить с учетом отсутствующих наблюдений.
Но в большинстве ситуаций любое,
даже небольшое количество наблюдений
изменит как-то ваше мнение
о неизвестных параметрах.
Но среди минусов байесовского подхода,
самый главный, на мой взгляд, это то,
что его не все знают.
Просто классический подход в силу
исторических причин, сейчас доминирует
байесовский, хотя байесовский гораздо
более естественный в смысле интерпретации.
И второй минус байесовского
подхода — это то,
что зачастую алгоритмы требуют
большого объема вычислений,
и какая-нибудь модель,
которая оценивается байесовским подходом,
вполне себе может на современной технике
оцениваться день, два или недельку.
И, соответственно, мы применим
байесовский подход к двум моделям.
Первая модель — логит.
Когда мы изучали логит-модель, мы
коснулись одной из проблем логит-модели,
а именно — идеальное прогнозирование.
Иногда в логит-модели
складывается ситуация,
когда оценки β с крышкой методом
максимального правдоподобия не существует.
Это связано с тем, что оптимально
взять коэффициент β с крышкой,
стремящийся к бесконечности.
Я вкратце напомню, что такое логит-модель.
У нас зависимая переменная
принимает значение 1 или 0,
у нас был пример — выжил ли пассажир
на «Титанике» или не выжил.
И при этом мы предполагаем, что есть
некая скрытая переменная y со звездочкой,
отвечающая за склонность выжить, и y_i = 1,
если склонность y_i со звездочкой > 0,
и y_i = 0, если скрытая
переменная yi со звездочкой < 0.
Ну и соответственно, уже скрытая
переменная yi со звездочкой линейно
зависит от объясняющих переменных,
yi со звездочкой = β1
+ β_2x_i + случайная составляющая, имеющая,
скажем, логистическое распределение.
Соответственно, эта модель в байесовском
подходе дополняется априорным
распределением о неизвестных параметрах β.
Соответственно, один из
вариантов дополнения,
вполне распространенный, состоит в том,
чтобы сказать, что априорно мы считает,
что β — неизвестные параметры имеют
многомерное нормальное распределение с
вектором средним b0 и ковариационной
матрицей В0 в (−1) степени.
Ну тут вопрос возникает: хорошо,
а как это вот уточнить?
Ну на практике берут совсем конкретную
спецификацию, например, мы считаем,
как правило, чаще всего мы ничего
не знаем о неизвестных параметрах,
поэтому разумно считать, что они лежат в
очень широком диапазоне от (− ∞) до (+∞),
ну или там от (− 100) до (+ 100).
Соответственно, можно сказать,
что математическое ожидание вектора,
параметров, вектора β — это b0,
это — вектор из нулей.
А, соответственно, матрица точности,
обратная матрице, ковариационной матрице,
диагональная матрица, где на диагонали
стоят d, d, d, d, d, вне диагонали — нули.
И вот это d — мало, соответственно,
у нас число d, имеет смысл,
что это 1, деленное на дисперсию
соответствующего коэффициента.
И, соответственно, если d мало,
то 1 на d, дисперсия достаточно высока,
и это отражает мою степень
неуверенности в параметре β.
И, соответственно, можно привести
пример проблемной ситуации: если
у меня объясняющая переменная x
принимает значения: 1, 2 и 3,
а y принимает значения: 0, 0 и 1,
то в этой ситуации оценки метода
максимального правдоподобия не существует.
Однако, если я априорно скажу, скажем,
что β_1 имеет нормальное распределение с
математическим ожиданием 0 и дисперсией
100, ну то есть это означает,
что 95%, я считаю,
что β1 лежит от (− 20) до 20.
А β_2 независимо от β_1 априорно
нормально распределено
с матожиданием 0 и дисперсией 100,
то тогда, оказывается,
с помощью алгоритма Монте-Карло по
схеме Марковской цепи можно установить,
что уравнение для скрытой переменной
оцененной имеет вид — y_i с
крышкой = −10,8 + 4,5 умножить на x_i.
И никаких проблем с несуществованием
оценок у нас в этом случае не оказывается.

