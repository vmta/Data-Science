
﻿1
00:00:13,250 --> 00:00:19,699
Кратко сравним медианную
и классическую регрессии.
В классической регрессии мы задаемся
вопросом: какие факторы связаны
с изменением среднего значения y
при фиксированных регрессорах?
В медианной регрессии мы задаемся
другим вопросом: мы задаемся
вопросом о том от чего зависит
условная медиана y_i-того?
То есть вполне возможно, что в одном
случае будут получаться одни оценки β
с крышкой и они будут состоятельными, а в
другом случае получаются другие оценки β с
крышкой и они тоже будут состоятельными.
Надо еще раз понимать,
что медианная и классическая
регрессии отвечают на разные вопросы.
Поэтому то, что там не совпадают оценки,
— это вполне возможно и
ничего плохого в этом нет.
Одна не является более правильной,
чем другая.
Они отвечают на разные вопросы.
В них совершенно сходна проверка гипотез.
Асимптотически мы можем сказать,
что β с крышкой — коэффициент
оцененной регрессии минус истинное
значение коэффициента делить на
стандартную ошибку, по распределению
эта случайная величина стремится к
нормальной стандартной случайной величине.
То есть фактически при большом количестве
наблюдений способ проверки гипотез,
способ построения доверительных
интервалов будет абсолютно сходный.
Ну, единственное, конечно что оценки β с
крышкой и стандартные ошибки β с крышкой
считаются по разным формулам.
Одни формулы для классической регрессии,
другие формулы для медианной регрессии,
но тем не менее,
в целом подходы совершенно сходны.
И, конечно, надо отметить,
что медиана и среднее медианное
математическое ожидание для
симметричного распределения совпадают,
поэтому, если распределение ε_i-тое
симметричное, то асимптотически никакой
разницы между медианной регрессией
и регрессией среднего не окажется.
Среди минусов медианной регрессии
можно отметить, пожалуй,
что нет явных формул для β с
крышкой и нет явных формул для
стандартных ошибок β с крышкой, то есть
некие численные компьютерные алгоритмы,
которые позволяют их оценить,
но какой-то компактной формулы,
чтобы можно было написать на доске
— такого в медианной регрессии нет.
И, в частности поэтому,
у медианой регрессии нет хороших
распределений для конечных выборок.
То есть в классической регрессии если
предположить нормальность остатков,
то можно получить какие-то
результаты для малых выборок.
В медианной регрессии такое,
к сожалению, не получается.
Даже если ε_i-тые нормальные,
все равно в медианной регрессии
мы не получим каких-то удобных простых
законов распределения в конечной выборке.
Ну плюсом медианной регрессии основным
является то, что она позволяет по-другому
взглянуть на данные,
это очень важно — другой взгляд на данные.
Ну другим тоже хорошим свойством,
но все-таки не таким важным,
как другой взгляд, является то,
что медианная регрессия более устойчива по
сравнению с классической к «выбросам»
— резко экстремальным значениям,
резко отрицательным сильно или сильно
положительным значениям случайной ошибки
ε_i-тое.
И медианную регрессию можно
обобщить до квантильной регрессии.
Поскольку медиана — это, говоря
другим языком, квантиль порядка 50 %,
то есть ниже нее находится
50 % наблюдений,
то можно говорить о квантильной
регрессии порядка τ.
Что такое квантиль порядка τ?
Это такое число, вероятность
попасть левей которого равна τ.
И, соответственно, можно говорить, скажем,
о квантиле порядка 10 % или
о квантиле порядка 90 %.
Соответственно, квантиль
порядка 10 % — это такое число,
вероятность попасть левей которого 10 %.
Соответственно, скажем, квантиль
доходов 10 %-ная — это, соответственно,
такой доход,
ниже которого имеют доходы 10 % населения.
И в квантильной регрессии предполагается,
что квантиль порядка
τ линейно зависит от
объясняющих переменных,
то есть q_τ = β_1 + β_2 * x_i.
И хотя зависимость
предполагается линейной,
но она может быть разной
для разных квантилей.
То есть квантиль порядка 10 % может
зависеть от регрессора одной зависимостью,
а квантиль порядка 90 % может
зависеть от регрессоров,
от объясняющих переменных
другой зависимостью.
То есть хотя зависимость там и там
линейная, она может быть разной линейной.
Для получения оценок в
квантильной регрессии
минимизируется не сумма квадратов
ошибок прогнозов, как в классической,
не сумма модулей ошибок прогнозов,
как в медианной,
а взвешенная или асимметричная
сумма модулей ошибок прогнозов.
А именно сумма w_i-тое —
какие-то веса — помножить
на разницу по модулю y_i-тое
минус прогноз y_i-тое с крышкой.
Соответственно, веса w_i-тое определяются
следующим образом: если в этом
наблюдении y_i-тое меньше, чем прогноз
yi-тое с крышкой, то вес равен 1 – τ,
а если y_i-тое больше, чем y с крышкой,
то вес определяется как τ.
И можно показать, что при выполнении
некоторых предпосылок при минимизации этой
взвешенной асимметричной суммы модулей
ошибок, мы получим состоятельные
оценки β₁ с крышкой и β₂ с крышкой
для коэффициентов β₁ и β₂.
И, соответственно, если вернуться к
изучавшемуся нами набору данных по
стоимости квартир в Москве, если
построить регрессию ну по условно назовем
недорогому жилью 10 %-ный квантиль,
соответственно,
квантиль 10 %-ный зависит как 3,9 + 1,3
умножить на общую площадь квартиры,
а для дорогого жилья — 90
%-ная квантиль окажется что
квантиль зависит как –102 +
3,6 умножить на общую площадь.
Что это означает?
Это означает, что для ну условно
недорогого жилья при росте общей площади
на 1 метр цена растет на 1,3 тысячи y.e.,
а для дорогого жилья
при росте общей площади
на 1 метр цена растет на 3,6 тысяч y.e..
Ну, соответственно, можно изобразить
эту зависимость на графике следующим
образом: по горизонтали отложена
общая площадь квартиры,
по вертикали отложена цена в тысячах y.e.
и, соответственно,
две линии на графике — это зависимость 10
%-ного квантиля в предположении
что она линейная,
и зависимость 90 %-ного квантиля
в предположении что она линейная.
И, соответственно,
эти две линии они позволяют по-другому
взглянуть на наш набор данных.
Они отвечают на вопрос не как средняя
стоимость квартиры зависит от метража,
они отвечают на вопрос как у дорогих
квартир и как у дешевых квартир
выглядит зависимость
цены от общего метража.

