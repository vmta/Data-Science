1
00:00:12,390 --> 00:00:19,530
Перейдем к оценке байесовской
регрессии пик-плато.

2
00:00:19,530 --> 00:00:24,701
Для этого мы воспользуемся
встроенным набором

3
00:00:24,701 --> 00:00:31,206
данных по машинам: регрессия,
spike and slab regression.

4
00:00:31,206 --> 00:00:37,194
Соответственно, мы в набор данных
h поместим данные по машинам.

5
00:00:37,194 --> 00:00:41,109
Ну давайте для того,
чтобы работать не с милями и футами,

6
00:00:41,109 --> 00:00:44,348
а с километрам и метрами,
мы сначала его изменим.

7
00:00:44,348 --> 00:00:49,457
То есть мы сделаем mutate,
изменим набор данных cars, и формулы,

8
00:00:49,457 --> 00:00:55,009
по которым мы будем менять скорость в
километрах в час — это 1,61 умножить

9
00:00:55,009 --> 00:00:59,972
на скорость в милях в час исходное,
которое была в массиве данных и,

10
00:00:59,972 --> 00:01:05,678
соответственно, длина тормозного пути,
чтобы перевести ее

11
00:01:05,678 --> 00:01:11,680
из футов в метры надо помножить на 0,3
исходную длину в наборе данных cars.

12
00:01:11,680 --> 00:01:14,579
Итого, вот у нас есть
привычные переменные,

13
00:01:14,579 --> 00:01:18,854
давайте мы в этот набор данных h,
во-первых, добавим переменную, которую

14
00:01:18,854 --> 00:01:22,886
назовем мусор — просто бессмысленная
переменная чтобы проэспериментировать и

15
00:01:22,886 --> 00:01:27,920
посмотреть как байесовская регрессия
относится к бессмысленной переменной.

16
00:01:27,920 --> 00:01:32,581
Значит, здесь мы просто создадим, сгенерим

17
00:01:32,581 --> 00:01:36,829
нормальные стандартные случайные величины.

18
00:01:36,829 --> 00:01:39,340
Это будет искусственная
мусорная переменная.

19
00:01:39,340 --> 00:01:47,320
Также давайте в набор данных
мы добавим переменную квадрат,

20
00:01:47,320 --> 00:01:52,070
мы изменим набор данных h и
туда добавим квадрат скорости,

21
00:01:52,070 --> 00:01:55,520
ну который, естественно,
будет равен скорости в квадрате.

22
00:01:55,520 --> 00:02:00,740
И теперь мы построим обычную модель,

23
00:02:00,740 --> 00:02:06,108
модель метода наименьших
квадратов по набору h.

24
00:02:06,108 --> 00:02:11,292
Мы построим регрессию длины тормозного

25
00:02:11,292 --> 00:02:17,300
пути на скорость и мусорную переменную,
которая просто так вносит шум.

26
00:02:17,300 --> 00:02:20,910
Здесь плюс конечно надо поставить, плюсик.

27
00:02:20,910 --> 00:02:27,600
Вот и давайте посмотрим на результаты
оценивания классической линейной модели.

28
00:02:27,600 --> 00:02:33,604
То есть мы видим, что здесь у нас
коэффициент, конечно, есть и при скорости

29
00:02:33,604 --> 00:02:38,300
и при переменной мусор, но переменная
мусор она у нас статистически незначима.

30
00:02:38,300 --> 00:02:44,389
А теперь мы по тем же самым данных оценим
регрессию пик-плато то же самой модели.

31
00:02:44,389 --> 00:02:48,307
И на этот раз мы сможем задать прямой
вопрос: а чему же равна вероятность того,

32
00:02:48,307 --> 00:02:52,160
что коэффициент при мусорной переменной
на самом деле в точности ноль.

33
00:02:52,160 --> 00:02:58,606
model_ spike and

34
00:02:58,606 --> 00:03:01,910
slab regression равняется.

35
00:03:01,910 --> 00:03:06,610
Функция называется spike and slab.

36
00:03:06,610 --> 00:03:10,706
Ну здесь надо отметить, что на самом
деле функция spike and slab оценивает,

37
00:03:10,706 --> 00:03:16,594
если быть честным, гораздо более сложные
модели, и это некоторое упрощение

38
00:03:16,594 --> 00:03:21,664
взгляда того, что там только возможен
коэффициент ноль или не ноль,

39
00:03:21,664 --> 00:03:25,070
там самом деле немножко более
тонкий вариант, но примерно такой.

40
00:03:25,070 --> 00:03:28,350
Соответственно, мы возьмем
данные из набора данных h,

41
00:03:28,350 --> 00:03:33,260
формула по которой мы оцениваем
будет точно такая же,

42
00:03:33,260 --> 00:03:38,506
то есть мы изучаем зависимость длины
тормозного пути от истинной скорости

43
00:03:38,506 --> 00:03:43,088
машины и некоторой искусственной
мусорной переменной,

44
00:03:43,088 --> 00:03:45,392
и давайте зададим размер выборки.

45
00:03:45,392 --> 00:03:48,799
По умолчанию, по-моему,
алгоритм генерит 500 наблюдений из

46
00:03:48,799 --> 00:03:51,980
апостериорного распределения,
но тут чем больше точность,

47
00:03:51,980 --> 00:03:55,480
чем больше наблюдений из апостериорного
распределения, тем выше точность.

48
00:03:55,480 --> 00:03:58,531
И единственное, чем вы ограничены
— это временем вычислений.

49
00:03:58,531 --> 00:04:01,530
То есть чем больше выборка из
апостериорного вычисления...

50
00:04:01,530 --> 00:04:04,017
распределения, тем точнее результат.

51
00:04:04,017 --> 00:04:08,118
Ну разве что вам нужно получить результат
очень быстрый, а увеличение выборки

52
00:04:08,118 --> 00:04:11,782
в 2 раза приводит к увеличению времени
там до двух дней или там до трех.

53
00:04:11,782 --> 00:04:16,571
Но в данном случае такого не произойдет,
поэтому мы просто возьмем, скажем,

54
00:04:16,571 --> 00:04:18,010
4000 наблюдений.

55
00:04:18,010 --> 00:04:23,799
И, соответственно,
запустим оцениваться нашу модель.

56
00:04:23,799 --> 00:04:30,553
Ну она занимает долгое время, потому что
алгоритм MCMC он не просто минимизирует,

57
00:04:30,553 --> 00:04:35,596
а там генерятся случайные величины,
и на каждом шагу много операций.

58
00:04:35,596 --> 00:04:39,440
Можем посмотреть, соответственно,
результаты оценивания.

59
00:04:39,440 --> 00:04:43,554
Поскольку пакет по spike and
slab regression еще не очень доработанный,

60
00:04:43,554 --> 00:04:49,258
то тут немножко нестандартный синтаксис:
не summary, а print model_ss.

61
00:04:49,258 --> 00:04:53,648
И, соответственно, мы видим,

62
00:04:53,648 --> 00:04:57,650
что здесь автоматом тоже
отобралась переменная speed,

63
00:04:57,650 --> 00:05:03,380
но мы можем более точно
заглянуть что делал пакет,

64
00:05:03,380 --> 00:05:07,055
можем посмотреть model_ss$ summary.

65
00:05:07,055 --> 00:05:12,025
И вот здесь вот можем
увидеть все-таки оценки

66
00:05:12,025 --> 00:05:15,203
коэффициентов, которые были за кадром.

67
00:05:15,203 --> 00:05:19,650
Вот bma.scale — это оценки
коэффициентов в оригинальном масштабе.

68
00:05:19,650 --> 00:05:23,821
Дело в том, что пакет на самом деле
при осуществлении своих операций он

69
00:05:23,821 --> 00:05:26,450
сначала отмасштабировал все переменные.

70
00:05:26,450 --> 00:05:27,975
Вот.
Ну и здесь видно,

71
00:05:27,975 --> 00:05:32,634
что значит с увеличением скорости
на единичку длина тормозного

72
00:05:32,634 --> 00:05:37,566
пути растет на 0,69, и,
соответственно, мусорная переменная,

73
00:05:37,566 --> 00:05:42,420
с увеличением ее на единичку,
длина тормозного пути растет на 0,20.

74
00:05:42,420 --> 00:05:47,654
Но, тем не менее, алгоритм автоматом
убрал мусорную переменную.

75
00:05:47,654 --> 00:05:51,740
Ну и можно все-таки
посмотреть на вероятности.

76
00:05:51,740 --> 00:05:55,548
Давайте мы посмотрим какие
коэффициенты включались в

77
00:05:55,548 --> 00:06:00,360
каждую их 4000 случайно оцененных,

78
00:06:00,360 --> 00:06:05,060
выборку из 4000 апостериорных
значений гамма.

79
00:06:05,060 --> 00:06:09,480
То есть гамма — это индикатор включалась
или не включалась переменная.

80
00:06:09,480 --> 00:06:15,100
Соответственно, давайте мы введем такой
массив данных included_regressors,

81
00:06:15,100 --> 00:06:19,990
и мы возьмем из

82
00:06:19,990 --> 00:06:25,400
модели ss список,

83
00:06:25,400 --> 00:06:29,590
который называется model.

84
00:06:29,590 --> 00:06:33,820
И применим функцию melt,

85
00:06:33,820 --> 00:06:39,520
которая превратит это в привычный
набор данных, в привычный dataframe.

86
00:06:39,520 --> 00:06:44,165
Ну и давайте чтобы понять,
что такое included

87
00:06:44,165 --> 00:06:48,810
regressors просто посмотрим,
что это такое мы страшное создали.

88
00:06:48,810 --> 00:06:54,160
Это такой здоровый список,

89
00:06:54,160 --> 00:06:59,034
в котором для каждого значения выборки
из апостериорного распределения

90
00:06:59,034 --> 00:07:02,030
указано какие регрессоры туда включались.

91
00:07:02,030 --> 00:07:04,872
Ну поскольку модель случайным
способом оценивается,

92
00:07:04,872 --> 00:07:07,955
то у вас при симуляции могут
получаться другие результаты.

93
00:07:07,955 --> 00:07:09,900
Ну, например, я про свои расскажу.

94
00:07:09,900 --> 00:07:15,250
Вот ежели я возьму 3031-е

95
00:07:15,250 --> 00:07:19,586
значение выборки из
апостериорного распределения,

96
00:07:19,586 --> 00:07:22,890
то там был и первый регрессор, и второй.

97
00:07:22,890 --> 00:07:30,306
Если я возьму, скажем, 3020-е значение
выборки из апостериорного распределения,

98
00:07:30,306 --> 00:07:33,230
то там включался только первый регрессор,
то там включался только первый регрессор.

99
00:07:33,230 --> 00:07:38,766
Ну и соответственно, поскольку я знаю,
что всего у меня размер 4000, то я могу,

100
00:07:38,766 --> 00:07:44,005
ну давайте я посмотрю на начало набора
данных included_regressors вот,

101
00:07:44,005 --> 00:07:48,945
соответственно, здесь есть value и l1.

102
00:07:48,945 --> 00:07:51,420
l1 — это номер наблюдений в выборке,

103
00:07:51,420 --> 00:07:54,570
а value — это какие
регрессоры туда включались.

104
00:07:54,570 --> 00:07:56,252
Вот видно, что в 1-ю,

105
00:07:56,252 --> 00:08:01,249
2-ю и в 3-ю и в 4-ю значения выборки
входил только первый регрессор.

106
00:08:01,249 --> 00:08:03,940
И я могу, например, посчитать сумму,

107
00:08:03,940 --> 00:08:10,846
included_reressors$value равняется,

108
00:08:10,846 --> 00:08:16,990
равняется единичке, и поделить на 4000
— общее количество элементов в выборке,

109
00:08:16,990 --> 00:08:21,924
соответственно, у меня получается,
что регрессор скорость включался в

110
00:08:21,924 --> 00:08:26,120
каждую из значений
апостериорного распределения.

111
00:08:26,120 --> 00:08:30,020
А если я посмотрю,
какова вероятность того,

112
00:08:30,020 --> 00:08:35,022
что переменная мусорная входит в модель,
у нее номер 2 и,

113
00:08:35,022 --> 00:08:39,780
соответственно, здесь я
получу вероятность 0,35.

114
00:08:39,780 --> 00:08:44,991
Соответственно, согласно байесовскому
подходу, у меня получается,

115
00:08:44,991 --> 00:08:49,987
что с вероятностью 1 переменная скорость

116
00:08:49,987 --> 00:08:54,300
влияет на, соответственно,
длину тормозного пути.

117
00:08:54,300 --> 00:08:58,006
Ну и поскольку у нас маленькая выборка,
в нашей выборке всего,

118
00:08:58,006 --> 00:09:01,192
наша выборка называется h nrow(h),

119
00:09:01,192 --> 00:09:05,995
в нашей выборке всего 50 наблюдений,
соответственно, по нашей выборке,

120
00:09:05,995 --> 00:09:09,715
когда мы добавили искусственную
переменную, из-за которой был только шум,

121
00:09:09,715 --> 00:09:15,692
ну по имеющимся данным, при имеющемся
априорном распределении, вероятность того,

122
00:09:15,692 --> 00:09:22,400
что мусорная переменная влияет на
длину тормозного пути равна 0,35.

123
00:09:22,400 --> 00:09:26,820
И на этом примере байесовской

124
00:09:26,820 --> 00:09:31,240
регрессии пик-плато мы закончим наши
компьютерные занятия в этот раз.

