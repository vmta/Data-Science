Приступим к
компьютерной части нашей
сегодняшней лекции.
Открываем файл-заготовку с загрузкой
пакетов: Open file, lab_10_beforeR.
Здесь у нас все необходимые пакеты
подгружаются, ну, их, конечно,
надо установить предварительно,
если они еще не установлены.
Быстренько их пробегаем,
Ctrl+Enter, Ctrl+Enter, Ctrl+Enter.
И, соответственно, загрузим данные
по стоимости квартир в Москве:
f равняется read.table.
Естественно, при этом нам
нужно встать в нужную папку.
Встать в нужную папку можно Session,
Set Working Directory и выбрать ту папку,
где хранится файл.
Файл у нас называется flats_moscow.txt.
Я напомню, что в этом файле есть
заголовок – название переменных,
то есть мы пишем header=TRUE.
Разделителем наблюдений является
табуляция, и разделителем десятичных,
десятичных знаков является точка.
Загрузили данные.
После загрузки всегда надо проверить,
а всё ли хорошо прошло.
Ну, вот в нашем случае всё прошло хорошо,
все данные загрузились,
и теперь мы можем оценить не обычную
регрессию, а квантильную регрессию.
Давайте оценим model квантильную,
для квантиля порядка 01.
Модель квантильной регрессии
оценивается с помощью функций из
пакета quantreg, quantile regression,
и функция называется rq, наверное,
от противоположного порядка слов
regression quantile, rq(data),
мы берем данные из набора данных f,
формулу мы указываем точно так же,
то есть мы говорим price
зависит от общей площади,
и тут нам надо указать квантиль.
Можно сразу tau, можно сразу оценить
модели для нескольких квантилей,
то есть tau,
пусть это будет вектор 0.1, 0.5 и 0.9,
то есть мы сразу оценим три разных модели:
модель для условно дешевых квартир,
квантиль порядка 10 %, медианную
регрессию, квантиль порядка 50 %,
и модель для дорогих квартир,
квантиль порядка 90 %.
Нажимаем Ctrl+Enter,
вот у нас замечательно оценилась модель.
Можем посмотреть summary по модели,
я ее назвал q01,
хотя на самом деле она,
конечно, по всем сразу.
И вот мы видим,
что здесь перечисляются три модели.
Для квантиля порядка 0.1, tau равно 0.1,
коэффициенты 3.93 и 1.31,
и, опять же, перед нами стандартная
совершенно табличка для проверки гипотез.
То есть мы здесь, за кадром не видим,
конечно, что формулы для рассчета
оценок коэффициентов и
стандартных ошибок были другие,
а вот t-статистика считается точно так же.
Автоматом проверяется гипотеза о том,
что коэффициент равен нулю,
то есть t-статистика считается как 3.93
минус 0, делить на стандартную ошибку.
Вот мы видим, что здесь у нас
коэффициент для дешевых квартир,
зависимость стоимости от метража, значим.
Аналогичная зависимость
для медианной регрессии,
то есть для средних квартир,
и для дорогого жилья.
И, соответственно,
мы можем нашу модель визуализировать.
Давайте построим базовый график.
Базовый график – это будет график...
Данные мы берем из набора данных f,
по горизонтали откладываем общую площадь,
по вертикали откладываем цену квартиры.
Соответственно, давайте покажем
на картинке базовый график.
А, я опечатался, не totps,
а totsp, общая площадь.
Теперь мы этот базовый график,
диаграмму рассеивания,
где при горизонтали
отложена площадь квартиры,
а по вертикали – цена, можем дополнить
линиями квантильной регрессии.
То есть мы добавим к базовому
графику сглаживание stat_smooth,
method – это квантильная регрессия,
tau равняется 0.1,
и стандартные ошибки здесь у нас нам не
нужны, мы просто построим саму линию.
Вот, соответственно,
мы провели линию для квантиля 0.1,
и точно так же можем, скопировав этот код,
построить две линии.
Еще одну для квантиля 0.9.
Вот, соответственно, мы получили
две линии квантильной регрессии.
Можем результат обозвать "базовый
график с квантилями base_q" и,
скажем, еще добавить автоматом разделение
на кирпичные и некирпичные дома.
Скажем, мы к базовому графику
с квантилями добавим эстетику,
то, что цвет точек colour будет
определяться переменной brick,
которая отвечала за кирпичность дома.
И, соответственно, здесь у нас brick она
посчитала за непрерывную переменную,
давайте укажем,
что она факторная, factor(brick).
И у нас получится,
теперь компьютер будет знать,
что у переменной brick – два значения,
1 и 0, кирпичные дома и некирпичные.
И вот на этом графике, если его увеличить,
мы уже видим несколько интересных фактов.
Ну, во-первых, мы видим, что естественно
ожидать, что и для дорогого жилья,
и для дешевого жилья с ростом площади
растет цена, при этом наклон,
наклон линии для дорогого жилья,
для 90%-ного квантиля больше,
чем для 10%-ного квантиля, то есть
стоимость дорогого жилья растет быстрее.
И видно, что у кирпичных
домов соответствующие линии
квантильной регрессии еще выше,
чем у некирпичных.
Теперь перейдем к
построению случайного леса,
то есть компьютер автоматом построит
много-много-много-много деревьев.
Значит, соответствующие функции,
они находятся в пакете randomForest.
Ну, к сожалению,
никаких красивых графиков для алгоритма
случайного леса у нас не получится.
Всё, для чего нужен алгоритм случайного
леса –это для прогнозирования.
Соответственно, мы построим
две модели: модель метода
наименьших квадратов
model_lm и модель с помощью
алгоритма случайного леса,
и посмотрим, как они прогнозируют.
Ну, чтобы прогнозирование было честным,
мы предварительно поделим
выборку на две части.
Значит, сначала мы
определим те наблюдения,
которые попадут в тестовую выборку,
и какие попадут в обучающую.
createDataPartition, мы возьмем из
набора данных f переменную price,
количество наблюдений в обучающей
части выборки пусть будет 75 %,
ну и поставим тут опцию list=FALSE.
Соответственно, in_sample,
можно посмотреть, что это такое.
head(in_sample) – это номера наблюдений,
которые у нас войдут в обучающую выборку,
по которой мы будем оценивать модели.
Соответственно, обучающая выборка
f_train – это будет кусок выборки f,
ну, соответственно, те номера,
которые выпали при случайном отборе.
И тестовая часть выборки
f_test – это будет те номера,
которые не выпали при отборе.
И, соответственно, как мы и собирались,
оценим модель линейной регрессии.
Это будет линейная модель,
данные мы возьмем из обучающей
выборки f_train,
а модель может быть сколь угодно сложной.
Пусть будет price равняется общая площадь
плюс площадь кухни плюс жилая
площадь плюс кирпичность дома,
ну, ограничимся такими.
И оценим ту же самую модель с
помощью алгоритма случайного леса.
model_rf равняется,
и здесь функция называется randomForest,
синтаксис точно такой же,
ничего изменять не надо.
Ну, единственное, конечно,
это ни в коем случае не линейная модель,
просто для удобства авторами сохранен
синтаксис функции, но ни в коем случае
здесь никаких коэффициентов β с крышкой
нет, будет построено 500 деревьев,
совершенно не похожих друг на друга,
и прогнозы будут совершенно нелинейные.
Ну, это занимает некоторое время на
компьютере, то есть можно увидеть
задержку, модель оценивается дольше,
чем модель линейной регрессии, и всё,
для чего нам нужна модель случайного
леса – это прогнозировать.
Давайте мы возьмем y из тестовой выборки,
то есть это f_test,
переменная price, и, соответственно,
это настоящие y в тестовой части,
и мы спрогнозируем y с
крышкой по модели lm.
Это будет predict,
мы предсказываем по модели lm.
И новые данные – это f_test,
то есть для тестовой части выборки
мы предсказываем по одной модели,
соответственно, точно так же по
тестовой части выборки предсказываем
для модели случайного леса, и теперь
мы можем сравнить качество прогнозов,
то есть можем посчитать сумму
квадратов ошибок прогнозов.
То есть мы возьмем y минус y
прогнозное по одной модели
в квадрате,
сумму квадратов ошибок по одной модели,
и возьмем сумму квадратов
ошибок по другой модели.
И мы видим в нашем случае, не смотря даже
на то, что мы не включили кучу переменных,
что модель случайного леса
дает ошибки прогноза,
сумму квадратов ошибок прогноза меньше,
чем модель линейной регрессии, ну,
соответственно, если бы мы добавили
объясняющих переменных, этот эффект стал
бы еще сильнее.

