1
00:00:13,250 --> 00:00:16,815
И завершающий сюжет — байесовский подход.

2
00:00:16,815 --> 00:00:22,215
Во-первых, следует обратить внимание,
что байесовский подход

3
00:00:22,215 --> 00:00:27,167
— это именно другой подход к
оцениванию неизвестных коэффициентов.

4
00:00:27,167 --> 00:00:31,524
Это не другая модель, ну то есть вот,
например, есть медианная регрессия — это

5
00:00:31,524 --> 00:00:35,825
одна модель для данных, есть классическая
регрессия — это другая модель для данных,

6
00:00:35,825 --> 00:00:39,334
а байесовский подход,
он устроен принципиально по-иному,

7
00:00:39,334 --> 00:00:43,420
то есть в рамках байесовского подхода
можно изучать классическую регрессию,

8
00:00:43,420 --> 00:00:46,670
в рамках байесовского подхода
можно изучать медианную регрессию.

9
00:00:46,670 --> 00:00:51,880
Байесовский подход — это другой подход
к оцениванию, а не другая модель.

10
00:00:51,880 --> 00:00:56,800
Суть байесовского подхода сводится
к следующей простой идее.

11
00:00:56,800 --> 00:01:01,686
Давайте наше незнание,
нашу неуверенность в истинном значении

12
00:01:01,686 --> 00:01:07,352
параметра θ оформим в виде функции
распределения, в виде функции плотности.

13
00:01:07,352 --> 00:01:12,383
То есть мы не уверены чему равно θ,
но давайте скажем, что мы считаем,

14
00:01:12,383 --> 00:01:17,747
что вот оно в среднем около нуля,
там или что-то в этом духе.

15
00:01:17,747 --> 00:01:23,734
Ну, например: если неизвестным является
параметр вероятности p какого-либо

16
00:01:23,734 --> 00:01:28,614
события, то мы можем предположить,
что у него есть априорная плотность,

17
00:01:28,614 --> 00:01:32,740
мы можем считать априорно,
что p равномерно распределен от 0 до 1.

18
00:01:32,740 --> 00:01:36,930
Ну раз я не знаю, чему равна вероятность,
ну за пределы 0, 1 она выходить не может,

19
00:01:36,930 --> 00:01:41,440
вот я и буду считать,
что она равномерно распределена от 0 до 1.

20
00:01:41,440 --> 00:01:44,376
А потом,
когда я получу какие-то наблюдения,

21
00:01:44,376 --> 00:01:48,540
я изменю свою мнение в соответствии
с законами теории вероятности.

22
00:01:48,540 --> 00:01:53,302
Если какой-то коэффициент, скажем,
я уверен, что он положительный, ну я,

23
00:01:53,302 --> 00:01:58,602
например, из экономических соображений
ожидаю, что предложение какого-то товара,

24
00:01:58,602 --> 00:02:02,862
величина предложения, положительно
связана с ценой этого товара, и,

25
00:02:02,862 --> 00:02:05,413
соответственно, я могу считать,
что коэффициент β,

26
00:02:05,413 --> 00:02:09,323
который показывает эту зависимость,
он положительный, и, соответственно,

27
00:02:09,323 --> 00:02:13,695
я могу считать, что он, допустим, может
иметь экспоненциальное распределение,

28
00:02:13,695 --> 00:02:17,630
то есть функция плотности может быть
сосредоточена на положительных β.

29
00:02:17,630 --> 00:02:22,797
И, соответственно, по прежнему,
у нас остается модель, то есть модель

30
00:02:22,797 --> 00:02:28,360
может быть, например, та же самая
классическая модель линейной регрессии,

31
00:02:28,360 --> 00:02:33,560
где y_i = β₁ + β₂x_i + ε_i,
где ε_i нормально (0, σ квадрат).

32
00:02:33,560 --> 00:02:39,585
Еще раз — изменения касаются моего способа
моделирования неизвестных параметров.

33
00:02:39,585 --> 00:02:45,153
Сейчас я считаю в байесовском подходе,
что неизвестные параметры β1,

34
00:02:45,153 --> 00:02:48,722
β2, σ квадрат — это случайные величины,

35
00:02:48,722 --> 00:02:53,698
и я предполагаю некоторый закон
распределения априорный на них,

36
00:02:53,698 --> 00:02:57,100
который отражает мое
незнание этих параметров.

37
00:02:57,100 --> 00:03:02,095
И логика байесовского подхода,
она предельно проста и понятна —

38
00:03:02,095 --> 00:03:07,090
я специфицирую модель для
наблюдаемых данных, модель для y_i,

39
00:03:07,090 --> 00:03:10,900
которой может быть, например,
классическая модель линейной регрессии,

40
00:03:10,900 --> 00:03:14,630
я специфицирую априорное распределение
на неизвестные параметры.

41
00:03:14,630 --> 00:03:17,358
Например, на — β₁, β₂ и σ квадрат.

42
00:03:17,358 --> 00:03:22,478
И дальше по формулам условной вероятности
я получаю то, что что называется

43
00:03:22,478 --> 00:03:28,110
апостериорным распределением, то есть моим
мнением с учетом имеющейся информации.

44
00:03:28,110 --> 00:03:32,593
То есть до того, как я приступал к анализу
данных, до того, как я имел какие-либо

45
00:03:32,593 --> 00:03:38,050
наблюдения, у меня было заранее
сформировано некоторое мнение,

46
00:03:38,050 --> 00:03:44,127
априорное мнение, после того, как я учел
имеющиеся наблюдения, у меня получилось

47
00:03:44,127 --> 00:03:48,870
апостериорное распределение, то есть мое
мнение с учетом имеющихся наблюдений.

48
00:03:48,870 --> 00:03:53,498
Формула условной вероятности, она,
которая преобразует априорное

49
00:03:53,498 --> 00:03:56,892
распределение в апостериорное
распределение, предельно проста.

50
00:03:56,892 --> 00:04:02,287
Это, соответственно, условная вероятность
y при фиксированных параметрах θ,

51
00:04:02,287 --> 00:04:06,460
помножить на вероятность для параметра θ,
делить на вероятность y.

52
00:04:06,460 --> 00:04:10,138
Ну или тоже самое в непрерывном
случае с функциями плотности.

53
00:04:10,138 --> 00:04:15,360
Но, поскольку нас интересует зависимость
именно от неизвестного параметра,

54
00:04:15,360 --> 00:04:20,153
а в знаменателе неизвестного
параметра θ нет, то можно сказать,

55
00:04:20,153 --> 00:04:24,532
что условная функция плотности θ
при известных y пропорциональна

56
00:04:24,532 --> 00:04:29,022
произведению условной функции
плотности y при известных θ,

57
00:04:29,022 --> 00:04:34,190
помножить на априорную
функцию плотности f от θ.

58
00:04:34,190 --> 00:04:41,812
И сейчас мы на простом примере покажем,
как из априорной информации,

59
00:04:41,812 --> 00:04:46,500
из априорной функции плотности получается
в простейшем случае апостериорное

60
00:04:46,500 --> 00:04:50,160
мнение о неизвестном параметре,
апостериорная функция плотности.

