
﻿1
00:00:13,270 --> 00:00:19,328
В завершающей лекции мы
расскажем о трех сюжетах,
чтобы показать многообразие эконометрики.
А именно эти три сюжета —
это квантильная регрессия,
алгоритм случайного леса
и байесовский подход.
Суть квантильной регрессии состоит в том,
чтобы отказаться
от моделирования среднего, как это
было в классической линейной модели,
а моделировать медиану распределения или
любой другой квантиль распределения.
А именно напомним, что в классической
модели линейной регрессии
предпосылки были
следующие: предполагалось,
то Y_i = β₁ + β₂x_i + ε_i и
предполагалось экзогенность ошибок,
а именно то, что средняя величина ошибки
при известных регрессорах равна 0.
И были еще другие, конечно,
предпосылки, но, в частности,
из первых двух предпосылок следовало то,
что условное математическое ожидание
Y_i при фиксированном X_i
— это есть β₁ + β₂Х_i.
То есть означает, что в классической
линейной регрессионной модели
с увеличением X на 1, среднее значение Y_i,
условное среднее, меняется на β₂.
То есть классическая линейная
регрессионная модель — это модель для
среднего значения Y.
И для оценки этой регрессионной
модели классической мы минимизировали
сумму квадратов ошибок прогноза,
сумму (Y_i- Y_i с крышкой) в квадрате.
И получались замечательные, а именно
состоятельные оценки коэффициентов,
β₁ с крышкой и β₂ с крышкой.
В медианной регрессии
модель состоит в том,
что медиана Y_i условная линейно
зависит β₁ + β₂Х_i от регрессора.
Давайте поясним еще раз разницу
между медианой и средним.
Если представить себе большую-большую
выборку при одинаковых X_i для
Y большую выборку, то,
соответственно, что такое среднее?
Ну, это просто среднее арифметическое,
то есть математическое ожидание похоже на
среднее арифметическое
при большой выборке.
А медиана — это такое число,
выше которого лежит 50 % наблюдений и
ниже которого тоже лежит 50 % наблюдений.
И, соответственно,
в медианной регрессии предполагается,
что не среднее,
а именно медиана зависит от регрессора.
И алгоритм оценивания немножко другой.
Мы минимизируем не сумму квадратов
остатков, а сумму модулей остатков,
сумму |Y_i- Y_i с крышкой|.
И при ее минимизации можно доказать,
что получаются состоятельные
оценки для медианной регрессии.
И сейчас мы рассмотрим совсем простой
пример, где будет одна объясняющая
переменная, и мы получим оценку β
с крышкой для медианной регрессии.
К сожалению,
явных формул в медианной регрессии нет,
но в каждом конкретном случае можно
численно получить оценку β с крышкой.
Рассмотрим пример задачи.
У нас есть некая модель.
На этот раз модель для медианы.
Мы предполагаем, что медиана Y_i при
известном значении X очень просто
зависит от X, а именно равняется просто
некой неизвестной — коэффициент β * Х_i.
Рассмотрим совсем простую
модель для решения руками.
И мы хотим в этой модели получить
оценку коэффициента β с крышкой.
Ну и, соответственно, у нас должны быть,
естественно, какие-то данные,
потому что без данных
оценить модель невозможно.
Ну, к примеру,
у нас будет всего три наблюдения.
Y-зависимая переменная будет
принимать значение 1, 2 и 6,
а X-объясняющая переменная будет
принимать значение 1, 5 и 5.
И я предполагаю, что медиана Y
зависит от X, зависит очень просто.
Вот хочу получить оценку β с крышкой.
Ну, соответственно,
идея медианной регрессии говорит,
что надо выписать, как от β с крышкой
зависит сумма модулей ошибок прогноза.
Ну, естественно, у нас i будет меняться
от 1 до 3 (всего 3 наблюдения),
и если модель устроена следующим образом,
то, соответственно,
у нас Y_i с крышкой,
прогноз Y будет устроен довольно просто.
Это будет β с крышкой помножить
на значение регрессора.
Ну и в нашем случае конкретном
трех наблюдений мы получаем
следующую формулу для суммы
модулей ошибок прогноза.
Это, соответственно, |Y₁- Y₁ с
крышкой| ошибка прогноза для первого
наблюдения плюс ошибка прогноза
для второго наблюдения по модулю
плюс ошибка прогноза для
третьего наблюдения.
Ну и в нашем случае поскольку прогнозы
у нас считаются в соответствии
с формулой для медианы,
то мы получаем |1- β с
крышкой * 1| + |2
(второй Y)- (формула для второго прогноза)
β с крышкой * 5 (второй X)|
+ |6 (последний Y) -
β крышкой * 5| И, соответственно,
медианная регрессия говорит,
что вот эту сумму надо минимизировать,
подбирая какое-нибудь β с крышкой, которое
обеспечит минимальную сумму модуля ошибок.
Ну, к сожалению или к счастью, эта функция
недифференцируемая, но она достаточно
проста, то есть график функции M(β
с крышкой) он довольно простой.
Здесь просто модули, и поэтому мы
знаем что при разных значениях β,
модули будут по-разному раскрываться,
но в любом случае на каждом участке,
где бы ни раскрывались модули,
будет получаться линейная функция.
Соответственно, наша вся функция M(β
с крышкой) – она кусочно-линейная,
кусочно-линейная, и благодаря этому
факту мы легко построим ее график.
Давайте поймем, где у нее изломы?
Ну, изломы там, где выражение
внутри модуля будет менять знак.
Первое выражение внутри модуля будет
менять знак, когда β с крышкой равно 1,
изломы этой функции — β с крышкой равно 1,
и значение функции в точке 1...
Ну, просто подставляем,
получаем, что это равно 4.
Другой излом, когда второй модуль равен 0,
то есть в точке β с крышкой равное 2/5,
и значение функции в точке 2/5,
если подставить 2/5 сюда, сюда и сюда
и все это сложить, то получится 4,6.
И последний излом нашей
функции в точке β с крышкой,
когда последний модуль равен 0, 6/5.
Соответственно, значение
функции в точке 6/5,
просто подставляем 6/5 в нашу функцию.
Получаем, что это равно 4,2.
И мы уже можем нарисовать
график нашей функции.
Значит, во вертикали мы откладываем
сумму модулей ошибок прогноза,
а по горизонтали откладываем β с крышкой.
Мы знаем,
что изломы в точке 0,4; 1 и 1,2 (это 6/5).
Соответственно, вот один
излом в точке 0,4 (это 2/5),
другой излом в точке 1 и
еще один излом в точке 1,2.
Значение функции в единичке — 4.
Масштаб у меня по горизонтали и по
вертикали может не соответствовать,
поэтому пусть будет тут 4, в 1,2 чуть
побольше и в единичке...
в 0,4 еще больше.
Соответственно, на этих участках функция
выглядит кусочно-линейно вот так.
Но остается понять, как функция будет
выглядеть за пределами этих участков.
Ну, например, если β с крышкой очень-очень
маленькое, очень отрицательное, ну,
например, 0,
то все модули раскрываются с плюсом,
и функция очень быстро убывает, то есть
у нее здесь резко отрицательный наклон.
Если все модули раскрыть, просто убрать
палочки вертикальные, то получится
итоговый наклон (- β с крышкой)- 5β с
крышкой- 5β с крышкой- 11β с крышкой.
Вот здесь вот наклон (-11).
А здесь, соответственно,
все модули раскроются с другим знаком,
и наклон будет положительный.
Вот так выглядит наша функция.
И несмотря на то,
что она не дифференцируема,
мы не можем взять производную,
приравнять к нулю, тем не менее, очевидно,
в прямом смысле этого слова,
что оптимальная оценка,
которая минимизирует сумму модулей ошибок
прогноза β с крышкой равно 1.
Таким образом,
мы в простом примере показали,
как получить оценку медианной регрессии.

