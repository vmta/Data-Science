1
00:00:13,620 --> 00:00:18,594
Мы можем применить байесовский подход и к

2
00:00:18,594 --> 00:00:21,996
нашей стандартной задаче классической
линейной регрессионной модели.

3
00:00:21,996 --> 00:00:26,997
То есть модель у нас та
же самая: y_i = β_1 + β_2x_i

4
00:00:26,997 --> 00:00:32,937
+ β_3z_i + ε_i Но мы дополняем
эту модель априорным мнением

5
00:00:32,937 --> 00:00:38,601
о неизвестных коэффициентах β_1,
β_2, β_3 и параметре σ квадрат.

6
00:00:38,601 --> 00:00:43,398
И мы сейчас дополним нашу модель
априорным распределением,

7
00:00:43,398 --> 00:00:46,171
которое называется регрессия пик-плато.

8
00:00:46,171 --> 00:00:49,052
Суть состоит в следующем:
мы будем предполагать,

9
00:00:49,052 --> 00:00:55,081
что коэффициенты β либо
точно 0 с вероятностью 1/2,

10
00:00:55,081 --> 00:00:58,779
либо «непонятно что» с вероятностью 1/2.

11
00:00:58,779 --> 00:01:03,597
Соответственно, это распределение можно
формально математически записать следующим

12
00:01:03,597 --> 00:01:07,939
образом: что β_j, j-тый коэффициент модели,
имеет нормальное распределение с

13
00:01:07,939 --> 00:01:15,554
математическим ожиданием 0 и некоторой
дисперсией — дисперсия γ_j * τ_j в квадрате,

14
00:01:15,554 --> 00:01:21,440
где γ_j либо 1 с вероятностью 1/2,
либо 0 с вероятностью 1/2.

15
00:01:21,440 --> 00:01:26,210
Соответственно, если вот этот
множитель γ_j оказывается равным 0,

16
00:01:26,210 --> 00:01:31,117
то получается, что β_j имеет нормальное
распределение с математическим

17
00:01:31,117 --> 00:01:35,740
ожиданием 0 и дисперсией 0, а что такое
случайная величина с дисперсией 0?

18
00:01:35,740 --> 00:01:36,620
Это константа.

19
00:01:36,620 --> 00:01:40,647
То есть если γ_j принимает значение 0,
получается,

20
00:01:40,647 --> 00:01:44,561
что β_j-тое в точности равно
0 с вероятностью 1/2.

21
00:01:44,561 --> 00:01:49,330
Получается как бы такой острый пик,
мы с вероятностью 1/2 β_j равно 0,

22
00:01:49,330 --> 00:01:54,118
а с вероятностью 1/2 γ_j окажется

23
00:01:54,118 --> 00:01:59,600
равным 1 и дисперсия β_j,
то есть наше априорное

24
00:01:59,600 --> 00:02:04,605
мнение будет состоять в том,
что дисперсия β_j равна τ_j в квадрате.

25
00:02:04,605 --> 00:02:09,631
И τ_j в квадрате предполагается
случайной величиной,

26
00:02:09,631 --> 00:02:12,235
имеющей обратное гамма-распределение.

27
00:02:12,235 --> 00:02:15,780
Для тех, кто не знает,
что такое обратное гамма-распределение,

28
00:02:15,780 --> 00:02:19,219
можно просто заметить,
что это некое распределение,

29
00:02:19,219 --> 00:02:23,400
которое гарантирует неотрицательность
величины τ_j в квадрате.

30
00:02:23,400 --> 00:02:26,587
Ну, поскольку,
дисперсия не бывает отрицательной,

31
00:02:26,587 --> 00:02:31,720
то от обратного гамма-распределения
требуется, чтобы оно было неотрицательным,

32
00:02:31,720 --> 00:02:36,720
и, соответственно, a_1 и a_2 — это
какие-то параметры, которые определяют

33
00:02:36,720 --> 00:02:41,850
форму обратного гамма-распределения.

34
00:02:41,850 --> 00:02:46,070
И, соответственно,
мы выберем форму гамма-распределения так,

35
00:02:46,070 --> 00:02:50,290
чтобы τ_j в квадрате принимало
довольно большие значения.

36
00:02:50,290 --> 00:02:55,671
Тогда получится, что с вероятностью
1/2 дисперсия β_j равна 0,

37
00:02:55,671 --> 00:03:02,382
то есть мы точно уверены, что β_j равно 0,
и с вероятностью 1/2 дисперсия β_j,

38
00:03:02,382 --> 00:03:07,555
τj в квадрате принимает огромное значение,
и это означает,

39
00:03:07,555 --> 00:03:12,754
что мы абсолютно не уверенны в том,
какое же значение принимает β_j.

40
00:03:12,754 --> 00:03:16,906
Вот получается у нас такая
смесь пика (мы точно уверены,

41
00:03:16,906 --> 00:03:22,084
что коэффициент равен 0) и плато (мы
не знаем, где коэффициент лежит).

42
00:03:22,084 --> 00:03:25,384
Поэтому эта регрессия
называется регрессия пик-плато.

43
00:03:25,384 --> 00:03:29,755
Ну и, конечно, еще априорное
распределение также должно моделировать,

44
00:03:29,755 --> 00:03:34,170
задавать законы распределения
неизвестного параметра σ-квадрат.

45
00:03:34,170 --> 00:03:38,100
Здесь тоже мы пользуемся
обратным гамма-распределением,

46
00:03:38,100 --> 00:03:42,900
чтобы гарантировать неотрицательность
параметра σ квадрат.

47
00:03:42,900 --> 00:03:48,623
И, соответственно, опять же, явных формул
тут никаких для оценок коэффициентов нет.

48
00:03:48,623 --> 00:03:54,001
Мы применяем алгоритм Монте-Карло по
схеме марковской цепи и на выходе,

49
00:03:54,001 --> 00:03:58,632
имея данные предпосылки,
на выходе получаем выборку из

50
00:03:58,632 --> 00:04:04,412
апостериорного закона распределения β_1,
β_2 β_3 и σ-квадрат.

51
00:04:04,412 --> 00:04:11,626
Достоинство регрессии пик-плато состоит в
следующем: она позволяет напрямую отвечать

52
00:04:11,626 --> 00:04:16,370
на вопрос: чему же равна вероятность того,
что коэффициент на самом деле 0?

53
00:04:16,370 --> 00:04:22,300
То есть вопрос о том, что вероятность
того, что β_2 = 0 при заданных наблюдениях,

54
00:04:22,300 --> 00:04:28,060
он вполне корректен, и на него можно в
рамках регрессии пик-плато ответить.

55
00:04:28,060 --> 00:04:32,129
Тот же самый вопрос в рамках
классической модели он бессмысленный.

56
00:04:32,129 --> 00:04:37,018
Мы могли проверять гипотезы,
но проверка гипотез — это

57
00:04:37,018 --> 00:04:43,119
все-таки не более, чем попытка
как-то приблизиться к этому ответу.

58
00:04:43,119 --> 00:04:47,870
Мы как бы говорим, что данные не сильно
противоречат гипотезе о том, что β_2 = 0,

59
00:04:47,870 --> 00:04:53,601
но на вопрос «так равно ли β_2 нулю или
нет?» мы в классическом подходе никогда не

60
00:04:53,601 --> 00:04:57,712
отвечаем, а регрессия пик-плато позволяет
прямо ответить на этот вопрос и сказать,

61
00:04:57,712 --> 00:04:59,110
чему равна эта вероятность.

62
00:04:59,110 --> 00:05:03,598
И, соответственно,
если вернуться к тому набору данных,

63
00:05:03,598 --> 00:05:08,247
с рассмотрения которого мы
начали наш курс эконометрики,

64
00:05:08,247 --> 00:05:14,191
с набора данных по машинам 1920-х годов,
где вот на графике по горизонтали

65
00:05:14,191 --> 00:05:19,727
отложена скорость машины в км/час, по
вертикали отложена длина тормозного пути.

66
00:05:19,727 --> 00:05:24,460
И мы можем оценить байесовскую
модель регрессию пик-плато,

67
00:05:24,460 --> 00:05:30,043
предположив, что длина тормозного пути
зависит от скорости и квадрата скорости.

68
00:05:30,043 --> 00:05:35,315
Получить апостериорное
распределение для β_1, β_2 и β_3.

69
00:05:35,315 --> 00:05:40,243
Ну, в регрессию имеет смысл записать,
конечно, средние значения,

70
00:05:40,243 --> 00:05:42,990
апостериорные математические ожидания.

71
00:05:42,990 --> 00:05:49,109
То есть 12,81; 0,28 и 0,01,
оценки коэффициентов —

72
00:05:49,109 --> 00:05:55,411
это апостериорные математические ожидания
коэффициентов β_1, β_2 и β_3 соответственно.

73
00:05:55,411 --> 00:05:59,702
И, соответственно, помимо этого уравнение,
которое прекрасно позволяет, например,

74
00:05:59,702 --> 00:06:00,590
прогнозировать.

75
00:06:00,590 --> 00:06:03,931
Мы теперь можем напрямую
ответить на вопрос...

76
00:06:03,931 --> 00:06:08,035
Поскольку за кадром у нас имеется
выборка из 1000 или 2000,

77
00:06:08,035 --> 00:06:11,113
или 10 000, сколько захотим,
значений β1, β2 и β3,

78
00:06:11,113 --> 00:06:14,391
мы можем напрямую ответить на
вопрос: а какой процент из них,

79
00:06:14,391 --> 00:06:18,930
какой процент в выборке из апостериорного
распределения оказались равны 0?

80
00:06:18,930 --> 00:06:22,658
И в данном примере оказывается,
что вероятносто того,

81
00:06:22,658 --> 00:06:26,559
что коэффициент при скорости равен 0,
она 0,15,

82
00:06:26,559 --> 00:06:32,910
а коэффициент при скорости в квадрате
равен 0, эта вероятность равна 5 %.

83
00:06:32,910 --> 00:06:36,787
То есть, соответственно, байесовская
регрессия позволяет нам сказать,

84
00:06:36,787 --> 00:06:41,522
что вот имеет смысл из двух коэффициентов,
если включать только один,

85
00:06:41,522 --> 00:06:44,630
то имеет смысл включить
скорость в квадрате.

86
00:06:44,630 --> 00:06:48,201
Ну, можно включить и оба.

87
00:06:48,201 --> 00:06:55,600
И на этом примере байесовской
регрессии мы заканчиваем наш курс.

88
00:06:55,600 --> 00:07:00,270
Большое-большое спасибо всем тем,
кто прослушал этот курс до конца.

89
00:07:00,270 --> 00:07:07,050
Мы всей командой получили очень большое
удовольствие от создания данного курса.

90
00:07:07,050 --> 00:07:10,234
И хотелось бы завершить его цитатой,

91
00:07:10,234 --> 00:07:14,682
что на самом деле мы ни в
коем случае не закончили курс

92
00:07:14,682 --> 00:07:19,995
эконометрики и мы скорее
поставили больше вопросов.

93
00:07:19,995 --> 00:07:25,190
По ходу курса мы поставили больше задач,
чем смогли решить.

94
00:07:25,190 --> 00:07:28,804
И в каком-то смысле мы
также ничего не знаем,

95
00:07:28,804 --> 00:07:33,338
как и до начала изучения курса
эконометрики, но мы верим,

96
00:07:33,338 --> 00:07:38,960
что наше незнание стало глубже, а не знаем
мы все более и более интересные вещи.

