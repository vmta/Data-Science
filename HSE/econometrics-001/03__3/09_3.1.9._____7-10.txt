Подведем итог и пополним
список хороших свойств,
большой список хороших свойств,
которыми обладают наши коэффициенты.
Если выполнен ряд предпосылок,
а именно истинная зависимость
имеет линейный вид 
y_i = β₁ + β₂ x_i + β_3 z_i + ε_i.
В матричном виде: y = xβ + ε.
Если мы оцениваем эту же модель с
помощью метода наименьших квадратов,
то есть мы, действительно, строим
регрессию у на константу и те переменных,
от которых он зависит.
В матричном виде это приведет к
оценке β с крышкой = (Х'X)^(-1) X'y.
Если наблюдений больше,
чем оцениваемых коэффициентов,
если имеет место строгая экзогенность,
то есть математическое ожидание от
случайной ошибки ε_i при фиксированных
регрессорах равно нулю.
Если имеет место условная
гомоскедастичность, то есть математическое
ожидание от ε_i в квадрате при
фиксированных регрессорах равно σ квадрат,
или, что тоже самое, что дисперсия ε_i при
фиксированных регрессорах равна σ квадрат.
Если имеет место условная
некоррелированность случайных ошибок,
корреляция ε_i, ε_j при фиксированном X 
равна нулю,
а так же выполнены предпосылки на
регрессоры, а именно отдельные
наблюдения являются случайной выборкой
из некоего большого набора объектов.
То есть регрессоры, относящиеся к
разным наблюдениям, независимы,
и вместе с тем разные наблюдения имеют
одинаковые законы распределения.
И кроме того, последняя предпосылка,
что с вероятностью 1 среди объясняющих
переменных, среди регрессоров,
отсутствует линейная зависимость,
то у нас есть ряд свойств.
Базовые свойства никак не меняются,
а вот теперь мы можем проверять гипотезу
о нескольких линейных ограничения сразу.
То есть у нас в асимптотических
свойствах появляется одно новое.
А именно: χ² – статистика,
рассчитываемая по формуле 
(RSS_R - RSS_UR), деленное на 
RSS_UR, деленное на (n - k),
эта величина стремится к χ² - распределению 
с r степенями свободы.
r — это количество предполагаемых
ограничений верных.
Это свойство добавляется
к старым свойствам.
То, что β_j стремится по вероятности, β_j 
с крышкой стремится по вероятности к β_j,
что дробь (β_j с крышкой минус β_j) делить
на соответствующую стандартную ошибку,
имеет асимптотически
нормальное распределение,
и σ квадрат с крышкой является
состоятельной оценкой для σ квадрат.
И точно также добавляется
соответствующее новое свойство,
тоже хорошее свойство при нормальности ε_i.
При нормальности ε_i мы
имеем новое свойство.
F-статистика равное 
(RSS_R - RSS_UR),
деленное на количество ограничений r,
делить на RSS_UR, делить на (n - k_un),
это случайная
величина имеет F-распределение 
с r,(n - k) степенями свободы.
Давайте в рамках этих предпосылок
отдельно оговорим два особых случая.
А что произойдет,
если я включу лишние переменные.
То есть на самом деле y зависит только от
x, а я этого не знаю же на самом деле.
И я буду строить, оценивать регрессию,
оценивать зависимость y от x и от z.
Что произойдет в этом случае,
когда я включу лишнюю переменную?
И что произойдет в противоположенном
случае, когда я не включу переменную,
от которой на самом деле зависимость есть?
Случай с лишними переменными, он,
к счастью, очень оптимистичный.
Из всей этой армады хороших свойств
потерянным в этом случае окажется только
свойство эффективности.
То есть оценки останутся несмещенными,
останутся состоятельными.
С ростом количества наблюдений они будут
все более и более похожи на настоящее β.
Можно будет проверять гипотезы по
старым формулам, гипотезы об отдельных
коэффициентах, гипотезы о нескольких
линейных ограничениях сразу.
Единственное, что будет
потеряно — оказывается,
что если какие-то коэффициенты на
самом деле, β настоящие, равны нулю,
то при оценки модели с лишними
регрессорами, вы потеряете эффективность.
То есть дисперсия ваших оценок β с
крышкой будет больше, чем могла бы быть,
если бы вы не включили эти коэффициенты.
То есть мораль отсюда, что, как бы,
если у вас есть переменные, от которых
зависимости нет, и вы об этом знаете,
то лучше их в модель не включать, тогда вы
получите более короткие доверительные
интервалы для неизвестных коэффициентов.
Более короткие доверительные интервалы для
прогнозов, то есть вы вернете потерянную
эффективность, если не
включите лишние переменные.
Противоположенный случай,
случай пропущенных переменных,
когда на самом деле истинная зависимость
имеет вид y_i = β₁ + β₂ х_i + β_3 z_i + ε_i,
а оценена регрессия на меньшее
количество переменных,
то, к несчастью, все плохо.
Единственное свойство,
которое осталось у оценок, это то,
что они остались линейными по y.
Все остальные свойства пропали.
Оценки стали смещенными, несостоятельными,
проверять гипотезы нельзя,
строить доверительные интервалы по
формулам, которые у нас есть, нельзя.
И из-за этой вот такой
вот совершенно разной,
разных последствиях неправильных действий,
видно, что лучше включить лишнюю
переменную, чем не включить нужную.
Поэтому, если у нас есть
теоретические основания считать,
что зависимость от переменной z есть,
а вы оцениваете регрессию,
она не значима, лучше ее оставить.
Если есть теоретические основания считать,
что зависимость от z есть,
значит z лучше оставить.
Если переменная, наоборот,
если переменная значима, но теория
говорит: ну не должно быть зависимости
от этой переменной, она вот значима.
Все равно лучше такую переменную оставить,
потому что невключение нужной переменной
— это гораздо страшнее ошибка,
чем включение ненужной переменной.
Конечно, не стоит впадать в крайности
и включать слишком много ненужных
переменных, от этого у вас доверительные
интервалы для коэффициентов станут очень
и очень широкими, и проверять гипотезы
тоже будет как-то бессмысленно,
ни одна гипотеза не будет отвергаться.
Но тем не менее, о важности этих двух
ошибок можно судить по данных выводам.

