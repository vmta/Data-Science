1
00:00:13,270 --> 00:00:18,204
Человек всегда пытается найти какое-то
простое решение для любой сложной

2
00:00:18,204 --> 00:00:18,975
проблемы.

3
00:00:18,975 --> 00:00:22,381
Иногда, правда,
это простое решение бывает неправильным,

4
00:00:22,381 --> 00:00:25,893
но тем не менее такое вот стремление,
— оно всегда есть: вместо сложной задачи

5
00:00:25,893 --> 00:00:28,700
хочется вот какой-то простой,
понятный, доступный ответ.

6
00:00:28,700 --> 00:00:33,074
И поэтому вместо всех этих f-тестов,
проверки гипотез о том,

7
00:00:33,074 --> 00:00:36,080
эту модель выбрать или эту модель выбрать,

8
00:00:36,080 --> 00:00:40,130
человек хочет какой-нибудь простой
показатель, чтобы вот посчитал его,

9
00:00:40,130 --> 00:00:44,493
и чем больше, тем модель лучше,
ну или чем меньше, тем модель лучше.

10
00:00:44,493 --> 00:00:49,854
Но, к сожалению, так просто поступить не
удаётся, иначе бы тогда никакой науки

11
00:00:49,854 --> 00:00:55,790
не было, а был бы вот этот показатель,
все бы его считали и смотрели, где лучше.

12
00:00:55,790 --> 00:01:00,450
Вот, но тем не менее какие-то
попытки упростить ситуацию и вместо

13
00:01:00,450 --> 00:01:04,007
тестирования гипотез просто посчитать
какой-то показатель быстрого взгляда

14
00:01:04,007 --> 00:01:08,213
на который достаточно, чтобы получить
первое представление о модели,

15
00:01:08,213 --> 00:01:09,960
— такие показатели есть.

16
00:01:09,960 --> 00:01:13,520
Значит, ну, во-первых,
надо сказать об R-квадрат.

17
00:01:13,520 --> 00:01:16,655
Его недостатком является то, что он,

18
00:01:16,655 --> 00:01:21,460
как мы выяснили, в неограниченной модели
всегда больше, чем в ограниченной.

19
00:01:21,460 --> 00:01:26,130
И поэтому выбрать на его основании
ограниченную и неограниченную нельзя,

20
00:01:26,130 --> 00:01:31,926
но тем не менее некий простой показатель
качества, — насколько спрогнозированная

21
00:01:31,926 --> 00:01:37,430
y с крышкой похожа на y,
— он представление даёт.

22
00:01:37,430 --> 00:01:41,950
Мы говорили о том, что R-квадрат — это

23
00:01:41,950 --> 00:01:45,940
квадрат выборочной корреляции
между y с крышкой и y.

24
00:01:45,940 --> 00:01:49,230
Соответственно, чтобы
избавиться от этой проблемы,

25
00:01:49,230 --> 00:01:53,850
что R-квадрат всегда в неограниченной
модели больше, чем в ограниченной,

26
00:01:53,850 --> 00:01:58,440
был придуман показатель R-квадрат
adjusted (скорректированный R-квадрат).

27
00:01:58,440 --> 00:02:00,294
В чём его суть?

28
00:02:00,294 --> 00:02:03,747
Ну хорошо, в более сложной модели,

29
00:02:03,747 --> 00:02:08,115
где больше коэффициентов, там,
соответственно, RSS меньше.

30
00:02:08,115 --> 00:02:12,613
Ну вот давайте, соответственно,
мы как-то более сложную модель «накажем»,

31
00:02:12,613 --> 00:02:17,067
то есть введём туда штраф за
количество коэффициентов.

32
00:02:17,067 --> 00:02:22,840
В формуле для обычного R-квадрат, он равен
единичка минус RSS, делённая на TSS.

33
00:02:22,840 --> 00:02:26,438
В R-квадрат adjusted,
в скорректированном R-квадрате,

34
00:02:26,438 --> 00:02:32,137
из единички вычитается RSS и TSS, делённые
соответственно на (n – k) и на (n – 1).

35
00:02:32,137 --> 00:02:37,787
Соответственно, с ростом k R-квадрат
adjusted при прочих равных будет падать,

36
00:02:37,787 --> 00:02:42,659
то есть мы «оштрафовали» R-квадрат
на количество регрессоров k,

37
00:02:42,659 --> 00:02:46,640
вернее, на количество
оцениваемых коэффициентов k.

38
00:02:46,640 --> 00:02:50,895
И, соответственно, вот по R-квадрат

39
00:02:50,895 --> 00:02:55,325
adjusted уже грубо можно сравнивать
ограниченную и неограниченную модели.

40
00:02:55,325 --> 00:02:59,535
Посмотрел на R-квадрат adjusted — если
один сильно-сильно-сильно больше,

41
00:02:59,535 --> 00:03:03,003
чем другой, то, соответственно,
скорее всего, мы выберем ту модель,

42
00:03:03,003 --> 00:03:05,790
где R-квадрат, скорректированный
R-квадрат adjusted, выше.

43
00:03:05,790 --> 00:03:07,119
Можно отметить,

44
00:03:07,119 --> 00:03:13,186
что сравнение R-квадратов adjusted — оно
равносильно сравнению σ-квадрат с крышкой.

45
00:03:13,186 --> 00:03:16,120
Если в двух моделях,
ограниченной и неограниченной,

46
00:03:16,120 --> 00:03:21,992
в одной R-квадрат adjusted больше,
то в ней σ-квадрат с крышкой будет меньше.

47
00:03:21,992 --> 00:03:27,485
Следующие два показателя качества,
критерий Акаике и критерий Шварца,

48
00:03:27,485 --> 00:03:30,461
они реализуют ту же самую мысль,
что модель плохая,

49
00:03:30,461 --> 00:03:34,110
если а) она плохо предсказывает
и если б) она слишком сложная.

50
00:03:34,110 --> 00:03:38,720
То есть если две модели одинаково хорошо
предсказывают, то лучше та, которая проще.

51
00:03:38,720 --> 00:03:43,623
Если две модели одинаково сложные,
в них одинаковое количество

52
00:03:43,623 --> 00:03:49,020
объясняющих переменных, то хуже та,
которая хуже предсказывает.

53
00:03:49,020 --> 00:03:52,642
Соответственно, штраф,

54
00:03:52,642 --> 00:03:58,147
некая плохая характеристика модели,
то есть штраф за модель,

55
00:03:58,147 --> 00:04:03,260
— это штраф за высокую RSS,
за высокую сумму квадратов остатков,

56
00:04:03,260 --> 00:04:08,050
и за высокое количество
объясняющих переменных.

57
00:04:08,050 --> 00:04:13,905
В штрафном критерии Акаике это,
соответственно,

58
00:04:13,905 --> 00:04:18,396
разбивается сумма штрафа на штраф за
высокий RSS, то есть это n на логарифм

59
00:04:18,396 --> 00:04:23,850
RSS, деленного на n, плюс штраф за
большое количество регрессоров, 2k.

60
00:04:23,850 --> 00:04:29,412
И в критерии Шварца соответствующий
штраф за количество регрессоров

61
00:04:29,412 --> 00:04:33,890
— это логарифм n, помноженный на k, на
количество регрессоров: чем больше k при

62
00:04:33,890 --> 00:04:38,916
прочих равных, тем выше штраф
Акаике и тем выше штраф Шварца.

63
00:04:38,916 --> 00:04:43,250
Соответственно, руководствуясь
этими двумя критериями,

64
00:04:43,250 --> 00:04:48,690
мы выбираем ту модель, в которой
штрафной критерий AIC или BIC меньше.

65
00:04:48,690 --> 00:04:54,163
Ну, соответственно, у этих критериев
есть достаточно серьёзное математическое

66
00:04:54,163 --> 00:04:58,450
обоснование за ним: тут неспроста
взят логарифм RSS, делённое на n,

67
00:04:58,450 --> 00:05:03,757
неспроста взят логарифм n, помноженный на
k, но оно выходит за рамки данного курса.

68
00:05:03,757 --> 00:05:08,150
Здесь надо просто представлять себе,
что есть два штрафных критерия,

69
00:05:08,150 --> 00:05:13,925
которые реализуют попытку исследователей
сделать какой-то простой критерий,

70
00:05:13,925 --> 00:05:17,150
по которому глянул на
модель и сразу понял,

71
00:05:17,150 --> 00:05:22,161
как-то хоть пусть упрощённо,
но зато быстро, — какая модель лучше.

72
00:05:22,161 --> 00:05:25,700
В данном случае лучше та,
где штрафной критерий меньше.

73
00:05:25,700 --> 00:05:31,498
В сегодняшней лекции мы, соответственно,
поговорили о нескольких сюжетах.

74
00:05:31,498 --> 00:05:36,227
Мы разобрали, как проводить
прогнозирование, как строить доверительные

75
00:05:36,227 --> 00:05:41,846
предиктивные интервалы при
прогнозировании, мы рассмотрели гипотезу,

76
00:05:41,846 --> 00:05:48,525
как проверять наличие нескольких линейных
ограничений, провели тест Рамсея,

77
00:05:48,525 --> 00:05:53,130
который позволяет нам судить о том,
а не забыли ли мы какие-нибудь переменные.

78
00:05:53,130 --> 00:05:58,648
А в следующих лекциях мы
поговорим о разных неприятностях,

79
00:05:58,648 --> 00:06:02,485
которые ждут исследователя,
который занимается оценкой регрессии.

80
00:06:02,485 --> 00:06:04,596
Именно мы поговорим о
гетероскедастичности,

81
00:06:04,596 --> 00:06:06,560
мультиколлинеарности, авторегрессии.

