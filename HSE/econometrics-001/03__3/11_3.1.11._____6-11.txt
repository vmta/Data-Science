Человек всегда пытается найти какое-то
простое решение для любой сложной
проблемы.
Иногда, правда,
это простое решение бывает неправильным,
но тем не менее такое вот стремление,
— оно всегда есть: вместо сложной задачи
хочется вот какой-то простой,
понятный, доступный ответ.
И поэтому вместо всех этих f-тестов,
проверки гипотез о том,
эту модель выбрать или эту модель выбрать,
человек хочет какой-нибудь простой
показатель, чтобы вот посчитал его,
и чем больше, тем модель лучше,
ну или чем меньше, тем модель лучше.
Но, к сожалению, так просто поступить не
удаётся, иначе бы тогда никакой науки
не было, а был бы вот этот показатель,
все бы его считали и смотрели, где лучше.
Вот, но тем не менее какие-то
попытки упростить ситуацию и вместо
тестирования гипотез просто посчитать
какой-то показатель быстрого взгляда
на который достаточно, чтобы получить
первое представление о модели,
— такие показатели есть.
Значит, ну, во-первых,
надо сказать об R-квадрат.
Его недостатком является то, что он,
как мы выяснили, в неограниченной модели
всегда больше, чем в ограниченной.
И поэтому выбрать на его основании
ограниченную и неограниченную нельзя,
но тем не менее некий простой показатель
качества, — насколько спрогнозированная
y с крышкой похожа на y,
— он представление даёт.
Мы говорили о том, что R-квадрат — это
квадрат выборочной корреляции
между y с крышкой и y.
Соответственно, чтобы
избавиться от этой проблемы,
что R-квадрат всегда в неограниченной
модели больше, чем в ограниченной,
был придуман показатель R-квадрат
adjusted (скорректированный R-квадрат).
В чём его суть?
Ну хорошо, в более сложной модели,
где больше коэффициентов, там,
соответственно, RSS меньше.
Ну вот давайте, соответственно,
мы как-то более сложную модель «накажем»,
то есть введём туда штраф за
количество коэффициентов.
В формуле для обычного R-квадрат, он равен
единичка минус RSS, делённая на TSS.
В R-квадрат adjusted,
в скорректированном R-квадрате,
из единички вычитается RSS и TSS, делённые
соответственно на (n – k) и на (n – 1).
Соответственно, с ростом k R-квадрат
adjusted при прочих равных будет падать,
то есть мы «оштрафовали» R-квадрат
на количество регрессоров k,
вернее, на количество
оцениваемых коэффициентов k.
И, соответственно, вот по R-квадрат
adjusted уже грубо можно сравнивать
ограниченную и неограниченную модели.
Посмотрел на R-квадрат adjusted — если
один сильно-сильно-сильно больше,
чем другой, то, соответственно,
скорее всего, мы выберем ту модель,
где R-квадрат, скорректированный
R-квадрат adjusted, выше.
Можно отметить,
что сравнение R-квадратов adjusted — оно
равносильно сравнению σ-квадрат с крышкой.
Если в двух моделях,
ограниченной и неограниченной,
в одной R-квадрат adjusted больше,
то в ней σ-квадрат с крышкой будет меньше.
Следующие два показателя качества,
критерий Акаике и критерий Шварца,
они реализуют ту же самую мысль,
что модель плохая,
если а) она плохо предсказывает
и если б) она слишком сложная.
То есть если две модели одинаково хорошо
предсказывают, то лучше та, которая проще.
Если две модели одинаково сложные,
в них одинаковое количество
объясняющих переменных, то хуже та,
которая хуже предсказывает.
Соответственно, штраф,
некая плохая характеристика модели,
то есть штраф за модель,
— это штраф за высокую RSS,
за высокую сумму квадратов остатков,
и за высокое количество
объясняющих переменных.
В штрафном критерии Акаике это,
соответственно,
разбивается сумма штрафа на штраф за
высокий RSS, то есть это n на логарифм
RSS, деленного на n, плюс штраф за
большое количество регрессоров, 2k.
И в критерии Шварца соответствующий
штраф за количество регрессоров
— это логарифм n, помноженный на k, на
количество регрессоров: чем больше k при
прочих равных, тем выше штраф
Акаике и тем выше штраф Шварца.
Соответственно, руководствуясь
этими двумя критериями,
мы выбираем ту модель, в которой
штрафной критерий AIC или BIC меньше.
Ну, соответственно, у этих критериев
есть достаточно серьёзное математическое
обоснование за ним: тут неспроста
взят логарифм RSS, делённое на n,
неспроста взят логарифм n, помноженный на
k, но оно выходит за рамки данного курса.
Здесь надо просто представлять себе,
что есть два штрафных критерия,
которые реализуют попытку исследователей
сделать какой-то простой критерий,
по которому глянул на
модель и сразу понял,
как-то хоть пусть упрощённо,
но зато быстро, — какая модель лучше.
В данном случае лучше та,
где штрафной критерий меньше.
В сегодняшней лекции мы, соответственно,
поговорили о нескольких сюжетах.
Мы разобрали, как проводить
прогнозирование, как строить доверительные
предиктивные интервалы при
прогнозировании, мы рассмотрели гипотезу,
как проверять наличие нескольких линейных
ограничений, провели тест Рамсея,
который позволяет нам судить о том,
а не забыли ли мы какие-нибудь переменные.
А в следующих лекциях мы
поговорим о разных неприятностях,
которые ждут исследователя,
который занимается оценкой регрессии.
Именно мы поговорим о
гетероскедастичности,
мультиколлинеарности, авторегрессии.

