1
00:00:13,250 --> 00:00:17,917
В сегодняшней четвертой лекции
мы расскажем о двух связанных

2
00:00:17,917 --> 00:00:22,359
явлениях — мультиколлинеарности
и методе главных компонент.

3
00:00:22,359 --> 00:00:23,968
Начнем с мультиколлинеарности.

4
00:00:23,968 --> 00:00:28,164
Мультиколлинеарность — это явление,
состоящее в том, что среди регрессоров,

5
00:00:28,164 --> 00:00:31,935
среди объясняющих переменных
существует линейная зависимость.

6
00:00:31,935 --> 00:00:35,562
Эта зависимость может быть идеальной,
тогда мы имеем дело со

7
00:00:35,562 --> 00:00:40,510
строгой мультиколлинеарностью, или
примерной зависимостью, то есть когда 1х,

8
00:00:40,510 --> 00:00:44,860
один регрессор, плюс другой, например,
примерно равняется третьему.

9
00:00:44,860 --> 00:00:48,570
Эта ситуация называется
нестрогой мультиколлинеарностью.

10
00:00:48,570 --> 00:00:51,988
Давайте рассмотрим пример
строгой мультиколлинеарности.

11
00:00:51,988 --> 00:00:55,725
Вот здесь, в этой матрице,
если присмотреться, то можно заметить,

12
00:00:55,725 --> 00:01:00,311
что если сложить второй столбец матрицы,
второй регрессор и третий столбец матрицы,

13
00:01:00,311 --> 00:01:06,394
третий регрессор, то получится значение
четвертого столбца, помноженное на 2.

14
00:01:06,394 --> 00:01:09,716
Это явление называется строгой
мультиколлинеарностью.

15
00:01:09,716 --> 00:01:13,260
Когда строгая мультиколлинеарность
может случиться в реальности?

16
00:01:13,260 --> 00:01:17,877
Ну, скорей всего,
это просто ошибка исследователя, например,

17
00:01:17,877 --> 00:01:21,040
неправильное введение дамми-переменных.

18
00:01:21,040 --> 00:01:25,554
Давайте рассмотрим ошибочно составленную
модель с использованием дамми-переменных,

19
00:01:25,554 --> 00:01:28,260
то есть переменных,
принимающих значение 0 или 1.

20
00:01:28,260 --> 00:01:31,631
Исследователь попытался оценить
зависимость зарплаты от

21
00:01:31,631 --> 00:01:34,258
количества лет обучения
и пола респондента.

22
00:01:34,258 --> 00:01:38,191
И для этого он включил переменную male,
которая равна единичке для мужчин,

23
00:01:38,191 --> 00:01:41,017
переменную female,
которая равна единичке для женщин.

24
00:01:41,017 --> 00:01:45,116
Ну, соответственно,
свободный член β1 и также

25
00:01:45,116 --> 00:01:49,647
количество лет обучения
— переменную education.

26
00:01:49,647 --> 00:01:54,240
Соответственно, в этой ситуации,
если мы составим матрицу всех регрессоров,

27
00:01:54,240 --> 00:01:57,360
то первый столбец матрицы —
это будет столбец из единичек.

28
00:01:57,360 --> 00:02:01,343
Второй столбец — это будет столбец, где
единички будут соответствовать мужчинам.

29
00:02:01,343 --> 00:02:05,340
Третий столбец — это будет столбец, где
единички будут соответствовать женщинам.

30
00:02:05,340 --> 00:02:09,660
И, соответственно, четвертый столбец будет
отвечать за количество лет обучения.

31
00:02:09,660 --> 00:02:13,476
Здесь мы видим, что если сложить
второй и третий столбец матрицы,

32
00:02:13,476 --> 00:02:16,480
то получится в точности
первый столбец матрицы.

33
00:02:16,480 --> 00:02:19,659
И мы имеем дело со строгой
мультиколлинеарностью.

34
00:02:19,659 --> 00:02:22,990
Каковы последствия строгой
мультиколлинеарности?

35
00:02:22,990 --> 00:02:27,443
Проблема состоит в том, что оценки
метода наименьших квадратов являются не

36
00:02:27,443 --> 00:02:29,440
единственными в такой ситуации.

37
00:02:29,440 --> 00:02:32,234
Действительно, давайте на
простом примере увидим,

38
00:02:32,234 --> 00:02:35,890
что единственных оценок метода
наименьших квадратов не существует.

39
00:02:35,890 --> 00:02:40,180
Вот посмотрим на три указанных
оцененных уравнения.

40
00:02:40,180 --> 00:02:45,811
Ну, например, уравнение,
где прогнозируемая заработная плата равна

41
00:02:45,811 --> 00:02:52,190
28 минус 10 умножить на переменную male,
минус 15 умножить на переменную female.

42
00:02:52,190 --> 00:02:56,137
Например, если я подставлю
переменную male, равную 1 и female,

43
00:02:56,137 --> 00:02:58,929
равную 0 для мужчины, то у меня получится,

44
00:02:58,929 --> 00:03:03,840
что зарплата равна 18 плюс 3
умножить на количество лет обучения.

45
00:03:03,840 --> 00:03:07,960
Ежели я рассмотрю модель, которая,
казалось бы — совершенно другие

46
00:03:07,960 --> 00:03:12,568
коэффициенты, прогнозируемая
заработная плата равна 18 плюс 0

47
00:03:12,568 --> 00:03:17,008
умножить на переменную male минус
5 умножить на переменную female,

48
00:03:17,008 --> 00:03:21,804
то опять же для мужчин male = 1,
female = 0, прогнозное значение

49
00:03:21,804 --> 00:03:26,742
заработной платы будет равно 18 плюс 3
умножить на количество лет обучения.

50
00:03:26,742 --> 00:03:30,767
То есть модели с принципиально разными
коэффициентами дают абсолютно одинаковые

51
00:03:30,767 --> 00:03:37,175
прогнозы, и поэтому никаким методом
выбрать модель из указанных не получится.

52
00:03:37,175 --> 00:03:42,318
Соответственно, поскольку строгая
мультиколлинеарность, как правило,

53
00:03:42,318 --> 00:03:47,122
является последствием какой-то ошибки
в введении объясняющих переменных,

54
00:03:47,122 --> 00:03:51,797
то нужно просто правильно
ввести объясняющие переменные.

55
00:03:51,797 --> 00:03:58,913
Например, ввести либо переменную, которая
равна единичке, когда респондент мужчина,

56
00:03:58,913 --> 00:04:03,023
то есть либо оставить переменную male и
убрать переменную female, либо, наоборот,

57
00:04:03,023 --> 00:04:05,420
убрать переменную male и
оставить переменную female,

58
00:04:05,420 --> 00:04:07,927
которая для женщин равна 1,
а для мужчин — 0.

59
00:04:07,927 --> 00:04:11,576
В такой ситуации строгой
мультиколлинеарности не возникнет,

60
00:04:11,576 --> 00:04:16,625
и оценки метода наименьших квадратов
будут существовать и будут единственны.

61
00:04:16,625 --> 00:04:20,721
Вторая ситуация гораздо более
распространенная — это нестрогая

62
00:04:20,721 --> 00:04:21,971
мультиколлинеарность.

63
00:04:21,971 --> 00:04:25,330
Когда между объясняющими переменными
есть примерная зависимость.

64
00:04:25,330 --> 00:04:27,761
Ну, это возникает в разных ситуациях.

65
00:04:27,761 --> 00:04:30,693
Как правило,
могут существовать регрессоры,

66
00:04:30,693 --> 00:04:32,779
которые меряют примерно одно и то же.

67
00:04:32,779 --> 00:04:35,804
Например, валютный курс,
померянный на начало торгов,

68
00:04:35,804 --> 00:04:37,930
начало дня и на момент окончания торгов.

69
00:04:37,930 --> 00:04:40,183
Естественно, они примерно равны.

70
00:04:40,183 --> 00:04:43,491
Соответственно, имеется
примерная зависимость,

71
00:04:43,491 --> 00:04:47,100
что курс на начало дня примерно
равен курсу на конец дня.

72
00:04:47,100 --> 00:04:51,375
И второй фактор — это наличие естественных
взаимосвязей между переменными.

73
00:04:51,375 --> 00:04:55,298
Например, если рассмотреть
такие регрессоры потенциальные,

74
00:04:55,298 --> 00:04:58,860
как стаж работы,
количество лет обучения и возраст,

75
00:04:58,860 --> 00:05:02,945
то между ними существует
примерное соотношение.

76
00:05:02,945 --> 00:05:07,482
Возраст примерно равен количеству
лет обучения, плюс стаж работы.

77
00:05:07,482 --> 00:05:09,890
Понятно, что есть люди не работавшие.

78
00:05:09,890 --> 00:05:15,080
Понятно, что есть кто-то, кто болел или
просто решил не выходить на работу,

79
00:05:15,080 --> 00:05:20,270
но в целом для большинства людей, которые
обучались, а потом вышли на работу,

80
00:05:20,270 --> 00:05:25,430
соответственно, возраст будет равен
сумме двух оставшихся переменных.

81
00:05:25,430 --> 00:05:29,750
Каковы же последствия нестрогой
мультиколлинеарности?

82
00:05:29,750 --> 00:05:32,715
Основным моментом,
который следует отметить,

83
00:05:32,715 --> 00:05:37,380
что нестрогая мультиколлинеарность
не нарушает стандартных предпосылок,

84
00:05:37,380 --> 00:05:41,328
в частности предпосылок теоремы
Гаусса-Маркова, которую мы обсуждали.

85
00:05:41,328 --> 00:05:45,064
То есть оценки, которые мы получаем
методом наименьших квадратов,

86
00:05:45,064 --> 00:05:46,020
они существуют.

87
00:05:46,020 --> 00:05:48,000
Эти β с крышкой — они несмещенные,

88
00:05:48,000 --> 00:05:51,102
у них наименьшая дисперсия
среди несмещенных оценок.

89
00:05:51,102 --> 00:05:54,560
Мы можем проверять гипотезы
стандартным способом.

90
00:05:54,560 --> 00:05:58,432
И в этом смысле мультиколлинеарность
гораздо меньшее зло,

91
00:05:58,432 --> 00:06:01,193
чем, скажем,
гетероскедастичность или автокорреляция,

92
00:06:01,193 --> 00:06:03,430
о которых мы будем говорить
в следующих лекциях.

93
00:06:03,430 --> 00:06:09,121
Но тем не менее, мультиколлинеарность
имеет свои последствия.

94
00:06:09,121 --> 00:06:13,539
Мы уже писали,
что стандартную ошибку коэффициента,

95
00:06:13,539 --> 00:06:18,214
можно определить как отношения
σ-квадрат с крышкой к

96
00:06:18,214 --> 00:06:22,590
сумме квадратов остатков
в регрессии j-того,

97
00:06:22,590 --> 00:06:26,881
j-той объясняющей переменной на
остальные объясняющие переменные.

98
00:06:26,881 --> 00:06:31,630
Соответственно, что произойдет,
если существует какая-то примерная

99
00:06:31,630 --> 00:06:35,550
линейная зависимость между
объясняющими переменными?

100
00:06:35,550 --> 00:06:40,509
И, допустим,
что j-тая объясняющая переменная входит

101
00:06:40,509 --> 00:06:44,287
в эту формулу зависимости
между регрессорами.

102
00:06:44,287 --> 00:06:49,058
Соответственно, в этом случае наш
регрессор будет замечательно объясняться

103
00:06:49,058 --> 00:06:49,680
другими.

104
00:06:49,680 --> 00:06:54,198
Ну, скажем, возраст будет
замечательно объясняться стажем

105
00:06:54,198 --> 00:06:57,157
работы и количеством лет обучения.

106
00:06:57,157 --> 00:07:00,589
И, соответственно, RSS в j-той регрессии,

107
00:07:00,589 --> 00:07:03,840
сумма квадратов остатков,
будет очень маленькой.

108
00:07:03,840 --> 00:07:04,960
И, стало быть,

109
00:07:04,960 --> 00:07:10,090
стандартная ошибка коэффициента β с
крышкой j-того будет очень большой.

110
00:07:10,090 --> 00:07:15,099
Еще используют такое простое разложение
для стандартной ошибки оценки

111
00:07:15,099 --> 00:07:20,198
j-того коэффициента,
что это 1 делить 1 минус R-квадрат

112
00:07:20,198 --> 00:07:25,507
в j-той регрессии, то есть в регрессии
j-той объясняющей переменной,

113
00:07:25,507 --> 00:07:31,590
на все остальные помножить, на σ-квадрат
с крышкой в основной регрессии,

114
00:07:31,590 --> 00:07:36,450
делить на общую сумму
квадратов j-того регрессора.

115
00:07:36,450 --> 00:07:40,962
Соответственно, основным
последствием мультиколлинеарности

116
00:07:40,962 --> 00:07:46,104
нестрогой являются высокие стандартные
ошибки коэффициентов β с крышкой.

117
00:07:46,104 --> 00:07:51,895
В свою очередь, высокие стандартные ошибки
коэффициентов β с крышкой приводят к тому,

118
00:07:51,895 --> 00:07:58,350
что доверительные интервалы для настоящих
неизвестных коэффициентов становятся шире.

119
00:07:58,350 --> 00:08:03,188
Коэффициенты сами получаются незначимыми,
то есть гипотеза о том,

120
00:08:03,188 --> 00:08:06,476
что истинный коэффициент равен 0,
не отвергается.

121
00:08:06,476 --> 00:08:09,908
Потому что доверительный интервал будет
широким, будет включать в себя 0, и,

122
00:08:09,908 --> 00:08:13,110
соответственно, гипотеза о том,
что неизвестный коэффициент равен 0,

123
00:08:13,110 --> 00:08:14,748
не будет отвергнута.

124
00:08:14,748 --> 00:08:19,235
И модель получается в каком-то
смысле очень неустойчивой.

125
00:08:19,235 --> 00:08:24,421
В условиях достаточно ярко выраженной
нестрогой мультиколлинеарности может

126
00:08:24,421 --> 00:08:29,471
сложиться такое, что при добавлении всего
лишь одного наблюдения или при выкидывании

127
00:08:29,471 --> 00:08:34,354
всего лишь одного наблюдения, оценки
коэффициентов резко меняются из-за того,

128
00:08:34,354 --> 00:08:36,674
что у них большие стандартные ошибки.

129
00:08:36,674 --> 00:08:38,464
С точки зрения исследователя,

130
00:08:38,464 --> 00:08:42,659
типичным проявлением нестрогой
мультиколлинеарности является следующее.

131
00:08:42,659 --> 00:08:46,520
Исследователь видит несколько
групп незначимых коэффициентов.

132
00:08:46,520 --> 00:08:51,140
То есть второй, скажем, регрессор
незначим, третий регрессор незначим.

133
00:08:51,140 --> 00:08:54,799
Он их выкидывает оба,
а модель при этом резко ухудшается.

134
00:08:54,799 --> 00:08:58,186
Вот это типичное проявление
мультиколлинеарности на практике.

135
00:08:58,186 --> 00:09:00,767
Когда мы видим,
что по отдельности коэффициент незначим,

136
00:09:00,767 --> 00:09:05,730
несколько коэффициентов, а в то же время
все их выкинуть было бы не верным,

137
00:09:05,730 --> 00:09:09,950
потому что от кого-то из
них зависимость явно есть.

138
00:09:09,950 --> 00:09:13,780
Для обнаружения мультиколлинеарности
используют следующие

139
00:09:13,780 --> 00:09:16,970
количественные признаки.

140
00:09:16,970 --> 00:09:21,707
Прежде всего, наиболее распространенным
является показатель вздутия дисперсии или

141
00:09:21,707 --> 00:09:23,570
коэффициент вздутия дисперсии,

142
00:09:23,570 --> 00:09:27,806
который считается отдельно для
каждой объясняющей переменной.

143
00:09:27,806 --> 00:09:32,361
То есть для каждой входящей в модель
объясняющей переменной у нас будет свой

144
00:09:32,361 --> 00:09:34,320
показатель вздутия дисперсии.

145
00:09:34,320 --> 00:09:39,320
Соответственно, он определяется
как 1 делить на 1 минус

146
00:09:39,320 --> 00:09:43,920
R-квадрат в j-той регрессии,
то есть в регрессии этой объясняющей

147
00:09:43,920 --> 00:09:47,160
переменной на остальные
объясняющие переменные.

148
00:09:47,160 --> 00:09:50,296
Почему этот показатель так называется?

149
00:09:50,296 --> 00:09:55,571
Название происходит в связи с
представлением стандартной ошибки

150
00:09:55,571 --> 00:10:00,390
j-того коэффициента в виде
показателя вздутия дисперсии,

151
00:10:00,390 --> 00:10:03,286
домноженного на σ-квадрат с крышкой,

152
00:10:03,286 --> 00:10:08,268
деленное на сумму квадратов,
общую сумму квадратов j-того регрессора.

153
00:10:08,268 --> 00:10:12,338
Соответственно, при большом
коэффициенте вздутия дисперсии,

154
00:10:12,338 --> 00:10:16,329
то есть при сильной зависимости
этого регрессора от остальных,

155
00:10:16,329 --> 00:10:20,110
стандартная ошибка коэффициента
будет существенно выше.

156
00:10:20,110 --> 00:10:23,092
И также часто используют
такой простой показатель,

157
00:10:23,092 --> 00:10:24,785
просто как выборочные корреляции.

158
00:10:24,785 --> 00:10:28,455
Смотрят выборочные корреляции
между отдельными регрессорами,

159
00:10:28,455 --> 00:10:34,290
и если они слишком велики, то это тоже
является показателем мультиколлинеарности.

160
00:10:34,290 --> 00:10:40,648
Тут нет никакой строгой границы,
но тем не менее в ряде источников приводят

161
00:10:40,648 --> 00:10:46,060
границы для показателя вздутия дисперсии
— 10 и для корреляции — около 0,9.

162
00:10:46,060 --> 00:10:50,835
То есть такие показатели — значения
коэффициентов вздутия дисперсии

163
00:10:50,835 --> 00:10:55,935
больше 10 или корреляции между
регрессорами больше 0,9 — может говорить

164
00:10:55,935 --> 00:11:00,525
о потенциальном наличии, о потенциальной
проблеме мультиколлинеарности.

165
00:11:00,525 --> 00:11:04,500
Далее мы рассмотрим, что можно сделать
с нестрогой мультиколлинеарностью.

