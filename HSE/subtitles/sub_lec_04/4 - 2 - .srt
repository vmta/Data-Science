1
00:00:13,250 --> 00:00:18,545
Что же можно сделать с
мультиколлинеарностью?

2
00:00:18,545 --> 00:00:23,269
Во-первых, нужно осознавать, что, может
быть, эта мультиколлинеарность не так уж и

3
00:00:23,269 --> 00:00:28,032
страшна, потому что теорема Гаусса-Маркова
по-прежнему в силе, соответственно,

4
00:00:28,032 --> 00:00:31,220
оценки, которые мы получаем,
— они несмещённые,

5
00:00:31,220 --> 00:00:35,920
и они обладают наименьшей дисперсией
среди несмещённых оценок.

6
00:00:35,920 --> 00:00:41,520
И поэтому если вас интересуют прогнозы и
вам неважна интерпретация коэффициентов,

7
00:00:41,520 --> 00:00:44,880
то на мультиколлинеарность можно
вообще не обращать внимания.

8
00:00:44,880 --> 00:00:49,755
Более того, если вот вам нужно оценить
именно коэффициенты при именно ваших

9
00:00:49,755 --> 00:00:54,544
переменных, то, к сожалению, ничего
лучше в условиях мультиколлинеарности

10
00:00:54,544 --> 00:00:58,317
придумать нельзя, потому что полученные
оценки являются несмещёнными,

11
00:00:58,317 --> 00:01:00,800
с наименьшей дисперсией
среди несмещённых оценок.

12
00:01:00,800 --> 00:01:06,624
Однако возможно вы не совсем уверены
в форме спецификации модели,

13
00:01:06,624 --> 00:01:08,827
вы можете не быть уверены в том,

14
00:01:08,827 --> 00:01:12,839
что ваш показатель именно зависит и от
валютного курса на момент начала торгов,

15
00:01:12,839 --> 00:01:16,664
и от валютного курса на момент окончания
торгов, вы можете просто предполагать,

16
00:01:16,664 --> 00:01:19,740
что он зависит от какого-то валютного
курса, но вы не знаете, от какого.

17
00:01:19,740 --> 00:01:23,216
Тогда вы можете пожертвовать
одной из переменных,

18
00:01:23,216 --> 00:01:27,250
которые участвуют в линейном соотношении,
и выкинуть её.

19
00:01:27,250 --> 00:01:32,197
И есть, конечно, ещё идеальный вариант,
который, как правило, недоступен,

20
00:01:32,197 --> 00:01:37,701
поэтому это вариант-мечта — заполучить
побольше наблюдений для оценивания модели.

21
00:01:37,701 --> 00:01:40,840
Это тоже метод борьбы с
мультиколлинеарностью.

22
00:01:40,840 --> 00:01:47,154
Итак, первый метод — это
ничего не делать и получить

23
00:01:47,154 --> 00:01:52,350
и так несмещённые оценки с
наименьшей возможной дисперсией.

24
00:01:52,350 --> 00:01:57,149
Второй метод — это пожертвовать
несмещённостью оценок с целью уменьшить их

25
00:01:57,149 --> 00:01:57,970
дисперсию.

26
00:01:57,970 --> 00:01:58,758
И третий.

27
00:01:58,758 --> 00:02:03,150
третья возможность — это
получить побольше наблюдений.

28
00:02:03,150 --> 00:02:09,266
Как можно пожертвовать несмещённостью
оценок и снизить дисперсию?

29
00:02:09,266 --> 00:02:11,073
Есть два основных пути.

30
00:02:11,073 --> 00:02:16,982
Первый путь, как мы уже сказали,
— это выкинуть несколько переменных,

31
00:02:16,982 --> 00:02:20,160
избавившись от нестрогого
линейного соотношения.

32
00:02:20,160 --> 00:02:25,985
И второй путь — это ввести штраф
в метод наименьших квадратов,

33
00:02:25,985 --> 00:02:29,422
то есть рассмотреть метод
наименьших квадратов со штрафом.

34
00:02:29,422 --> 00:02:35,680
Давайте рассмотрим простое упражнение,
где мы, зная коэффициенты R-квадрат

35
00:02:35,680 --> 00:02:40,956
во всех регрессиях всех объясняющих
переменных на остальные, попробуем понять,

36
00:02:40,956 --> 00:02:45,422
имеет ли место у нас мультиколлинеарность,
и между какими переменными есть линейная

37
00:02:45,422 --> 00:02:49,895
взаимосвязь, какие переменные имеет смысл,
возможно, выкинуть из рассмотрения.

38
00:02:49,895 --> 00:02:53,939
Давайте на примере посмотрим, как
коэффициенты вздутия дисперсии позволяют

39
00:02:53,939 --> 00:02:58,442
определить наличие мультиколлинеарности
в модели линейной регрессии и выяснить,

40
00:02:58,442 --> 00:03:00,900
между какими переменными есть зависимость.

41
00:03:00,900 --> 00:03:04,699
Давайте рассмотрим следующий пример.

42
00:03:04,699 --> 00:03:12,905
Исследователь оценил модель зависимости
y от нескольких объясняющих переменных,

43
00:03:12,905 --> 00:03:18,236
y_i = β₁ + β₂ *

44
00:03:18,236 --> 00:03:23,635
x_i + β₃ *

45
00:03:23,635 --> 00:03:28,943
z_i + β_4 * w_i

46
00:03:28,943 --> 00:03:34,115
+ случайная составляющая ε_i.

47
00:03:34,115 --> 00:03:39,026
И между регрессорами,
между объясняющими переменными x, z и w,

48
00:03:39,026 --> 00:03:42,840
потенциально ожидается
мультиколлинеарность.

49
00:03:42,840 --> 00:03:47,230
И известно,
что при построении регрессора x на

50
00:03:47,230 --> 00:03:55,700
остальные z и w R-квадрат второй во
вспомогательной регрессии равен 0,5.

51
00:03:55,700 --> 00:04:03,841
То есть это имеется в виду R-квадрат в
регрессии x на остальные регрессоры,

52
00:04:03,841 --> 00:04:09,105
то есть в регрессии (x_i = γ₁ + γ₂ *

53
00:04:09,105 --> 00:04:16,210
zi + γ₃ * w_i + какая-то
своя случайная ошибка.

54
00:04:16,210 --> 00:04:22,853
R-квадрат во вспомогательной
регрессии z на остальные

55
00:04:22,853 --> 00:04:27,760
регрессоры равен 0,95.

56
00:04:27,760 --> 00:04:32,866
То есть имеется ввиду,
что это R-квадрат в регрессии z_i

57
00:04:32,866 --> 00:04:40,245
= γ₁ + γ₂ * x_i + γ₃ * w_i + u_i.

58
00:04:40,245 --> 00:04:45,429
Здесь я сознательно,
чтобы не вводить новые буквы,

59
00:04:45,429 --> 00:04:50,310
пишу одинаковые гаммы, хотя,
конечно, результаты оценивания

60
00:04:50,310 --> 00:04:54,344
регрессии x на остальные объясняющие
переменные и регрессии z на остальные

61
00:04:54,344 --> 00:04:58,716
объясняющие переменные, конечно же,
отличаются, но поскольку мне они неважны,

62
00:04:58,716 --> 00:05:03,378
то поэтому я для экономии букв буду
использовать одинаковые буквы,

63
00:05:03,378 --> 00:05:08,390
хотя, конечно,
эта γ₁ и эта γ₁ — это разные константы.

64
00:05:08,390 --> 00:05:16,341
И при регрессии третьего регрессора
R-квадрат четыре оказался равен 0,98.

65
00:05:16,341 --> 00:05:21,621
То есть это регрессия объясняющей
переменной w_i на остальные

66
00:05:21,621 --> 00:05:26,730
объясняющие переменные, w_i = γ₁ + γ₂ * x_i

67
00:05:26,730 --> 00:05:32,950
+ γ₃ * z_i + u_i.

68
00:05:32,950 --> 00:05:37,030
Соответственно, мне нужно выяснить,

69
00:05:37,030 --> 00:05:45,030
есть ли мультиколлинеарность
в этой модели,

70
00:05:51,660 --> 00:05:58,290
и между какими переменными
есть линейная связь.

71
00:06:14,520 --> 00:06:22,753
Для ответа на эти вопросы мы воспользуемся
коэффициентом вздутия дисперсии.

72
00:06:22,753 --> 00:06:27,829
Я напомню, что коэффициент вздутия
дисперсии, он считается отдельно

73
00:06:27,829 --> 00:06:32,350
для каждого регрессора,
для каждой объясняющей переменной.

74
00:06:32,350 --> 00:06:36,456
Соответственно, коэффициент вздутия
дисперсии считается как единичка делить на

75
00:06:36,456 --> 00:06:40,726
один минус R² в соответствующей
вспомогательной регрессии.

76
00:06:40,726 --> 00:06:42,465
Мы легко их рассчитываем.

77
00:06:42,465 --> 00:06:47,370
Коэффициент вздутия дисперсии для

78
00:06:47,370 --> 00:06:52,309
объясняющей переменной x равен 1/ (1-

79
00:06:52,309 --> 00:06:57,660
0,5) = 2.

80
00:06:57,660 --> 00:07:02,741
Коэффициент вздутия дисперсии
для переменной z равен

81
00:07:02,741 --> 00:07:07,486
1/ (1- 0,95), = 20.

82
00:07:07,486 --> 00:07:13,724
И коэффициент вздутия
дисперсии для переменной

83
00:07:13,724 --> 00:07:18,570
w равен 1/ (1- 0,98),

84
00:07:18,570 --> 00:07:22,630
= 50.

85
00:07:22,630 --> 00:07:25,114
Ещё раз хочу подчеркнуть,

86
00:07:25,114 --> 00:07:30,094
что строгой границы для
показателя вздутия дисперсии нет,

87
00:07:30,094 --> 00:07:34,756
но, считается, что где-то значения
больше десяти уже являются

88
00:07:34,756 --> 00:07:39,240
мягким нестрогим признаком
наличия мультиколлинеарности.

89
00:07:39,240 --> 00:07:43,744
Соответственно, мы видим в данном случае,
что переменная x

90
00:07:43,744 --> 00:07:48,543
довольно плохо объясняется остальными
переменными, то есть переменная x

91
00:07:48,543 --> 00:07:52,420
— её можно считать линейно
независимой от остальных переменных.

92
00:07:52,420 --> 00:07:57,054
А вот переменные z и w
достаточно сильно зависят от

93
00:07:57,054 --> 00:08:02,279
других регрессоров, то есть у них
высокий R², соответственно,

94
00:08:02,279 --> 00:08:06,260
высокий коэффициент вздутия дисперсии,
существенно больше 10-ти.

95
00:08:06,260 --> 00:08:08,482
Соответственно, в данном случае мы видим,

96
00:08:08,482 --> 00:08:10,937
что в модели имеет место
мультиколлинеарность.

97
00:08:10,937 --> 00:08:15,275
Эта мультиколлинеарность вызвана тем,
что z хорошо объясняется другими

98
00:08:15,275 --> 00:08:19,340
регрессорами и w хорошо
объясняется другими регрессорами.

99
00:08:19,340 --> 00:08:23,973
Ну и давайте попробуем ответить теперь
на второй вопрос: какие же переменные,

100
00:08:23,973 --> 00:08:26,336
какие регрессоры зависимы между собой?

101
00:08:26,336 --> 00:08:28,015
Ну если их, условно говоря,

102
00:08:28,015 --> 00:08:33,159
нарисовать вот так вот точечками:
вот это x, это z, это w.

103
00:08:33,159 --> 00:08:36,330
Вот мы видим,
что x плохо зависит от z и w.

104
00:08:36,330 --> 00:08:40,890
Плохо в том смысле, что зависимость
далека от линейной: R² маленький,

105
00:08:40,890 --> 00:08:44,807
коэффициент вздутия дисперсии маленький.

106
00:08:44,807 --> 00:08:47,679
То есть x линейно не объясняется с z и w.

107
00:08:47,679 --> 00:08:50,170
Однако z линейно объясняется x и w.

108
00:08:50,170 --> 00:08:53,349
Но z не может объясняться линейно x,

109
00:08:53,349 --> 00:08:58,054
поэтому получается,
что связь линейная есть между z и w.

110
00:08:58,054 --> 00:09:02,684
Мы так можем интерпретировать то,
что коэффициент вздутия дисперсии

111
00:09:02,684 --> 00:09:06,293
при z большой, коэффициент
вздутия дисперсии при w большой,

112
00:09:06,293 --> 00:09:09,650
однако x от них, от z и w, не зависит.

113
00:09:09,650 --> 00:09:14,055
Соответственно, по имеющимся
вспомогательным регрессиям мы можем

114
00:09:14,055 --> 00:09:17,059
сделать вывод, что да,
мультиколлинеарность, скорее всего,

115
00:09:17,059 --> 00:09:21,600
имеет место,
а связаны между собой переменные z и w.

