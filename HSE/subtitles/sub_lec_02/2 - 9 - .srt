1
00:00:13,270 --> 00:00:19,600
Константа σ² неизвестна,
и именно эта

2
00:00:19,600 --> 00:00:24,570
константа входит в формулу для условной
дисперсии любого оценённого коэффициента.

3
00:00:24,570 --> 00:00:27,747
А нам хочется строить доверительные
интервалы для коэффициентов,

4
00:00:27,747 --> 00:00:28,770
проверять гипотезы,

5
00:00:28,770 --> 00:00:32,500
поэтому нам нужен какой-то способ
оценить неизвестную константу σ².

6
00:00:32,500 --> 00:00:36,557
И, соответственно,
такой способ существует.

7
00:00:36,557 --> 00:00:41,429
В качестве оценки σ² с σ²
крышкой для неизвестной константы

8
00:00:41,429 --> 00:00:46,411
можно взять RSS, сумму квадратов
остатков регрессии, делённую на (n – k),

9
00:00:46,411 --> 00:00:49,860
где n — это количество наблюдений,
а k — это количество коэффициентов.

10
00:00:49,860 --> 00:00:53,245
Сейчас мы выпишем список хороших свойств,
где увидим,

11
00:00:53,245 --> 00:00:56,954
что эта оценка обладает
хорошими свойствами.

12
00:00:56,954 --> 00:01:01,765
Имея оценку σ² с
крышкой - оценку дисперсии ε_i

13
00:01:01,765 --> 00:01:06,571
при фиксированных регрессорах,
можно, соответственно,

14
00:01:06,571 --> 00:01:11,370
получить оценку дисперсии
любого коэффициента оценённого.

15
00:01:11,370 --> 00:01:17,088
Так, например, поскольку настоящая
дисперсия оценки β_j с крышкой — это есть

16
00:01:17,088 --> 00:01:23,179
σ² умножить на какую-то функцию от
регрессоров, от объясняющих переменных,

17
00:01:23,179 --> 00:01:28,650
то, соответственно,
в качестве оценки дисперсии β_j с крышкой

18
00:01:28,650 --> 00:01:33,460
мы возьмём σ² с крышкой,
получаемую по формуле RSS делить на (n – k),

19
00:01:33,460 --> 00:01:37,130
помножить на ту же самую
функцию от регрессоров.

20
00:01:37,130 --> 00:01:42,533
То есть конкретно мы получаем в
силу сформулированной нами теоремы,

21
00:01:42,533 --> 00:01:47,570
что оценка дисперсии β_j с крышкой при
фиксированных иксах — это σ²

22
00:01:47,570 --> 00:01:52,666
с крышкой, делённая на RSS_j,
где RSS_j — это RSS в

23
00:01:52,666 --> 00:01:58,550
регрессии j-ой объясняющей переменной
на остальные объясняющие переменные.

24
00:01:58,550 --> 00:02:03,124
Для удобства, чтобы строить доверительные
интервалы и проверять гипотезы,

25
00:02:03,124 --> 00:02:05,660
используется корень из оценки дисперсии.

26
00:02:05,660 --> 00:02:10,547
Чтобы это долго не писать
«корень из оценки дисперсии» и там ещё

27
00:02:10,547 --> 00:02:15,744
что-то в скобочках и условие на x ставить,
вводят более простое обозначение,

28
00:02:15,744 --> 00:02:18,900
а именно: вводят понятие
«стандартной ошибки».

29
00:02:18,900 --> 00:02:22,466
Стандартная ошибка коэффициента
β_j с крышкой — это и есть корень

30
00:02:22,466 --> 00:02:23,583
из оценки дисперсии.

31
00:02:23,583 --> 00:02:26,640
И чтобы вот долго не писать
«корень из оценки дисперсии»,

32
00:02:26,640 --> 00:02:30,921
пишут «стандартная ошибка»,
обозначают стандарт se (от английского

33
00:02:30,921 --> 00:02:33,910
«standard error») и в
скобках β_j с крышкой.

34
00:02:33,910 --> 00:02:38,324
Например, в модели парной регрессии,

35
00:02:38,324 --> 00:02:43,220
где y_i = β₁ + β₂ x_i + ε_i
стандартная ошибка β₂ с крышкой

36
00:02:43,220 --> 00:02:47,989
— это корень из: в числителе
σ² с крышкой, получаемое по

37
00:02:47,989 --> 00:02:52,242
формуле RSS делить на (n – k), делить
на сумму xi – x среднее, в квадрате.

38
00:02:52,242 --> 00:02:56,303
И, соответственно, если мы владеем
средствами линейной алгебры,

39
00:02:56,303 --> 00:03:01,128
то мы можем записать компактную формулу
не только для оценки дисперсии отдельного

40
00:03:01,128 --> 00:03:05,702
коэффициента, но и для оценки всей
ковариационной матрицы коэффициентов,

41
00:03:05,702 --> 00:03:09,915
то есть для оценки дисперсий каждого
коэффициента и всех возможных ковариаций

42
00:03:09,915 --> 00:03:12,921
условных каждого коэффициента
с каждым коэффициентом.

43
00:03:12,921 --> 00:03:17,446
И, конечно, по аналогии, оценка такой
ковариационной матрицы имеет вид:

44
00:03:17,446 --> 00:03:22,360
σ² с крышкой умножить
на X'X в минус первой.

45
00:03:22,360 --> 00:03:24,922
Эту матрицу,
в отличие от настоящей дисперсии,

46
00:03:24,922 --> 00:03:28,340
которая неизвестна,
которую мы никогда не узнаем в реальности,

47
00:03:28,340 --> 00:03:30,569
эту оценённую матрицу мы
легко можем посчитать.

48
00:03:30,569 --> 00:03:33,434
И, соответственно,
в R это будет всего лишь одна команда vcov

49
00:03:33,434 --> 00:03:37,099
(variance-covariance, то есть
ковариационная матрица) от

50
00:03:37,099 --> 00:03:44,190
оценённой модели.

