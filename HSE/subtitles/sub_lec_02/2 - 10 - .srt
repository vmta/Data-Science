1
00:00:13,260 --> 00:00:19,301
Оценки коэффициентов,
которые мы получаем методом

2
00:00:19,301 --> 00:00:23,370
наименьших квадратов, обладают рядом
замечательных статистических свойств.

3
00:00:23,370 --> 00:00:30,022
Эти свойства, их очень много, и поэтому,
чтобы как-то их было проще все осознать,

4
00:00:30,022 --> 00:00:34,918
мы их поделим на три части: это
базовые свойства – свойства,

5
00:00:34,918 --> 00:00:39,603
которые верны без предположения о
нормальном распределении ошибок ε_i и

6
00:00:39,603 --> 00:00:45,404
которые верны даже для малых или больших
выборок, неважен размер выборки.

7
00:00:45,404 --> 00:00:48,817
Вторая группа свойств – это
асимптотические свойства,

8
00:00:48,817 --> 00:00:53,136
то есть свойства хорошие, которые
верны при больших n, то есть при n,

9
00:00:53,136 --> 00:00:57,360
формально стремящихся к бесконечности,
с бытовой точки зрения,

10
00:00:57,360 --> 00:01:00,660
на больших выборках в десятки
или в сотни тысяч наблюдений.

11
00:01:00,660 --> 00:01:06,769
И третья группа свойств, которые верны на
малых выборках, даже там на 20, или 30,

12
00:01:06,769 --> 00:01:12,522
или 40 наблюдениях, но которые
требуют выполнения предпосылки о том,

13
00:01:12,522 --> 00:01:16,480
что ошибки ε_i имеют
нормальное распределение.

14
00:01:16,480 --> 00:01:18,099
Вот три группы свойств.

15
00:01:18,099 --> 00:01:22,264
Базовые, казалось бы,
самые интересные, но, к сожалению,

16
00:01:22,264 --> 00:01:24,999
их меньше всего,
и с помощью базовых свойств,

17
00:01:24,999 --> 00:01:29,712
не предположив нормальность распределения
или большой размер выборки, к сожалению,

18
00:01:29,712 --> 00:01:33,417
нельзя проверять гипотезы и
строить доверительные интервалы.

19
00:01:33,417 --> 00:01:37,601
Теперь перейдем ко всем предпосылкам,
рассмотрим каждую предпосылку одну за

20
00:01:37,601 --> 00:01:41,238
одной и посмотрим,
какие три группы свойств из них следуют.

21
00:01:41,238 --> 00:01:44,650
Итак, предпосылки,
необходимые нам для применения

22
00:01:44,650 --> 00:01:49,350
метода наименьших квадратов и для
получения хороших свойств коэффициентов.

23
00:01:49,350 --> 00:01:54,589
Итак, во-первых, мы предполагаем,
что истинная зависимость имеет

24
00:01:54,589 --> 00:01:59,840
вид y_i = β₁ + β₂ х_i + β_3 z_i и так далее.

25
00:01:59,840 --> 00:02:04,560
Я буду рассказывать на примере двух
объясняющих переменных, и, естественно,

26
00:02:04,560 --> 00:02:10,100
большее количество объясняющих
переменных мы просто делаем по аналогии.

27
00:02:10,100 --> 00:02:13,019
Соответственно, первая
предпосылка говорит о том,

28
00:02:13,019 --> 00:02:16,383
что зависимость игрека от
объясняющих переменных линейная.

29
00:02:16,383 --> 00:02:19,686
Линейность у нас по
регрессорам по отдельным,

30
00:02:19,686 --> 00:02:23,020
линейность у нас по коэффициентам β есть.

31
00:02:23,020 --> 00:02:28,770
В матричном виде эту предпосылку
можно записать, что y = Xβ + ε.

32
00:02:28,770 --> 00:02:32,735
Вторая предпосылка состоит в том,
что как устроена реальность,

33
00:02:32,735 --> 00:02:37,011
такая модель и оценивается,
то есть не только истина так устроена,

34
00:02:37,011 --> 00:02:41,855
но мы с помощью метода наименьших
квадратов строим регрессию объясняемой

35
00:02:41,855 --> 00:02:46,100
переменной именно на те регрессоры,
от которых y зависит,

36
00:02:46,100 --> 00:02:50,090
то есть строим регрессию
на константу, х_i и z_i.

37
00:02:50,090 --> 00:02:54,958
В матричном виде мы знаем, что в таком
случае можно выписать формулу для оценок

38
00:02:54,958 --> 00:03:00,410
МНК, и она будет равна (X'X)^{-1} X'y.

39
00:03:00,410 --> 00:03:04,615
Третья предпосылка простая, то,
что у нас наблюдений все-таки больше,

40
00:03:04,615 --> 00:03:08,379
чем оцениваемых коэффициентов β с крышкой.

41
00:03:08,379 --> 00:03:14,590
Дальше следует группа предпосылок о том,
как распределены ошибки ε_i.

42
00:03:14,590 --> 00:03:17,028
Во-первых, мы предполагаем предпосылку,

43
00:03:17,028 --> 00:03:20,220
которая называется предпосылкой
о строгой экзогенности.

44
00:03:20,220 --> 00:03:24,495
Чуть позже мы поговорим,
почему она так называется и так далее,

45
00:03:24,495 --> 00:03:29,358
а сейчас мы просто предположим, что
условное математическое ожидание ошибки

46
00:03:29,358 --> 00:03:33,770
ε_i при фиксированных
регрессорах X равняется нулю.

47
00:03:33,770 --> 00:03:36,529
В матричном виде, соответственно,

48
00:03:36,529 --> 00:03:40,033
E от ε_i при фиксированной
матрице X равняется нулю.

49
00:03:40,033 --> 00:03:44,894
Вторая предпосылка, которая называется
условная гомоскедастичность, то есть

50
00:03:44,894 --> 00:03:51,953
условная дисперсия ε_i при фиксированных
регрессорах постоянна и равна σ².

51
00:03:51,953 --> 00:03:56,440
И третья предпосылка –
отсутствие условной корреляции,

52
00:03:56,440 --> 00:04:03,480
то есть ковариация между ε_i и ε_j,
между разными ε, то есть i не равно j.

53
00:04:03,480 --> 00:04:07,870
Эта ковариация при
фиксированном X равна нулю.

54
00:04:07,870 --> 00:04:10,010
Это были предпосылки на ε.

55
00:04:10,010 --> 00:04:14,110
Кроме того,
у нас есть предпосылки на регрессоры.

56
00:04:14,110 --> 00:04:20,020
Они, конечно, стахостические, случайные,
но все-таки не совсем произвольные.

57
00:04:20,020 --> 00:04:21,995
Во-первых, мы предполагаем,

58
00:04:21,995 --> 00:04:28,040
что вероятность получения строгой линейной
зависимости между регрессорами равна нулю.

59
00:04:28,040 --> 00:04:32,520
То есть например, эта предпосылка
исключает возможность того, что мы среди

60
00:04:32,520 --> 00:04:36,986
объясняющих переменных включили рост в
сантиметрах и рост в метрах одновременно,

61
00:04:36,986 --> 00:04:38,226
либо то, либо другое.

62
00:04:38,226 --> 00:04:42,360
Эти переменные, очевидно, зависимые,
одна в сто раз больше, чем другая.

63
00:04:42,360 --> 00:04:47,100
Это наличие линейной зависимости, оно по
предпосылкам запрещено среди регрессоров.

64
00:04:47,100 --> 00:04:51,985
В матричном виде предпосылку о том, что
среди регрессоров нет линейно зависимых,

65
00:04:51,985 --> 00:04:56,870
можно сформулировать по-разному: можно
сказать, что rank матрицы X равен k,

66
00:04:56,870 --> 00:05:01,383
или можно сказать, что матрица X,
траспонированное X обратима.

67
00:05:01,383 --> 00:05:06,380
И еще одна предпосылка,
здесь для разных ситуаций

68
00:05:06,380 --> 00:05:11,309
используется разная предпосылка,
мы рассмотрим случай,

69
00:05:11,309 --> 00:05:16,932
когда у нас имеет место случайная выборка,
то есть мы предполагаем, что наши объекты,

70
00:05:16,932 --> 00:05:21,130
по которым мы пытаемся оценить
зависимость, – это случайная выборка.

71
00:05:21,130 --> 00:05:25,912
А именно, допустим, у нас было
много-много-много индивидов, все индивиды

72
00:05:25,912 --> 00:05:31,478
страны, мы пытались понять, как зарплата
зависит от уровня образования, и для этого

73
00:05:31,478 --> 00:05:36,144
мы случайным образом из всех-всех-всех
индивидов страны отобрали 1 000 человек

74
00:05:36,144 --> 00:05:40,204
для нашего исследования, или это может
быть случайная выборка предприятий.

75
00:05:40,204 --> 00:05:43,904
Мы пытались понять, от каких факторов,
скажем, зависит, будет

76
00:05:43,904 --> 00:05:48,445
ли предприятие банкротом или нет, и для
этого мы сделали случайную выборку всех

77
00:05:48,445 --> 00:05:53,222
предприятий из всего множества, мы взяли
1 000 предприятий или 2 000 предприятий,

78
00:05:53,222 --> 00:05:57,390
и оценивали модель по
этой случайной выборке.

79
00:05:57,390 --> 00:06:02,709
В частности, это означает, что регрессоры,
относящиеся к различным наблюдениям, 

80
00:06:02,709 --> 00:06:08,010
скажем, уровень образования i-человека
и уровень образования j-человека,

81
00:06:08,010 --> 00:06:09,130
они независимы.

82
00:06:09,130 --> 00:06:12,973
И поскольку имеет место случайная выборка,
то мы предполагаем,

83
00:06:12,973 --> 00:06:17,240
что закон распределения регрессоров
для каждого наблюдения одинаковый.

84
00:06:17,240 --> 00:06:22,840
Когда все эти предпосылки, которые мы
сформулировали, предпосылки о зависимости

85
00:06:22,840 --> 00:06:29,204
y от x, предпосылки о распределении ε,
предпосылки о регрессорах,

86
00:06:29,204 --> 00:06:33,900
когда все они выполнены, мы получаем,
что верны три группы свойств.

87
00:06:33,900 --> 00:06:36,024
Во-первых, базовые свойства.

88
00:06:36,024 --> 00:06:39,717
Первое базовое свойство говорит,
что β с крышкой j-ое,

89
00:06:39,717 --> 00:06:44,310
получаемое по методу наименьших квадратов,
эти оценки линейны по y,

90
00:06:44,310 --> 00:06:48,088
то есть любая оценка коэффициентов,

91
00:06:48,088 --> 00:06:53,290
будь то оценка коэффициента β₁ с крышкой,
или β₂ с крышкой, или β_3 с крышкой,

92
00:06:53,290 --> 00:06:58,869
любая оценка коэффициентов
представляется в виде некоторой функции:

93
00:06:58,869 --> 00:07:04,526
c_1 y_1 + c_2 y_2 и так далее,
где c_1 и c_2 – это величины,

94
00:07:04,526 --> 00:07:09,790
которые зависят только от
объясняющих переменных, но не от y.

95
00:07:09,790 --> 00:07:15,370
Из этого, в частности, следуют кое-какие
свойства приятные о β с крышкой.

96
00:07:15,370 --> 00:07:18,983
Это говорит о том,
что если мы, например,

97
00:07:18,983 --> 00:07:22,424
исследовали зависимость зарплаты в рублях,

98
00:07:22,424 --> 00:07:27,026
то если мы будем измерять зависимую
переменную в других единицах измерения,

99
00:07:27,026 --> 00:07:31,792
скажем, от рублей перейдем к
тысячам рублей, то, соответственно,

100
00:07:31,792 --> 00:07:36,257
все y_i упадут в 1 000 раз,
и линейность по y означает,

101
00:07:36,257 --> 00:07:41,009
что и все оценки коэффициентов β с
крышкой тоже упадут в 1 000 раз.

102
00:07:41,009 --> 00:07:44,592
Если мы регрессоры трогать не будем,
то есть у нас константы c_1, c_2,

103
00:07:44,592 --> 00:07:45,610
c_3 не поменяются,

104
00:07:45,610 --> 00:07:49,777
а зависимую переменную мы будем мерить в
других единицах измерения, соответственно,

105
00:07:49,777 --> 00:07:54,070
и коэффициенты, определяющие зависимость,
точно также упадут в 1 000 раз.

106
00:07:54,070 --> 00:07:57,066
Второе хорошее базовое свойство – то,

107
00:07:57,066 --> 00:08:02,319
что оценки коэффициентов β_j с крышкой
условно не смещены и не смещены,

108
00:08:02,319 --> 00:08:07,137
то есть математическое ожидание
от β с крышкой при фиксированных

109
00:08:07,137 --> 00:08:12,140
регрессорах равно соответствующему
истинному коэффициенту β.

110
00:08:12,140 --> 00:08:14,230
Это очень хорошее свойство.

111
00:08:14,230 --> 00:08:19,788
Это говорит о том, что наш метод, конечно,
может ошибаться, и оценка β с крышкой,

112
00:08:19,788 --> 00:08:24,702
которую мы получаем, может не совпадать
с настоящим β, но β с крышкой иногда

113
00:08:24,702 --> 00:08:29,742
будет больше настоящего β,
иногда будет меньше настоящего β,

114
00:08:29,742 --> 00:08:34,555
но в среднем, β с крышкой
попадает то влево, то вправо,

115
00:08:34,555 --> 00:08:38,910
но в среднем попадает в
неизвестный коэффициент β.

116
00:08:38,910 --> 00:08:43,955
Следующая группа базовых свойств – это то,
что оценки β с крышкой 

117
00:08:43,955 --> 00:08:48,918
являются эффективными среди
линейных и несмещенных оценок.

118
00:08:48,918 --> 00:08:52,940
Что это означает, эффективность
среди линейных и несмещенных оценок?

119
00:08:52,940 --> 00:08:59,220
Это означает, что если мы рассмотрим
любую другую линейную несмещенную оценку,

120
00:08:59,220 --> 00:09:04,263
то есть оценку, которая линейно
выражается через y и которая не смещена,

121
00:09:04,263 --> 00:09:07,120
то есть какая-то β с
крышкой альтернативная,

122
00:09:07,120 --> 00:09:11,330
получаемая не методом наименьших
квадратов, а каким-то другим методом,

123
00:09:11,330 --> 00:09:16,616
то эта оценка будет обладать,
альтернативная оценка,

124
00:09:16,616 --> 00:09:21,309
будет обладать большим разбросом, большей
дисперсией, большей неопределенностью,

125
00:09:21,309 --> 00:09:24,614
меньшей точностью,
чем оценка метода наименьших квадратов.

126
00:09:24,614 --> 00:09:26,130
Это очень хорошее свойство.

127
00:09:26,130 --> 00:09:29,900
Оно говорит о том, что если вы хотите
простую оценку, то есть линейную,

128
00:09:29,900 --> 00:09:35,690
хотите оценку несмещенную, которая в
среднем бы попадала в неизвестную истину,

129
00:09:35,690 --> 00:09:41,772
то ничего лучше оценок метода наименьших
квадратов у нас не получится.

130
00:09:41,772 --> 00:09:46,624
То есть математически это означает,
что условная дисперсия β

131
00:09:46,624 --> 00:09:50,796
с крышкой при фиксированных регрессорах
альтернативного больше либо равна,

132
00:09:50,796 --> 00:09:55,017
чем условная дисперсия β с крышкой,
полученная по методу наименьших

133
00:09:55,017 --> 00:09:59,747
квадратов опять же при
фиксированных регрессорах.

134
00:09:59,747 --> 00:10:04,891
Следующая группа базовых свойств
— это свойства о том, чему

135
00:10:04,891 --> 00:10:10,860
конкретно равны дисперсии, ковариации,
математические ожидания получаемых оценок.

136
00:10:10,860 --> 00:10:15,900
А именно: мы доказывали, что
ковариационная матрица вектора β с крышкой

137
00:10:15,900 --> 00:10:22,280
при фиксированных иксах — это σ²,
помноженная на (X'X) в минус первой.

138
00:10:22,280 --> 00:10:27,582
Соответственно, дисперсия отдельно
взятого коэффициента β_j с крышкой

139
00:10:27,582 --> 00:10:32,649
при фиксированных иксах — это σ²,
делённая на RSS_j,

140
00:10:32,649 --> 00:10:38,480
где RSS_j — это RSS, получаемая в регрессии
j-ой объясняющей переменные на остальные.

141
00:10:38,480 --> 00:10:43,065
Ковариация β_j с крышкой с

142
00:10:43,065 --> 00:10:48,450
любым остатком ε_i с крышкой при
фиксированных иксах равна нулю.

143
00:10:48,450 --> 00:10:53,410
И математическое ожидание σ²
с крышкой при фиксированных иксах

144
00:10:53,410 --> 00:10:54,770
равно σ².

145
00:10:54,770 --> 00:10:58,567
То есть это свойство говорит,
что оценка σ² с крышкой,

146
00:10:58,567 --> 00:11:03,070
предложенная нами, — RSS, делённая
на (n – k), — она тоже несмещённая,

147
00:11:03,070 --> 00:11:06,016
и она несмещённым образом
оценивает σ².

148
00:11:06,016 --> 00:11:08,571
На этом базовые свойства оканчиваются.

149
00:11:08,571 --> 00:11:12,444
Оставшиеся свойства требуют либо
предположения о том, что n,

150
00:11:12,444 --> 00:11:15,325
количество наблюдений,
очень-очень большое,

151
00:11:15,325 --> 00:11:21,343
либо предположение о том дополнительное,
что ε_i имеют нормальное распределение.

152
00:11:21,343 --> 00:11:23,850
Сначала рассмотрим свойства,

153
00:11:23,850 --> 00:11:28,270
которые требуют предположения о том,
что количество наблюдений велико.

154
00:11:28,270 --> 00:11:33,530
Итак, с ростом n, по мере того,
как n стремится к бесконечности,

155
00:11:33,530 --> 00:11:36,514
оценки β_j с крышкой,
— это случайные величины,

156
00:11:36,514 --> 00:11:41,550
— они будут стремится по вероятности
к неизвестным константам β_j.

157
00:11:41,550 --> 00:11:46,017
Второе свойство,
которое нам позволяет проверять гипотезы,

158
00:11:46,017 --> 00:11:50,718
и это очень здорово, что β_j с крышкой,
оценка коэффициента,

159
00:11:50,718 --> 00:11:55,010
минус истинный коэффициент
β_j делить на стандартную

160
00:11:55,010 --> 00:11:59,341
ошибку β_j с крышкой — эта величина,
случайная величина,

161
00:11:59,341 --> 00:12:03,969
и с ростом количества наблюдений
распределение этой случайной величины

162
00:12:03,969 --> 00:12:08,797
становится все более и более похожим на
стандартное нормальное распределение.

163
00:12:08,797 --> 00:12:13,364
Ещё одно свойство, что оценка σ²
с крышкой — тоже замечательное: с

164
00:12:13,364 --> 00:12:17,302
ростом количества наблюдений
случайная величина σ²

165
00:12:17,302 --> 00:12:21,170
с крышкой по вероятности
стремится к константе σ².

166
00:12:21,170 --> 00:12:27,610
На всякий случай я напомню, что σ²
с крышкой — это RSS, делённая на (n – k).

167
00:12:27,610 --> 00:12:35,000
Следующая группа свойств — это свойства,
требующие предположения о нормальности ε_i,

168
00:12:35,000 --> 00:12:41,031
а именно: теперь дополнительно известно,
что при фиксированных иксах ε_i нормальное,

169
00:12:41,031 --> 00:12:45,740
с математическим ожиданием
ноль и дисперсией σ².

170
00:12:45,740 --> 00:12:50,902
При таком дополнительном
предположении оказывается,

171
00:12:50,902 --> 00:12:55,603
что оценки β_j с крышкой эффективные,
то есть являются самыми лучшими,

172
00:12:55,603 --> 00:12:58,960
с наименьшей дисперсией не только
среди линейных по игрекам,

173
00:12:58,960 --> 00:13:01,119
но и среди любых несмещённых оценок.

174
00:13:01,119 --> 00:13:04,425
Это говорит о том,
что если мы хотим несмещённую оценку,

175
00:13:04,425 --> 00:13:08,440
которая в среднем попадает в
неизвестный коэффициент β,

176
00:13:08,440 --> 00:13:13,316
то ничего лучше метода наименьших
квадратов, — даже архисложного,

177
00:13:13,316 --> 00:13:18,664
нелинейного по игрекам, нелинейного
по иксам, — ничего придумать нельзя,

178
00:13:18,664 --> 00:13:23,480
то есть наши оценки β с крышкой
— это несмещённая мечта.

179
00:13:23,480 --> 00:13:28,230
И ещё два свойства,
которые позволяют проверять гипотезы.

180
00:13:28,230 --> 00:13:30,873
Если мы хотим проверить
гипотезы о неизвестном β_j,

181
00:13:30,873 --> 00:13:32,980
то мы можем воспользоваться тем фактом,

182
00:13:32,980 --> 00:13:37,673
что оценка β_j с крышкой минус β_j
делить на стандартную ошибку в малой

183
00:13:37,673 --> 00:13:42,685
выборке при фиксированных иксах имеет
t-распределение с (n – k) степенями свободы,

184
00:13:42,685 --> 00:13:46,370
где n — это количество наблюдений,
k — это количество коэффициентов.

185
00:13:46,370 --> 00:13:50,848
И аналогично можно проверять гипотезы
и строить доверительные интервалы для

186
00:13:50,848 --> 00:13:54,210
σ², используя тот факт, что RSS,

187
00:13:54,210 --> 00:13:57,906
делённая на σ²
при фиксированных иксах,

188
00:13:57,906 --> 00:14:03,670
имеет хи-квадрат распределение
с (n – k) степенями свободы.

189
00:14:03,670 --> 00:14:07,440
На этом весь список свойств
пока заканчивается.

