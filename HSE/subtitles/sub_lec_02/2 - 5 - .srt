1
00:00:13,250 --> 00:00:19,161
Из геометрической
интерпретации можно запомнить,

2
00:00:19,161 --> 00:00:23,130
собственно, две важных составляющих,
а именно то,

3
00:00:23,130 --> 00:00:28,322
что мы интерпретируем дисперсию как
квадрат длины случайной величины, и мы

4
00:00:28,322 --> 00:00:34,060
интерпретируем корреляцию между случайными
величинами как косинус угла между ними.

5
00:00:34,060 --> 00:00:39,040
Возникает такой естественный вопрос:
а зачем нам вообще случайные величины

6
00:00:39,040 --> 00:00:40,967
интерпретировать геометрически?

7
00:00:40,967 --> 00:00:45,794
Ответ такой, что на самом деле при
такой интерпретации начинает работать

8
00:00:45,794 --> 00:00:50,227
вся школьная геометрия: оказывается, верна
теорема Пифагора для случайных величин или

9
00:00:50,227 --> 00:00:52,446
теорема Фалеса,
или теорема о трёх перпендикулярах,

10
00:00:52,446 --> 00:00:56,650
— все их можно выписать для случайных
величин, и они будут верными, и,

11
00:00:56,650 --> 00:01:01,500
фактически, появляется бесплатное
доказательство каких-то сложных фактов.

12
00:01:01,500 --> 00:01:06,036
С помощью условного математического
ожидания мы сформулируем стандартные

13
00:01:06,036 --> 00:01:08,620
предпосылки на случайную составляющую ε,

14
00:01:08,620 --> 00:01:12,704
а именно: мы будем
предполагать три предпосылки.

15
00:01:12,704 --> 00:01:17,147
Математическое ожидание от каждой
случайной составляющей при известных

16
00:01:17,147 --> 00:01:20,890
иксах, то есть при известном каждом
регресссоре для каждого наблюдения,

17
00:01:20,890 --> 00:01:22,564
я пишу коротко матрицу X.

18
00:01:22,564 --> 00:01:26,978
Значит, условное математическое
ожидание от ε_i при всех известных

19
00:01:26,978 --> 00:01:28,510
регрессорах равна 0.

20
00:01:28,510 --> 00:01:31,937
Условное математическое
ожидание от ε_i^2 при

21
00:01:31,937 --> 00:01:34,847
всех условных регрессорах равна σ².

22
00:01:34,847 --> 00:01:36,684
Или можно точно так же сказать,

23
00:01:36,684 --> 00:01:41,525
что условная дисперсия ε_i при
известной матрице X равна σ².

24
00:01:41,525 --> 00:01:47,621
И ковариация между ε_i и ε_j при
фиксированной матрице X равна нулю.

25
00:01:47,621 --> 00:01:52,084
Эти три предпосылки, точнее,
две — про дисперсию и ковариацию —

26
00:01:52,084 --> 00:01:56,931
можно коротко записать с помощью такого
понятия, как ковариационная матрица.

27
00:01:56,931 --> 00:02:00,810
Когда мы говорим «ковариационная
матрица некоего случайного вектора»,

28
00:02:00,810 --> 00:02:02,830
мы имеем в виду здоровую табличку чисел.

29
00:02:02,830 --> 00:02:08,630
Первое число в первой
строке — это дисперсия ε_1.

30
00:02:08,630 --> 00:02:13,478
Второе число в первой строке, то есть
первая строчка, второй столбец, 

31
00:02:13,478 --> 00:02:16,350
(1,2) координаты, — это ковариация ε_1 и ε_2.

32
00:02:16,350 --> 00:02:20,621
Соответственно, в ковариационной матрице,
скажем, в третьей строчке,

33
00:02:20,621 --> 00:02:26,620
втором столбце находится ковариация ε_3 и
ε_2, это третья строчка, второй столбец.

34
00:02:26,620 --> 00:02:32,336
Соответственно, в этой матрице
находятся и все дисперсии каждого ε,

35
00:02:32,336 --> 00:02:37,392
и все попарные ковариации: ε_1 с ε_3,
ε_2 с ε_7... все-все-все ковариации

36
00:02:37,392 --> 00:02:41,715
и дисперсии находятся в одной матрице,
в одной табличке чисел.

37
00:02:41,715 --> 00:02:47,336
Соответственно, первые наши две
предпосылки на ε можно как сформулировать?

38
00:02:47,336 --> 00:02:53,360
Что ковариации равны нулю,
а на диагонали дисперсии равны σ².

39
00:02:53,360 --> 00:02:55,845
То есть матрица принимает
такой простой вид.

40
00:02:55,845 --> 00:02:59,542
В этой табличке чисел на диагонали
стоят σ², σ²... σ².

41
00:02:59,542 --> 00:03:02,550
А вне диагонали стоят одни нули.

42
00:03:02,550 --> 00:03:06,930
В линейной алгебре есть такое понятие
«единичная матрица», это матрица,

43
00:03:06,930 --> 00:03:09,890
у которой по диагонали стоят единички,
а вне диагонали — нули.

44
00:03:09,890 --> 00:03:14,860
Соответственно, в нашем случае наши
предпосылки можно записать как

45
00:03:14,860 --> 00:03:19,830
ковариационная матрица вектора ε
при фиксированных регрессорах X

46
00:03:19,830 --> 00:03:23,378
равна σ² умножить на
эту самую единичную матрицу,

47
00:03:23,378 --> 00:03:28,721
которая обозначается буковкой I,
сокращение от английского «identity».

48
00:03:28,721 --> 00:03:29,950
Подведём итог.

49
00:03:29,950 --> 00:03:32,232
Какие у нас есть предпосылки?

50
00:03:32,232 --> 00:03:37,488
У нас есть предпосылки, что дисперсия ε
при фиксированных регрессорах X равна

51
00:03:37,488 --> 00:03:43,248
σ² умножить на единичную матрицу,
что на самом деле просто означает,

52
00:03:43,248 --> 00:03:47,670
что дисперсия ε_i при
фиксированном X равна σ²,

53
00:03:47,670 --> 00:03:51,580
а ковариация разных ε при
фиксированном X равна нулю.

54
00:03:51,580 --> 00:03:56,268
Математическое ожидание от ε_i
при фиксированных X равно нулю,

55
00:03:56,268 --> 00:03:59,580
но нам эта предпосылка
даже не потребуется.

56
00:03:59,580 --> 00:04:04,650
И у нас есть линейная модель,
то есть y_i = β₁ + β₂ x_i + ε_i

57
00:04:04,650 --> 00:04:09,979
(случайная составляющая).

58
00:04:09,979 --> 00:04:13,420
Это для примера двух
объясняющих переменных.

59
00:04:13,420 --> 00:04:19,051
И, соответственно, эти предпосылки
позволяют посчитать дисперсию

60
00:04:19,051 --> 00:04:26,760
любой оценки МНК β_j с крышкой и любую
ковариацию β_j с крышкой и β_l с крышкой.

61
00:04:26,760 --> 00:04:32,055
Мы посчитаем для начала дисперсию,
условную, оценки метода наименьших

62
00:04:32,055 --> 00:04:37,343
квадратов и ковариацию оценки метода
наименьших квадратов для случая

63
00:04:37,343 --> 00:04:41,960
парной регрессии.

