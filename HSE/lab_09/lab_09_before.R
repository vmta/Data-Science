# Esli russkie bukvi prevratilitis v krakozyabry,
# to File - Reopen with encoding... - UTF-8 - Set as default - OK

# lab 09

library("dplyr") # манипуляции с данными
library("caret") # стандартизованный подход к регрессионным и классификационным моделям
library("AER") # инструментальные переменные 
library("ggplot2") # графики
library("sandwich") # робастные стандартные ошибки
library("ivpack") # дополнительные плющки для инструментальных переменных
library("memisc") # табличка mtable 


#
# Для хорошего и качественного прогнозирования, необходимо 
# разделить выборку на обучающую (по которой будет строиться модель)
# и тестовую, на которой будет проверяться оценка прогнозов и
# качество самой модели.
#
# Загружаем данные
h <- read.csv("../datasets/flats_moscow.txt", header = TRUE, sep = "\t", dec = ".")
glimpse(h)

# Создадим набор отобранных номеров по которым будем моделировать
# (задаем зависимую переменную h$price, укажем, что из наших данных
# в этот набор должно попасть 75% наблюдейни, а по оставшимся 25%
# позже будем тестировать полученную модель)
in_train <- createDataPartition(y = h$price, p = 0.75, list = FALSE)
# in_train
h_train <- h[in_train,]
h_test <- h[-in_train,]

# Оценим две линейные модели по обучающему набору данных.
model_1 <- lm(data = h_train, log(price) ~ log(totsp) + log(kitsp) + log(livesp))
model_2 <- lm(data = h_train, log(price) ~ log(totsp) + brick)

# Проведем тестирование моделей.
# y - реальные значения из тестируемого набора данных
y <- log(h_test$price)
# y_hat_1 - спрогнозированные значения по модели 1
y_hat_1 <- predict(model_1, h_test)
y_hat_1
# y_hat_2 - спрогнозированные значения по модели 2
y_hat_2 <- predict(model_2, h_test)
y_hat_2
# Посчитаем сумму квадратов ошибок прогноза (Метод Наименьших Квадратов)
sum((y - y_hat_1)^2)
sum((y - y_hat_2)^2)

# В данном случае, сумма квадратов ошибок по модели 2 показывает
# лучшее значение не смотря на тот факт, что в этой модели меньше
# регрессоров!!! (Вывод - вторая модель лучше для прогнозирования,
# т.к. она ближе к реальным значениям!)


#
# ЭНДОГЕННОСТЬ
# Двухшаговый метод наименьших квадратов в парной регрессии
#

# Рассмотрим на примере данных по сигаретам
data("CigarettesSW")
h <- CigarettesSW

# Индивиду важен его доход, а не совокупный доход по штату, как и
# относительная индивидуальная цена потребления (а не совокупная)
# поэтому изменим набор данных.
# Добавим переменные:
#  - rprice - цена/индекс потребительских цен
#  - rincome - доход/индекс потребительски цен/численность населения
#  - rtax - налог/индекс потребительских цен
h2 <- mutate(h,
             rprice = price/cpi,
             rincome = income/cpi/population,
             rtax = tax/cpi)
# Т.к. в исходном наборе данные собраны за два года, отберем наиболее
# "свежие" за 1995 год
h3 <- filter(h2, year == 1995)
# Построим линейную модель зависимости количества пачек от стоимости
model_0 = lm(data = h3, log(packs) ~ log(rprice))
summary(model_0)
# Модель качественная, все коэффициенты значимы, НО зачем такая
# модель? Такая модель может быть интересна с точки зрения гос.
# политики. Т.е. например - если гос. стремится снизить потребление
# сигарет, то как повлияет ценовая политика на спрос...

# Но нам может быть интересна иная модель, которая тянет за собой
# эндогенность... попробуем разобраться.
#
# Two stage OLS (ordinary least squares)

# Making it manually...
# step 1
st_1 <- lm(data = h3, log(price) ~ rtax)
h3$log_price_hat <- fitted(st_1)
# step 2
st_2 <- lm(data = h3, log(packs) ~ log_price_hat)
summary(st_2)

# Making it automatically...
model_iv <- ivreg(data = h3, log(packs) ~ log(rprice) | rtax)
summary(model_iv)

# Сравним все три модели (МНК, ручная модель на 2м шаге,
# двушаговая модель МНК или ментод инструментальных переменных)
mtable(model_0, st_2, model_iv)
# МНК коэффициенты отличаются ввиду неучтенной эндогенности
# Коэффициенты авто/ручного моделирования совпадают, но в авто
# режиме правильно считаются коэффициенты стандартных ошибок,
# тогда как в ручном режиме считаются стандартные ошибки как если
# бы моделировалась обычная линейная регрессия, поэтому они не
# являются ни корректными, ни состоятельными.

# Нюансы двушагового МНК
#
# 1. робастные стандартные ошибки
coeftest(model_iv, vcov=vcovHC)
# 2. Наличие экзогенных регрессоров
model_iv_2 <- ivreg(data = h3,
                  log(packs) ~ log(rprice) + log(rincome) |
                    log(rincome) + rtax)
coeftest(model_iv_2, vcov=vcovHC)
# 3. Наличие нескольких инструментальных переменных
h3 <- mutate(h3, rtax2 = (taxs - tax)/cpi)
model_iv_3 <- ivreg(data = h3,
                  log(packs) ~ log(rprice) + log(rincome) |
                    log(rincome) + rtax + rtax2)
coeftest(model_iv_3, vcov=vcovHC)
