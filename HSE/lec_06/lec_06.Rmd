---
title: "Автокорреляция"
babel-lang: russian
lang: russian
header-includes: 
  - \author[Эконометрика. Лекция 6]{Эконометрика. Лекция 6}
  - \newcommand{\e}{\varepsilon}
output:
  beamer_presentation:
    keep_tex: yes
    theme: Madrid
    colortheme: whale
  ioslides_presentation: default
---



# Автокорреляция

Автокорреляция --- нарушение предпосылки

$Cov(\e_i,\e_j|X)=E(\e_i \e_j | X)=0$ при $i\neq j$

# Прежняя предпосылка

Для проверки гипотез мы предполагали условную некоррелированность ошибок:

$Cov(\e_i,\e_j|X)=E(\e_i \e_j | X)=0$ при $i\neq j$

Что произойдет если эта предпосылка будет нарушена?


# Когда логично ожидать автокорреляцию?

* "близость" наблюдений во времени или в пространстве

* наличие ненаблюдаемого фактора, действующего на "соседние" наблюдения

# Автокорреляцию подробно изучают!

* анализ временных рядов

* пространственная эконометрика

* анализ панельных данных

# Автокорреляция бывает небезобидной

* может привести к несостоятельности оценок $\hat{\beta}$

# Пример у доски

Известно, что все ошибки равны между собой, и равновероятно принимают значения +1 или -1, т.е.

$\e_1=\e_2=\ldots=\e_n=\pm 1$

Будут ли МНК оценки коэффициентов модели $y_i=\beta_1 + \beta_2 x_i + \e_i$ состоятельны?

Отметим, что $E(\e_1\e_2 | X)=1$

# Автокорреляция может иметь очень сложную богатую структуру

Не пугайтесь, эти страшные слова означают лишь определенный тип структуры автокорреляции:

* AR, MA, ARMA, ARIMA, VAR, VMA, VARMA, VECM, ARCH, GARCH, EGARCH, FIGARCH, TARCH, AVARCH, ZARCH, CCC, DCC, BEKK, VEC, DLM, ... 


# Мы рассмотрим автокорреляцию порядка $p$

* Начнем с автокорреляции первого порядка, $p=1$

 $\e_{t}=\rho \e_{t-1}+u_t$

# Предпосылки 

* $\e_{t}=\rho \e_{t-1}+u_t$

* $u_t$ --- независимы между собой, 

* $u_t$ независимы от регрессоров

* $u_t$ одинаково распределены

* $E(u_t)=0$, $Var(u_t)=\sigma^2_u$


# Упражнение [доска]

Как выглядит $Corr(\e_{t}, \e_{t-k})$ при автокорреляции первого порядка?

# Автокорреляция порядка $p$:

$\e_{t}=\phi_1 \e_{t-1}+\phi_2 \e_{t-2} +\ldots + \phi_p \e_{t-p}+u_t$

Допускает более богатую структуру $Corr(\e_i, \e_j)$

Как и в случае автокорреляции первого порядка, 
\[
\lim_{k\to\infty} Corr(\e_t, \e_{t-k})=0
\]

# Условная автокорреляция и другие предпосылки

* автоматом нарушена предпосылка о незавимости наблюдений $(x_{i.}, y_i)$  

* во временных рядах обычно нарушена предпосылка $E(\e_t | X)=0$

Например, использование $y_{t-1}$ в качестве регрессора нарушает $E(\e_t | X)=0$

Мы будем анализируем ситуацию, в которой все остальные предпосылки кроме некоррелированности ошибок выполнены.

# Мы используем прежние формулы:

* Для оценок коэффициентов:
$\hat{\beta}=(X'X)^{-1}X'y$

* Для оценки ковариационной матрицы оценок коэффициентов,
$\widehat{Var}(\hat{\beta}|X)=\frac{RSS}{n-k}(X'X)^{-1}$

* В частности, $\widehat{Var}(\hat{\beta}_j|X)=\frac{\hat{\sigma}^2}{RSS_j}$
и $se(\hat{\beta}_j)=\sqrt{\widehat{Var}(\hat{\beta}_j|X)}$


# Три группы свойств:

* конечная выборка без предположения о нормальности $\e$

* конечная выборка с предположением о нормальности $\e$

* асимптотические свойства без предположения о нормальности  $\e$

Что происходит в каждом случае?

# Конечная выборка без предположения о нормальности $\e$

* (+) Линейность по $y$

* (+) Несмещенность, $E(\hat{\beta}|X)=\beta$, $E(\hat{\beta})=\beta$

* (---) Оценки неэффективны

#  Конечная выборка с предположением о нормальности $\e$

* (---) $\frac{\hat{\beta}_j-\beta_j}{se(\hat{\beta}_j)} | X \sim t_{n-k}$

* (---) $\frac{RSS}{\sigma^2} |X \sim \chi^2_{n-k}$

* (---) $\frac{(RSS_R-RSS_{UR})/r}{RSS_{UR}/(n-k)} \sim F_{r,n-k}$

#  Асимптотические свойства:

* (+) $\hat{\beta} \to \beta$

* (+) $\frac{RSS}{n-k} \to \sigma^2$ 

* (---) $\frac{\hat{\beta}_j-\beta_j}{se(\hat{\beta}_j)} \to N(0,1)$

* (---) $\frac{RSS_R-RSS_{UR}}{RSS_{UR}/(n-k)} \to \chi^2_r$

# Мораль:

* Сами $\hat{\beta}$ можно интерпретировать и использовать

* Стандартные ошибки $se(\hat{\beta}_j)$ несостоятельны

* Не можем строить доверительные интервалы для $\beta_j$ и проверять гипотезы о $\beta_j$

# Что делать?

* Исправить стандартные ошибки! 

* Другая формула для оценки $\widehat{Var}_{HAC}(\hat{\beta}|X)$

* Следовательно, другие $se_{HAC}(\hat{\beta}_j)$

# Робастная (устойчивая) к условной гетероскедастичности и автокорреляции оценка ковариационной матрицы

* Вместо $\widehat{Var}(\hat{\beta}|X)=\frac{RSS}{n-k}(X'X)^{-1}$ 

использовать 
\[
\widehat{Var}_{HAC}(\hat{\beta}|X)=(X'X)^{-1}\hat{\Phi}(X'X)^{-1}
\]

* Нью-Вест (Newey-West), 1987  (Существует много вариантов)

\[
\hat{\Phi} = \sum_{j=-k}^k \frac{k-|j|}{k} \left(  \sum_t \hat{\e}_t\hat{\e}_{t+j} x_{t\,\cdot}'x_{t+j\,\cdot} \right)
\]

# Суть корректировки:

Мы меняем $se(\hat{\beta}_j)$ на $se_{HAC}(\hat{\beta}_j)$

Какие проблемы решены?

* (+) $\frac{\hat{\beta}_j-\beta_j}{se_{HAC}(\hat{\beta}_j)} \to N(0,1)$ (УРА!)

# Какие проблемы не решены?

* (---) оценки $\hat{\beta}$ не меняются и остаются неэффективными


Даже при предположении о нормальности $\e$:

* (---) $\frac{\hat{\beta}_j-\beta_j}{se(\hat{\beta}_j)} | X \sim t_{n-k}$

* (---) $\frac{RSS}{\sigma^2} |X \sim \chi^2_{n-k}$

* (---) $\frac{(RSS_R-RSS_{UR})/r}{RSS_{UR}/(n-k)} \sim F_{r,n-k}$

# С практической точки зрения:

* Новая формула для $\widehat{Var}_{HAC}(\hat{\beta}|X)$, и, следовательно, для  $se_{HAC}(\hat{\beta}_j)$

* Робастная оценка ковариационной матрицы в R:
```{r, eval=FALSE}
model <- lm(y~x, data=data)
vcovHAC(model)
```

* С ней жизнь прекрасна!

$\frac{\hat{\beta}_j-\beta_j}{se_{HAC}(\hat{\beta}_j)} \to N(0,1)$

# Когда следует использовать 

* Когда мы подозреваем наличие автокорреляции и не хотим заниматься её моделированием

# Обнаружение автокорреляции

* Оцениваем интересующую нас модель с помощью МНК

* Строим график остатков в осях $\hat{\e}_{t-1}$, $\hat{\e}_{t}$

# Положительная автокорреляция

$\e_t=0.9\e_{t-1}+u_t$

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library("ggplot2")
n <- 100
eps <- arima.sim(model=list(ar=0.9),n=n)
x <- rnorm(n)
y <- 2+3*x+eps
model <- lm(y~x)
r <- resid(model)
qplot(head(r,-1),tail(r,-1),ylab="e_t",xlab="e_{t-1}")
```


# Отрицательная автокорреляция

$\e_t=-0.9\e_{t-1}+u_t$

```{r, message=FALSE, warning=FALSE, echo=FALSE}
n <- 100
eps <- arima.sim(model=list(ar=-0.9),n=n)
x <- rnorm(n)
y <- 2+3*x+eps
model <- lm(y~x)
r <- resid(model)
qplot(head(r,-1),tail(r,-1),ylab="e_t",xlab="e_{t-1}")
```


# Отсутствие автокорреляции

$\e_t$ независимы

```{r, message=FALSE, warning=FALSE, echo=FALSE}
n <- 100
eps <- rnorm(n)
x <- rnorm(n)
y <- 2+3*x+eps
model <- lm(y~x)
r <- resid(model)
qplot(head(r,-1),tail(r,-1),ylab="e_t",xlab="e_{t-1}")
```


# Формальные тесты на автокорреляцию

* тест Дарбина-Уотсона (Durbin-Watson)

* тест Бройша-Годфри (Breusch-Godfrey)


# Тест Дарбина-Уотсона, предпосылки:

* Автокорреляция первого порядка в остатках

\[
\e_t=\rho \e_{t-1} + u_t
\]

* нормальность ошибок $\e$

* сильная экзогенность, $E(\e_t | X)=0$

* $H_0$ об отсутствии автокорреляции, $\rho=0$

# Процедура теста Дарбина-Уотсона

* Шаг 1. Оценить основную регрессию, получить $\hat{\e}_i$

* Шаг 2. Посчитать статистику 
$$DW=\frac{\sum_{i=2}^n(\hat{\e}_i-\hat{\e}_{i-1})^2}{\sum_{i=1}^n \hat{\e}_i^2}$$

# Распределение статистики $DW$

* $H_0$ об отсутствии автокорреляции, $\rho=0$

* Точный закон распределения статистики $DW$ сложным образом зависит от $X$

* Если $\hat{\rho}$ --- выборочная корреляция остатков, то $DW=2(1-\hat{\rho})$

# Качественные выводы по статистике $DW$

$DW=2(1-\hat{\rho})$, поэтому $0<DW<4$

* $DW\approx 0$ означает положительную автокорреляцию $\hat{\rho}\approx 1$

* $DW\approx 2$ означает отсутствие автокорреляции $\hat{\rho}\approx 0$

* $DW\approx 4$ означает отрицательную автокорреляцию $\hat{\rho}\approx -1$

<!--
# иллюстрация (рисунок прилагается: график про Дарбина-Уотсона)

теховские надписи для графиков:

$H_0$ не отвергается
$H_0$ отвергается
$DW_{cr}$
$H_0$: $\rho=0$
-->

# С практической точки зрения: 

* `R` рассчитывает точные P-значения для теста $DW$

* Если P-значение меньше уровня значимости $\alpha$, то гипотеза $H_0$ об отсутствии автокорреляции отвергается

* Для любителей истории существуют статистические таблицы  диапазонов критических значений


# Тест Бройша-Годфри, предпосылки

* для тестирования автокорреляции порядка $p$ в ошибках
\[
\e_t=\phi_1 \e_{t-1} + \ldots + \phi_p \e_{t-p} + u_t
\]

* не требуется нормальность остатков 

* верен при ряде нарушений предпосылки $E(\e_t | X)=0$

* асимптотический

$H_0$: $\phi_1=\phi_2=\ldots=\phi_p=0$

# Процедура теста Бройша-Годфри

* Шаг 1. Оцениваем исходную модель, получаем остатки $\hat{\e}_t$

* Шаг 2. Строим вспомогательную регрессию  $\hat{\e}_t$ на исходные регрессоры, $\hat{\e}_{t-1}$, $\hat{\e}_{t-2}$, ..., $\hat{\e}_{t-p}$, находим $R^2_{aux}$

* Шаг 3. Считаем статистику $BG=(n-p)R^2_{aux}$

# Распределение статистики $BG$

* При верной $H_0$ об отсутствии автокорреляции

$H_0$: $\phi_1=\phi_2=\ldots=\phi_p=0$

$BG=(n-p)R^2_{aux} \sim \chi^2_p$

* Если статистика $BG$ больше критического значения $\chi^2_{cr}$, то $H_0$ об отсутствии автокорреляции отвергается.
<!--
Здесь график распределения BG (рисунок прилагается)
Подписи на графике:

$H_0$ не отвергается
$H_0$ отвергается
$\chi^2_{cr}$
$H_0$: $\phi_1=\phi_2=\ldots=\phi_p=0$
-->
 
# Тест Бройша-Годфри требует меньше предпосылок

Хотя тест Дарбина-Уотсона распространен, следует предпочитать тест Бройша-Годфри.

# Пример теста Дарбина-Уотсона и Бройша-Годфри [доска]



# Мораль

* Мы рассмотрели ситуацию нарушения предпосылки условной некоррелированности ошибок модели

* Нарушена во временных рядах и пространственных данных
 
* В простейшем случае для проверки гипотез достаточно использовать специальные стандартные ошибки $se_{HAC}$ 
 
* Если заниматься исследованием структуры автокорреляции серьезно, то это отдельные дисциплины


# Источники мудрости:

* Артамонов Н.В., Введение в эконометрику: глава 3.4, 3.5

* Борзых Д.А., Демешев Б.Б. Эконометрика в задачах и упражнениях: глава 11

* Катышев П.К., Пересецкий А. А. Эконометрика. Начальный курс: глава 6.2

* Себер Дж., Линейный регрессионный анализ: глава 6.2
