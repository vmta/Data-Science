# ESLI RUSSKIE BUKVI NE VIDNI ---> File -- Reopen with encoding --- utf8 --- set
# as default --- ok


library("HSAUR")  # из этого пакета возьмем набор данных по семиборью
library("dplyr")  # манипуляции с данными
library("psych")  # описательные статистики
library("lmtest")  # тесты для линейных моделей
library("glmnet")  # LASSO + ridge
library("ggplot2")  # графики
library("car")  # vif

# Последствия мультиколлинеарности


# поместим в табличку h известный нам набор данных
h <- cars

# всегда строим график
qplot(data = h, speed, dist)
# настала пора признаться: мы не видим однозначно, является ли зависимость
# линейной или квадратичной или даже кубической


# добавим квадрат и куб скорости в исходный набор данных
h <- mutate(h, speed2 = speed^2, speed3 = speed^3)

# оценим модель с тремя объясняющими переменными
model_mk <- lm(data = h, dist ~ speed + speed2 + speed3)
summary(model_mk)
# все коэффициенты незначимы явно имеет место мультиколлинеарность

# количественный признаки мультиколлинеарности
vif(model_mk)  # коэффициент вздутия дисперсии

# создадим матрицу из регрессоров (без единичного столбца)
X0 <- model.matrix(data = h, dist ~ 0 + speed + speed2 + speed3)
head(X0)  # начало матрицы
cor(X0)  # выборочные корреляции всех регрессоров

# доверительные интервалы для отдельных коэффициентов широкие
confint(model_mk)
# а сами коэффициенты незначимы:
coeftest(model_mk)

# однако попробуем спрогнозировать создадим набор данных с одной машиной:
nd <- data.frame(speed = 10, speed2 = 100, speed3 = 1000)
# построим предиктивный интервал
predict(model_mk, newdata = nd, interval = "prediction")
# и он оказывается не так уж плох :)

# для сравнения вспомним старую модель с одним регрессором
model <- lm(data = h, dist ~ speed)
coeftest(model)  # коэффициент значим

# предиктивный интервал по модели без мультиколлинеарности
predict(model, newdata = nd, interval = "prediction")

# доверительные интервалы
confint(model)  # модель с одним регрессором без коллинеарности
confint(model_mk)  # модель с тремя регрессорами и коллинеарностью





# Ридж и LASSO



y <- h$dist  # в вектор y поместим зависимую переменную
# еще раз создадим матрицу регрессоров без свободного члена
X0 <- model.matrix(data = h, dist ~ 0 + speed + speed2 + speed3)

# LASSO для функции реализующей алгоритм LASSO в R важно, чтобы labmda шли в
# убывающем порядке (!!!)
lambdas <- seq(50, 0.1, length = 30)

# оценим LASSO регрессию
m_lasso <- glmnet(X0, y, alpha = 1, lambda = lambdas)

# по горизонтали: логарифм штрафного коэффициента лямбда по вертикали: оценки
# коэффициентов как меняются коэффициенты с ростом штрафного коэффициента?
plot(m_lasso, xvar = "lambda", label = TRUE)

# по горизонтали: доля объясненной дисперсии по вертикали: оценки коэффициентов
# какой долей объяснённой дисперсии мы жертвуем, отказываясь от МНК оценок?
plot(m_lasso, xvar = "dev", label = TRUE)
# правая граница --- это МНК (лямбда=0) видно, что небольшая жертва в доле
# объясненной дисперсии (по горизонтали) приводит к существенному сокращению
# коэффициента 1

# по горизонтали: сумма модулей оценок коэффициентов по вертикали: модуль оценок
# коэффициентов какой коэффициент вносит наибольший вклад в суммарный штраф?
plot(m_lasso, xvar = "norm", label = TRUE)

# коэффициенты LASSO модели для лямбда=0.1 и лямбда=1
coef(m_lasso, s = c(0.1, 1))

# оценка ридж-регрессии:
m_rr <- glmnet(X0, y, alpha = 0, lambda = lambdas)

# Можно точно так же посмотреть на результаты коэффициенты ридж-модели для
# лямбда=0.1 и лямбда=1
coef(m_rr, s = c(0.1, 1))

# и построить аналогичные графики
plot(m_rr, xvar = "lambda", label = TRUE)
plot(m_rr, xvar = "dev", label = TRUE)
plot(m_rr, xvar = "norm", label = TRUE)



# Кросс-валидация позволяет выбрать оптимальное лямбда
cv <- cv.glmnet(X0, y, alpha = 1)

# зависимость кросс-валидационной RSS от лямбда
plot(cv)

# для LASSO как правило используют либо
cv$lambda.min  # лямбда минимизирующую кросс-валидационную RSS
cv$lambda.1se  # консервативную лямбда (увеличенную на одну стандартную ошибку)

# модель соответствующая консервативной лямбда
coef(cv, s = "lambda.1se")




# Метод главных компонент


h <- heptathlon  # набор данных по семиборью
help(heptathlon)  # он встроенный, поэтому глянем справку
glimpse(h)  # бросим взглян на набор данных
h <- select(h, -score)  # удалим итоговый результат спортсмена
describe(h)  # описательные статистики

# корреляционная матрица
cor(h)

# метод главных компонент с предварительной стандартизацией переменных
h.pca <- prcomp(h, scale = TRUE)

# извлекли первую главную компоненту:
pca1 <- h.pca$x[, 1]
head(pca1)

# извлекли веса, с которыми переменные входят в первую главную компоненту:
v1 <- h.pca$rotation[, 1]
v1

# выборочная дисперсия каждой компоненты:
summary(h.pca)
# например, первые две компоненты имеют суммарную выборочную дисперсию равную 80%
# от суммарной выборочный дисперсии 7 видов спорта

# и первая главная компонента действительно здорово дифференциирует спортсменов!
cor(heptathlon$score, pca1)

# выборочная дисперсия каждой компоненты на графике:
plot(h.pca)

# исходный набор данных в новых осях по горизонтали --- pc1 по вертикали --- pc2
biplot(h.pca, xlim = c(-1, 1))
