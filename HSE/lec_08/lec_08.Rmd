---
title: "Временные ряды"
babel-lang: russian
lang: russian
output:
  beamer_presentation:
    colortheme: whale
    keep_tex: yes
    theme: Madrid
  ioslides_presentation: default
  slidy_presentation: default
header-includes:
- \author[Эконометрика. Лекция 8]{Эконометрика. Лекция 8}
- \newcommand{\e}{\varepsilon}
- \usepackage{tikz}
- \usetikzlibrary{cd}
---


# Временные ряды:

* Одномерные. 

Один показатель для каждого момента времени.

* Многомерные

Несколько показателей для каждого момента времени.

# Пример многомерного временного ряда

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library("pander")
library("sophisthse")
library("dplyr")
library("knitr")
library("zoo")
library("forecast")
opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)

y <- sophisthse(series.name = c("POPNUM_Y","GDPVA_EEA_Y"), output="data.frame")
y_sel <- y %>% filter(T>2001,T<2014) %>% select(T, POPNUM_Y, GDPVA_EEA_Y)
colnames(y_sel) <- c("Год", "Население (тыс. чел.)", "ВВП (млрд. руб.)")
pander(y_sel)
```



# Одномерный временной ряд

Временной ряд --- последовательность случайных величин

\[
y_1, y_2, y_3, y_4 \ldots
\]

# Без предположений невозможно прогнозировать

1, 2, 3, 4, 5, ?

Какое число будет следующим?

# Без предположений невозможно прогнозировать

1, 2, 3, 4, 5, 42

# Базовое предположение --- стационарность

Временной ряд называется стационарным, если:

* $E(y_1)=E(y_2)=E(y_3)=\ldots$
* $Var(y_1)=Var(y_2)=Var(y_3)=\ldots=\gamma_0$
* $Cov(y_1,y_2)=Cov(y_2,y_3)=Cov(y_3,y_4)=\ldots=\gamma_1$
* $Cov(y_1,y_3)=Cov(y_2,y_4)=Cov(y_3,y_5)=\ldots=\gamma_2$
* \ldots


# Предпосылки коротко:

Временной ряд называется стационарным, если:

* $E(y_t)=const$
* $Cov(y_t,y_{t-k})=\gamma_k$, т.е. не зависит от $t$

# Автоковариационная функция

$\gamma_k=Cov(y_t, y_{t-k})$ --- (авто)-ковариационная функция процесса

# Самый простой пример --- белый шум

Ряд $\e_t$ --- белый шум, если:

* $E(\e_t)=0$
* $Var(\e_t)=\sigma^2$
* $Cov(\e_t,\e_{t-k})=0$

# Пример белого шума

$\e_t \sim N(0,4)$ и независимы

```{r, warning=FALSE, echo=FALSE, message=FALSE, fig.height=6}
set.seed(42)
y <- as.zoo(rnorm(200, sd=2))
plot(y,main = "",xlab="",ylab="")
```


# Конвенция об обозначениях

На эту лекцию $\e_t$ всегда обозначает белый шум!

# Примеры нестационарных процессов

* Процесс с детерминистическим трендом

* Случайное блуждание

# Процесс с детерминистическим трендом

* $y_t= 5 + 6t + \e_t$. Нестационарность: $E(y_t)=5+6t\neq const$


```{r,fig.height=6}
n <- 200
y <- 5+6*(1:n) + arima.sim(n=n, list(order=c(0,0,0)), sd=200 )
# tsdisplay(y)
plot(y,main = "", xlab="", ylab="")
```



# Случайное блуждание

* $\begin{cases}
y_0 = 0 \\
y_t = y_{t-1} + 2 + \e_t
\end{cases}$. Нестационарность: $Var(y_t)=t \sigma^2\neq const$

```{r, fig.height=6}
n <- 200
y <- 20*arima.sim(n=n, list(order=c(0,1,0))  ) + 2*(0:n)
plot(y,main = "",xlab="",ylab="")
```

# Процесс скользящего среднего

Процесс представимый в виде 

\[
y_t= \mu + \e_t + a_1 \e_{t-1} + \ldots + a_q \e_{t-q}
\]

# Обозначение процесса скользящего среднего 

$y_t \sim MA(q)$, Moving Average


# Пример MA процесса [у доски]

\[
y_t = 5 + \e_t + 3 \e_{t-1} -2\e_{t-2}
\]


Найдите $E(y_t)$, $Var(y_t)$, $Cov(y_t,y_{t-k})$

# Запись с помощью оператора лага

$L$ --- оператор лага:

* $Ly_t=y_{t-1}$

* $L\cdot L\cdot y_t=L^2y_t=y_{t-2}$

* \ldots

# Пример записи с помощью оператора лага

$MA(2):$

\[
y_t= 2 + \e_t + 3\e_{t-1}-2\e_{t-2}
\]

\[
y_t= 2 +(1+3L-2L^2)\e_t
\]


# Интерпретация:

Коэффициенты плохо интерпретируемы. 

У стационарного процесса есть (авто)-корреляционная функция:

\[
\rho_k=Corr(y_t, y_{t-k})=\frac{Cov(y_t,y_{t-k})}{\sqrt{Var(y_t)Var(y_{t-k})}}=\frac{\gamma_k}{\gamma_0}
\]

# Интерпретация

Если $y_t$ --- стационарный процесс и $y_t \sim N(\mu_y, \sigma^2_y)$, то:

$\rho_k$ --- на сколько в среднем изменится $y_{t}$ при росте $y_{t-k}$ на единицу 

# Автокорреляционная функция MA процесса [у доски]

\[
y_t = 5 + \e_t + 3 \e_{t-1} -2\e_{t-2}
\]

Найдите $\rho_k$

# Частная автокорреляционная функция-идея {.fragile}
$\rho_k$ --- автокорреляционная функция. Измеряет совокупный эффект воздействия $y_{t-k}$ на $y_t$ как напрямую, так и через $y_{t-k+1}$, $y_{t-k+2}$, ..., $y_{t-1}$


$\phi_k$ --- частная автокорреляционная функция. Измеряет прямой эффект воздействия $y_{t-k}$ на $y_t$, устранив сквозное воздействие через $y_{t-k+1}$, $y_{t-k+2}$, ..., $y_{t-1}$.


\begin{tikzcd}
y_1  \arrow[r] \arrow[rr, bend left=50] \arrow[bend left=75]{rrr}{\phi_3} &
y_2  \arrow[r] \arrow[rr, bend left=50] &
y_3  \arrow[r] &
y_4 
\end{tikzcd}


# Частная автокорреляционная функция-интерпретация

Если $y_t$ --- стационарный процесс и $y_t \sim N(\mu_y, \sigma^2_y)$, то:

$\phi_k$ --- на сколько в среднем изменится $y_{t}$ при росте $y_{t-k}$ на единицу 

при фиксированных $y_{t-1}$, $y_{t-2}$, \ldots, $y_{t-k+1}$


# Частная автокорреляционная функция-определение

\[
\phi_{k}=Cor(y_t - P(y_t), y_{t-k} - P(y_{t-k}))
\]

где $P(y_t)$ --- проекция случайной величины $y_t$ на линейную оболочку величин $y_{t-1}$, $y_{t-2}$, \ldots, $y_{t-k+1}$.

# Частная автокорреляция алгоритм подсчёта


\[
\gamma_0 \phi_1 = \gamma_1  
\]

\[
\begin{cases}
\gamma_0 *_1 + \gamma_1 \phi_2  = \gamma_1 \\
\gamma_1 *_1 + \gamma_0 \phi_2  = \gamma_2 
\end{cases}
\]


\[
\begin{cases}
\gamma_0 *_1 + \gamma_1 *_2 + \gamma_2 \phi_3 = \gamma_1 \\
\gamma_1 *_1 + \gamma_0 *_2 + \gamma_1 \phi_3 = \gamma_2 \\
\gamma_2 *_1 + \gamma_1 *_2 + \gamma_0 \phi_3 = \gamma_3
\end{cases}
\]

\ldots


# Частная автокорреляционная функция MA процесса [у доски]

\[
y_t = 5 + \e_t + 3 \e_{t-1} -2\e_{t-2}
\]

Найдите $\phi_1$, $\phi_2$, $\phi_3$

# Процесс авторегрессии

* Стационарный процесс вида
\[
y_t=c + b_1 y_{t-1} + b_2 y_{t-2} + \ldots + b_p y_{t-p} + \e_t
\]

# Обозначение процесса авторегрессии

$y_t \sim AR(p)$, AutoRegression

# Частная и обычная автокорреляционные функции для AR процесса [у доски]
\[
y_t = 2 + 0.5 y_{t-1} + \e_t \; \e_t \sim N(0,\sigma^2)
\]

Найдите $\rho_k$, $\phi_k$

# Альтернативная форма записи

\[
y_t = 2 + 0.5 y_{t-1} + \e_t
\]

Или

\[
(y_t - 4) = 0.5 (y_{t-1} - 4) + \e_t
\]

# Важное предупрежедение

Из одного уравнения $y_t = 2 + 0.5 y_{t-1} + \e_t$ не следует автоматически стационарность (!)

# Пример множества решений [у доски]

\[
y_t = 2 + 0.5 y_{t-1} + \e_t \; \e_t \sim N(0,1)
\]

* $y_0=0$, $y_1\sim N(2,1)$, $y_2\sim N(3, 1.25)$, \ldots

* $y_0\sim N(3, 4/3)$, $y_1\sim N(3, 4/3)$, $y_2\sim N(3, 4/3)$, \ldots


# Подразумеваем стационарное решение

Пишем:
\[
y_t = 2 + 0.5 y_{t-1} + \e_t \; \e_t \sim N(0,1)
\]

Подразумеваем:

* $y_0\sim N(3, 4/3)$, $y_1\sim N(3, 4/3)$, $y_2\sim N(3, 4/3)$, \ldots

# AR процесс можно записать с помощью лага

\[
y_t=2+0.5y_{t-1}-0.06y_{t-2}+\e_t
\]

\[
(1-0.5L+0.06L^2)y_t=2+\e_t
\]


# Характеристический многочлен

\[
(1-0.5L+0.06L^2)y_t=2+\e_t
\]

\[
f(L)y_t=2+\e_t
\]

$f(L)$ --- характеристический многочлен

# Когда у есть стационарное решение?

\[
f(L)y_t=c+\e_t
\]


Если корни характеристического уравнения AR процесса, $f(z)=0$, по модулю больше единицы, то существует единственное стационарное решение, в котором $y_t$ выражается через прошлые шумы, то есть через $\e_t$, $\e_{t-1}$, $\e_{t-2}$, \ldots

# Упражнение [у доски]

Пример 1. $y_t=7+0.5y_{t-1}-0.06y_{t-2}+\e_t$

Пример 2. $y_t=-3+1.2y_{t-1}-0.2y_{t-2}+\e_t$

Есть ли у этих уравнений стационарные решения?

# Прогнозирование

Прогноз на $h$ шагов вперед: $E(y_{t+h}|y_t, y_{t-1}, y_{t-2}, \ldots)$

Часто кратко обозначают: $\hat{y}_{t+h}$

# Упражнение на прогнозирование [у доски]

\[
y_t=2+0.5y_{t-1}-0.06y_{t-2}+\e_t \; \e_t \sim N(0;4)
\]

$y_{100}=4$, $y_{99}=3$.

Постройте точечный и интервальный прогноз на 1 и 2 шага вперед

# Модель авторегрессии и скользящего среднего

* Стационарный процесс вида
\begin{multline}
\nonumber
y_t=c + b_1 y_{t-1} + b_2 y_{t-2} + \ldots + b_p y_{t-p} + \\
+\e_t + a_1 \e_{t-1} + \ldots + a_q \e_{t-q}
\end{multline}

где сумма $p+q$ минимально возможна

# Обозначение

* $y_t \sim ARMA(p,q)$

# Сумма $p+q$ минимально возможная 

* $y_t=\e_t$

* $y_t-y_{t-1}=\e_t-\e_{t-1}$

В этом примере $y_t \sim ARMA(0,0)$

# ARMA --- это наше всё!

Теорема. 

Любой стационарный процесс можно представить в виде $MA(\infty)$

Практический вывод. С помощью $ARMA(p,q)$ можно компактно и сколь угодно точно описать любой стационарный процесс

# Итого про ARMA(p,q)

* коэффициенты не интерпретируемы

* используются для прогнозирования

# Оценивание коэффициентов 

Есть $T$ наблюдений: $y_1$, $y_2$, $y_3$, \ldots, $y_T$

Чаще всего используется метод максимального правдоподобия


# Подробности метода максимального правдоподобия

* Как правило, предполагается нормальность $\e_t \sim N(0;\sigma^2)$

* Стационарность $y_t$

\begin{multline}
\nonumber
y_t=c + b_1 y_{t-1} + b_2 y_{t-2} + \ldots + b_p y_{t-p} + \\
+\e_t + a_1 \e_{t-1} + \ldots + a_q \e_{t-q}
\end{multline}

# Результат метода максимального правдоподобия

На выходе получаем оценки 

\[
\hat{\theta}=(\hat{c}, \hat{a}_1, \ldots, \hat{a}_q, \hat{b}_1, \ldots, \hat{b}_q, \hat{\sigma}^2)
\]

И оценку их ковариационной матрицы $\widehat{Var}(\hat{\theta})$

# Проверка гипотез и доверительные интервалы

\[
\frac{\hat{a}_j - a_j}{se(\hat{a}_j)} \to N(0;1)
\]

# Выборочная автокорреляционная функция

ACF --- autocorrelation function

\[
\hat{\rho}_k = \frac{\sum_{t=k+1}^{T} (y_t-\bar{y})(y_{t-k}-\bar{y})}{\sum_{t=1}^{T} (y_t-\bar{y})^2}
\]

# Выборочная частная автокорреляционная функция

PACF --- partial autocorrelation function

Получим $\hat{\phi}_k$ из оценки регрессии

\[
\hat{y}_t = * + * \cdot y_{t-1} + * \cdot y_{t-2} + \ldots + *  \cdot y _{t-k+1} + \phi_k y_{t-k} + u_t
\]

# Примечания к расчету автокорреляционной функции

* Для оценки каждого $\hat{\phi}_k$ строится отдельная регрессия

* Из каждой регрессии нужен только последний коэффициент

# Алгоритм на практике

1. Графики ряда, ACF, PACF

1. Если ряд нестационарный, то преобразуем 

1. Выбираем $p$ и $q$ 

1. Оцениваем $ARMA(p,q)$

1. Прогнозируем

# Основное преобразование

Взятие разности: переход от $y_t$ к $\Delta y_t$

# Обозначение

* $y_t \sim ARIMA(p,1,q)$ равносильно $\Delta y_t \sim ARMA(p,q)$

* $y_t \sim ARIMA(p,0,q)$ равносильно $y_t \sim ARMA(p,q)$

# Выбор $p$ и  $q$ по графикам

График выборочной корреляционной функции есть даже у нестационарного процесса!

У нестационарного процесса $\rho_k$, $\phi_k$ не существуют, однако компьютер всегда может построить график выборочной автокорреляционной и выборочной частной автокорреляционной функции!

# Белый шум

Белый шум, $y_t=\e_t$

```{r}
y <- arima.sim(n=n, list(order=c(0,0,0)) )

# tsdisplay(y)
tsdisplay(y,main = "")
```


# Случайное блуждание (нестационарный процесс!)

Случайное блуждание, $y_t=y_{t-1}+\e_t$. Истинные $\rho_k$ и $\phi_k$ НЕ существуют!

```{r}
y <- arima.sim(n=n, list(order=c(0,1,0)) )
tsdisplay(y)
```



# Процесс с трендом (нестационарный процесс!)

Процесс с трендом, $y_t=0.02\cdot t +\varepsilon_t$. Истинные $\rho_k$ и $\phi_k$ НЕ существуют!

```{r}
y <- 0.02*(1:n)+arima.sim(n=n, list(order=c(0,0,0)) )
tsdisplay(y,main = "")
```

# AR(1) 

AR(1), $y_t=0.7y_{t-1} +\varepsilon_t$

```{r}
y <- arima.sim(n=n, list(ar=0.7) )
tsdisplay(y,main = "")
```

# AR(2)

AR(2), $y_t=0.9y_{t-1}-0.5y_{t-2} +\varepsilon_t$


```{r}
y <- arima.sim(n=n, list(ar=c(0.9,-0.5)) )
tsdisplay(y,main = "")
```


# MA(1) 

MA(1), $y_t=0.7\varepsilon_{t-1} +\varepsilon_t$

```{r}
y <- arima.sim(n=n, list(ma=0.7) )
tsdisplay(y,main = "")
```

# MA(2)

MA(2), $y_t=0.9\varepsilon_{t-1} +0.5\varepsilon_{t-1}+\varepsilon_t$

```{r}
y <- arima.sim(n=n, list(ma=c(0.9,0.5)) )
tsdisplay(y,main = "")
```

# ARMA(1,1)

ARMA(1,1), $y_t=0.7y_{t-1}+0.5\varepsilon_{t-1} +\varepsilon_t$

```{r}
y <- arima.sim(n=n, list(ar=0.7, ma=0.5) )
tsdisplay(y,main = "")
```

# Мораль

* Временные ряды: стационарные и нет

* Для стационарных --- модель ARMA


# Источники мудрости:

* Борзых Д.А., Демешев Б.Б. Эконометрика в задачах и упражнениях: глава 12

* Катышев П.К., Пересецкий А. А. Эконометрика. Начальный курс: главы 11.3, 11.4

* Носко В.П., Эконометрика. Введение в регрессионный анализ временных рядов: глава 2

