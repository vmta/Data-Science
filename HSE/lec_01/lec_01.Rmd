---
title: "Метод наименьших квадратов"
lang: russian
babel-lang: russian
header-includes: 
  - \author[Эконометрика. Лекция 1/10]{Эконометрика. Coursera. Лекция 1}
  - \usepackage{tikz}
output:
  beamer_presentation:
    keep_tex: yes
    theme: Madrid
    colortheme: whale
  ioslides_presentation: default
---

## Эконометрика на одном слайде :)

### Вопросы:

- Как устроен мир? Как переменная $x$ влияет на переменную $y$?
- Что будет завтра? Как спрогнозировать переменную $y$?

### Ответ: 

Модель --- формула для объясняемой переменной

### Например:

- $y_i=\beta_1+\beta_2 x_i + \varepsilon_i$
<!-- $y_{t}=y_{t-1}+\varepsilon_t$ -->

```{r, echo=FALSE, include=FALSE}
library(knitr)
opts_chunk$set(cache = FALSE, dev = 'png', dpi = 300)
```


## Основные типы данных:

- Временные ряды
- Перекрёстные данные
- Панельные данные 

Есть много-много других!

## Временные ряды  

Данные по России:

Год   | Население   |  Безработица  
------|-------------|--------------
2010 | 142962       |  7.4  
2011 | 142914       |  6.5  
2012 | 143103       |  5.5  
2013 | 143395       |  5.5  

## Перекрёстная выборка  

Результаты зимних Олимпийских игр 2014:

Страна | Золото | Серебро | Бронза  
-------|--------|---------|--------
Россия | 13 | 11 |	9 	
Норвегия | 11 |	5 |	10 	
Канада |	10 |	10 	| 5 	
США |	9  |	7 |	12  

## Панельные данные 

Cочетание первых двух: данные по нескольким переменным для множества объектов в разные моменты времени


## Данные --- обозначения

- Одна зависимая, объясняемая, переменная: $y$
- Несколько регрессоров, объясняющих, переменных: $x$, $z$, $\ldots$
- По каждой переменной $n$ наблюдений: $y_1$, $y_2$, $\ldots$, $y_n$


## Данные --- пример 

```{r, message=FALSE, results='asis', echo=FALSE, include=FALSE}
library("knitr")
library("dplyr")
library("ggplot2")
d <- cars %>% mutate(dist = 0.3 * dist, speed = 1.61 * speed)
kable(head(d))
```

Исторические данные 1920-х годов :)

| Длина тормозного пути (м), $y_i$ | Скорость машины (км/ч), $x_i$ |
|-----:|----:|
| 0.6 |  6.44|  
| 3.0 |  6.44|  
| 1.2 | 11.27|
| ...|  ...|



## Всегда изображайте данные!

```{r, warning=FALSE, echo=FALSE}
qplot(data = d, speed, dist, 
      xlab = "Скорость машины (км/ч)",
      ylab = " Длина тормозного пути (м)",
      main = "Данные по машинам 1920-х годов") + theme_bw()
```

## Модель:

Пример: $y_i=\beta_1 + \beta_2 x_i + \varepsilon_i$

- Наблюдаемые переменные: $y$, $x$
- Неизвестные параметры: $\beta_1$, $\beta_2$
- Случайная составляющая, ошибка: $\varepsilon$

### План действий
- придумать адекватную модель  
- получить оценки неизвестных параметров: $\hat{\beta}_1$, $\hat{\beta}_2$  
- прогнозировать, заменив неизвестные параметры на оценки:  
\[
\hat{y}_i=\hat{\beta}_1 + \hat{\beta}_2 x_i
\]


## Метод наименьших квадратов

- Способ получить оценки неизвестных параметров модели исходя из реальных данных.

Ошибка прогноза: $\hat{\varepsilon}_i=y_i-\hat{y}_i$.

Сумма квадратов ошибок прогноза:
\[
Q(\hat{\beta}_1,\hat{\beta}_2)=\sum_{i=1}^n \hat{\varepsilon}_i^2=\sum_{i=1}^n (y_i-\hat{y}_i)^2
\]

Суть МНК: В качестве оценок взять такие $\hat{\beta}_1$, $\hat{\beta}_2$, при которых сумма квадратов ошибок прогноза, $Q$, минимальна.

## Пример с машинами:

Фактические данные: 

$x_1=6.68$, $x_2=6.68$, ..., 

$y_1=0.6$, $y_2=3$, ...

Модель: $y_i=\beta_1+\beta_2 x_i+\varepsilon_i$. Формула для прогнозов:  $\hat{y}_i=\hat{\beta}_1 + \hat{\beta}_2 x_i$

Сумма квадратов ошибок прогнозов: $Q=\sum_{i=1}^n (y_i-\hat{y}_i)^2$

\[
Q=(0.6-\hat{\beta}_1-\hat{\beta}_2 6.68)^2+(3-\hat{\beta}_1-\hat{\beta}_2 6.68)^2+...
\]

Точка минимума, найдена в R: $\hat{\beta}_1=-5.3$, $\hat{\beta}_2=0.7$:

Формула для прогнозов:  $\hat{y}_i=-5.3 + 0.7 x_i$



## Простой пример [у доски]

Имя | Вес (кг), $y_i$ | Рост (см), $x_i$ |
|----:|-----:|----:|
| Вася|  60|  170|  
| Коля| 70|  170|  
| Петя| 80| 181|

Оцените модели:  
$y_i=\beta  +\varepsilon_i$,   
$y_i=\beta_1+\beta_2 x_i +\varepsilon_i$  

Маленькая подготовка: $n\bar{x}=\sum_i x_i=\sum_i \bar{x}$, $\sum_i (x_i - \bar{x})=0$.

<!--
Модель: $y_i=\beta x_i +\varepsilon_i$, Прогнозы: $\hat{y}_i=\hat{\beta}x_i$.

Сумма квадратов ошибок:
\[
Q(\hat{\beta})=(60-\hat{\beta}170)^2+(70-\hat{\beta}170)^2+(80-\hat{\beta}180)^2
\]

## Решение задачи минимизации

Сумма квадратов ошибок:
\[
Q(\hat{\beta})=(60-\hat{\beta}170)^2+(70-\hat{\beta}170)^2+(80-\hat{\beta}180)^2
\]

Производная:
\begin{multline}
Q'(\hat{\beta})=-2\cdot 170\cdot (60-\hat{\beta}170)-2\cdot 170\cdot(70-\hat{\beta}170)\\
-2\cdot 180\cdot(80-\hat{\beta}180)
\end{multline}

Приравняв производную к нулю получаем:
\[
\hat{\beta}=0.4047
\]
-->

## Готовые формулы МНК. Регрессия на константу

В модели $y_i=\beta +\varepsilon_i$ 

\[
\hat{\beta}=\bar{y}
\]

Интерпретация:

В модели без объясняющих переменных наилучший прогноз --- это среднее значение зависимой переменной

## Готовые формулы МНК. Парная регрессия 


В модели $y_i=\beta_1+\beta_2 x_i +\varepsilon_i$  

\[
\hat{\beta}_2=\frac{\sum (x_i-\bar{x})(y_i-\bar{y})}{\sum (x_i-\bar{x})^2}
\]
\[
\hat{\beta}_1=\bar{y}-\hat{\beta}_2\bar{x}
\]

Интерпретация:

Точка $(\bar{x},\bar{y})$ лежит на линии регрессии $\hat{y}=\hat{\beta}_1+\hat{\beta}_2 x$

## Терминология и обозначения:

$y_i$ --- зависимая, объясняемая, переменная

$x_i$ --- регрессор, объясняющая переменная

$\varepsilon_i$ --- ошибка, ошибка модели, случайная составляющая

$\hat{y}_i$ --- прогноз, прогнозное значение

$\hat{\varepsilon}_i=y_i-\hat{y}_i$ --- остаток, ошибка прогноза

$RSS=\sum_{i=1}^n \hat{\varepsilon}_i^2$ --- сумма квадратов остатков


## Регрессия проходит через среднюю точку [у доски]

```{r, warning=FALSE, echo=FALSE}
qplot(data = d, speed, dist, 
      xlab = "Скорость машины (км/ч)",
      ylab = " Длина тормозного пути (м)",
      main = "Данные по машинам 1920-х годов") + theme_bw() +
  stat_smooth(method = "lm", se = FALSE) +
  geom_vline(xintercept = mean(d$speed), linetype = "longdash") +
  geom_hline(yintercept = mean(d$dist), linetype = "longdash")
```

<!--
## Три простых случая в явном виде

### $y_i=\beta+\varepsilon_i$
### $y_i=\beta x_i+\varepsilon_i$
### $y_i=\beta_1+\beta_2 x_i+\varepsilon_i$

## Случай $y_i=\beta+\varepsilon_i$. Нет объясняющей переменной. 

- Сколько лет Васе?

Анна: 35. Белла: 27. Вика: 34.

Спрогнозируем Васин возраст с помощью МНК!

Наблюдения: $y_1$, $y_2$, ..., $y_n$

Модель: $y_i=\beta+\varepsilon_i$. Прогнозы: $\hat{y}_i=\hat{\beta}$

Сумма квадратов остатков: $Q=\sum_{i=1}^n (y_i-\hat{y}_i)^2= \sum_{i=1}^n(y_i-\hat{\beta})^2$

Находим производную: $Q'(\hat{\beta})=\sum_{i=1}^n -2(y_i-\hat{\beta})$

## Случай $y_i=\beta+\varepsilon_i$. Решение.

Упрощаем производную:

\begin{multline}
Q'(\hat{\beta})=\sum_{i=1}^n -2(y_i-\hat{\beta})=-2 \sum_{i=1}^n (y_i-\hat{\beta})=\\
-2 (\sum_{i=1}^n y_i-\sum_{i=1}^n \hat{\beta})=-2(\sum_{i=1}^n y_i-n\hat{\beta})
\end{multline}

Приравняв к нулю получаем: $\sum_{i=1}^n y_i=n\hat{\beta}$ или

$\hat{\beta}=\sum_{i=1}^n y_i /n=(y_1+y_2+...+y_n)/n=\bar{y}$

МНК-прогноз возраста Васи: $\hat{\beta}=(35+27+34)/3=32$


## Случай $y_i=\beta x_i+\varepsilon_i$. Пропорциональность. 


## Случай $y_i=\beta_1 + \beta_2 x_i+\varepsilon_i$. Парная регрессия.


Предварительные замечания:

$\bar{x}=\sum_{i=1}^n x_i$, поэтому:

$n\bar{x}=\sum_{i=1}^n x_i$ или $\sum_{i=1}^n \bar{x}=\sum_{i=1}^n x_i$.
-->

## Много объясняющих переменных [у доски]

$y_i=\beta_1+\beta_2 x_i +\beta_3 z_i+\varepsilon_i$

Выпишем систему уравнений для оценок $\hat{\beta}_1$, $\hat{\beta}_2$, $\hat{\beta}_3$

\[
\begin{cases}
\sum \hat{\varepsilon}_i \cdot 1 =0 \\
\sum \hat{\varepsilon}_i \cdot x_i =0 \\
\sum \hat{\varepsilon}_i \cdot z_i =0
\end{cases}
\]

## Суммы квадратов

* Сумма квадратов остатков 
\[
RSS=\sum \hat{\varepsilon}_i^2
\]

* Общая сумма квадратов
\[
TSS=\sum (y_i-\bar{y})^2
\]

* Объясненная сумма квадратов
\[
ESS=\sum (\hat{y}_i-\bar{y})^2
\]


## Абсолютный ликбез по линейной алгебре 

Вектора: $y$, $x$, $\hat{y}$, $\varepsilon$, \ldots

\[
y=\begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{pmatrix}\;
x=\begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{pmatrix}\;
\hat{\varepsilon}=\begin{pmatrix}
\hat{\varepsilon}_1 \\
\hat{\varepsilon}_2 \\
\vdots \\
\hat{\varepsilon}_n
\end{pmatrix}\;
\vec{1}=\begin{pmatrix}
1 \\
1 \\
\vdots \\
1
\end{pmatrix}
\] 

В нашей модели:
$\hat{y}=\hat{\beta}_1 \cdot \vec{1}+\hat{\beta}_2 \cdot x +\hat{\beta}_3 \cdot z$

## Матрица всех регрессоров

\[
X=\begin{pmatrix}
1 & x_1 & z_1 \\
1 & x_2 & z_2 \\
\vdots \\
1 & x_n & z_n 
\end{pmatrix}
\]

## Длина вектора

Длина вектора, $|y|=\sqrt{y_1^2+y_2^2+\ldots+ y_n^2}$

Квадрат длины вектора, $|y|^2=y_1^2+y_2^2+\ldots + y_n^2=\sum_i y_i^2$

Примеры:  
$RSS=\sum \hat{\varepsilon}_i^2$ --- квадрат длины вектора $\hat{\varepsilon}$  
$TSS=\sum (y_i-\bar{y})^2$ --- квадрат длины вектора $(y-\bar{y}\cdot \vec{1})$

$\begin{pmatrix}
y_1-\bar{y} \\
y_2-\bar{y} \\
\vdots \\
y_n-\bar{y} 
\end{pmatrix} =
\begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n 
\end{pmatrix} -
\bar{y}\begin{pmatrix}
1 \\
1 \\
\vdots \\
1 
\end{pmatrix}=y-\bar{y}\cdot \vec{1}$

## Скалярное произведение двух векторов:

\[
(x,y)=|x|\cdot |y|\cdot cos(x,y)
\]

\[
(x,y)=x_1 y_1 +x_2 y_2 +\ldots+ x_n y_n=\sum_i x_i y_i
\]

Условие перпендикулярности: 

\[
x \perp y  \Leftrightarrow  \sum_i x_i y_i=0
\]

т.к. $cos(90^\circ)=0$.

## Иллюстрация для регрессии на константу [у доски]

Модель: $y_i=\beta + \varepsilon_i$

Прогнозы: $\hat{y}_i=\hat{\beta}=\bar{y}$

\begin{figure}
\begin{tikzpicture}
\draw [->, thick] (0,0) -- (1,2);
\draw [->, thick] (0,0) -- (3,0);
\draw [->, thick] (0,0) -- (1,0);
\draw [dashed, thick] (1,2) -- (1,0);
\node [below] at (1,0) {$\bar{y}\cdot\vec{1}$};
\node [above] at (1,2) {$y$};
\node [below] at (3,0) {$\vec{1}$};
\end{tikzpicture}
\end{figure}


## Геометрическая интерпретация условий первого порядка

\[
\begin{cases}
\sum \hat{\varepsilon}_i \cdot 1 =0 \\
\sum \hat{\varepsilon}_i \cdot x_i =0 \\
\sum \hat{\varepsilon}_i \cdot z_i =0
\end{cases}\; \Leftrightarrow \;
\begin{cases}
\hat{\varepsilon}\perp \vec{1} \\
\hat{\varepsilon}\perp x \\
\hat{\varepsilon}\perp z \\
\end{cases}
\]

## Иллюстрация для множественной регрессии  [у доски]

\includegraphics{3dvec-ols.pdf}



## Если в регрессию включён свободный член $\beta_1$


Если в регрессию включён свободный член, $y_i=\beta_1 + \ldots$, и оценки МНК 
единственны, то:

- $\sum \hat{\varepsilon}_i=0$  
- $\sum y_i = \sum \hat{y}_i$  
- $\bar{y}=\bar{\hat{y}}$  
- $TSS=RSS+ESS$  


## Коэффициент детерминации --- простой показатель качества

В моделях со свободным членом  $R^2=ESS/TSS$  

$TSS$ --- общий разброс $y$  
$ESS$ --- объясненный регрессорами разброс  
$R^2$ --- доля объясненного разброса в общем разбросе

Теорема. Если в регрессию включён свободный член, $y_i=\beta_1 + \ldots$, и оценки МНК 
единственны, то $R^2$ равен выборочной корреляции между $y$ и $\hat{y}$, т.е.

\[
R^2=(sCorr(y,\hat{y}))^2=\left(\frac{\sum (y_i-\bar{y})(\hat{y}_i-\bar{y})}{\sqrt{\sum(y_i-\bar{y})^2}\sqrt{\sum(\hat{y}_i-\bar{y})^2}}\right)^2
\]


## Явная формула для оценок коэффициентов

Модель: $y_i=\beta_1 + \beta_2 x_i +\beta_3 z_i +\varepsilon_i$ 

\[
y=\begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n 
\end{pmatrix}
\; X=\begin{pmatrix}
1 & x_1 & z_1 \\
1 & x_2 & z_2 \\
\vdots \\
1 & x_n & z_n 
\end{pmatrix}
\]

Линейная алгебра позволяет получить явные формулы:

\[
\hat{\beta}=(X'X)^{-1}X'y
\]

## Мораль

УРА!!! МНК позволяет оценивать модели!!!

Предположив $y_i=\beta_1 + \beta_2 x_i +\beta_3 z_i +\varepsilon_i$ 

Получаем $\hat{\beta}_1$, $\hat{\beta}_2$, $\hat{\beta}_3$

<!--
## Первая регрессия в R

(!) Установите R, Rstudio и дополнительные пакеты

Три режима работы с Rstudio:

- диалоговый, консольный

- написание скрипта или программы

- написание документа, "грамотное программирование"

## Консольный режим

... тут скринкаст


## Консольный режим. Резюме...

- R отличает заглавные и прописные буквы. 
```{r}
a <- 5
A <- 4
a + A
```

- присваивания `a <- 5` и `a = 5` абсолютно равнозначны

- знак `+` в командной строке означает неоконченную команду

    - может означать забытую незакрытую скобку
    - избавиться можно нажатием клавиши `Esc`

- `tab` облегачает жизнь, дописывая длинные названия

## Написание скрипта


(тут скринкаст)

## Написание скрипта. Резюме...

- `ctrl+Enter` (`cmd+Enter` на Маке) исполняет текущую строчку или несколько строк

- два основных объекта: вектор и табличка с данными

```{r}
x <- c(5,2,1)
d <- data.frame(rost=c(170,170,180),ves=c(60,70,80))
```

- любой реальный скрипт начинается с загрузки дополнительных пакетов
```{r}
library("dplyr")
library("ggplot2")
```

## Резюме. Загрузка данных.

Загрузка данных из электронной таблицы (Excel, Libre Office Calc, Gnumeric ...)

1. Причесать данные

2. Сохранить данные в формате csv

3. Прочитать данные в R командой

```{r, eval=FALSE}
d <- read.table("mydata.csv")
```

## Резюме. Поглядеть на табличку

Данные в табличке `d`.

- начало и конец таблички: `head(d)`, `tail(d)`
- описание таблички: `str(d)`
- описательные статистики: `summary(d)`
- достать переменную `speed` из таблички: `d$speed`
- достать вторую строку из таблички: `d[2,]`
- достать второй столбец из таблички: `d[,2]`
- преобразовать или создать новую переменную: `d <- mutate(d,speed2=speed^2)`

## Резюме. Два базовых графика

- Гистограмма


- Диаграмма рассеяния


- Не забывайте подписи!!!

## Резюме. Простой пример регрессии

-->


## Вопросы

- Как выбрать форму модели?

- А будет ли решение задачи минимизации единственным?

- А будет ли решение задачи минимизации вообще существовать?

- А почему сумма квадратов остатков, а не, скажем, модулей?

- А насколько точны полученные оценки?

- ...

## Источники мудрости:

* Артамонов Н.В., Введение в эконометрику: главы 1.1, 1.2, 2.1

* Борзых Д.А., Демешев Б.Б. Эконометрика в задачах и упражнениях: глава 1

* Катышев П.К., Пересецкий А. А. Эконометрика. Начальный курс: главы 2.1, 2.2, 3.1, 3.2

* Себер Дж., Линейный регрессионный анализ: главы 1.0, 1.1, 1.2, 2.1, 3.1



<!--

## Написание документа

```{r}
library("dplyr")
library("ggplot2")
d <- cars 
head(cars)
# %>% mutate(dist=0.3*dist,speed=1.67*speed)
```



---
$\bar{y}=(y_1+y_2+...+y_n)/n$ --- среднее значение $y$



$TSS=\sum_{i=1}^n (y_i-\bar{y})^2$ --- общая сумма квадратов

$ESS=\sum_{i=1}^n (\hat{y}_i-\bar{y})^2$ --- объясненная сумма квадратов 



-->

